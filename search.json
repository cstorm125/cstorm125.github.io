[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigureÂ 1: A line plot on a polar axis"
  },
  {
    "objectID": "notebook/sales_prediction.html",
    "href": "notebook/sales_prediction.html",
    "title": "Predict Zero-inflated and Long-tailed Outcomes",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport random\nfrom ucimlrepo import fetch_ucirepo \nimport boto3\nimport json\nfrom tqdm.auto import tqdm\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom autogluon.tabular import TabularDataset, TabularPredictor\nfrom sklearn.metrics import (\n    mean_squared_error, mean_absolute_error, r2_score, median_absolute_error,\n    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n)\nfrom scipy.stats import pearsonr\n\ndef calculate_regression_metrics(y_true, y_pred):\n    return {\n        'root_mean_squared_error': np.sqrt(mean_squared_error(y_true, y_pred)),\n        'mean_squared_error': mean_squared_error(y_true, y_pred),\n        'mean_absolute_error': mean_absolute_error(y_true, y_pred),\n        'r2': r2_score(y_true, y_pred),\n        'pearsonr': pearsonr(y_true, y_pred)[0],  # Pearson correlation coefficient\n        'median_absolute_error': median_absolute_error(y_true, y_pred)\n    }\n\ndef caluclate_classification_metrics(y_true, y_pred):\n    return {\n        'accuracy': accuracy_score(y_true, y_pred),\n        'precision': precision_score(y_true, y_pred, average='weighted'),\n        'recall': recall_score(y_true, y_pred, average='weighted'),\n        'f1_score': f1_score(y_true, y_pred, average='weighted'),\n        'confusion_matrix': confusion_matrix(y_true, y_pred)\n    }\n\ndef string_to_yearmon(date):\n    date = date.split()\n    date = date[0].split('/') + date[1].split(':')\n    date = date[2] + '-' + date[0].zfill(2) #+ '-' + date[1].zfill(2) + ' ' + date[3].zfill(2) + ':' + date[4].zfill(2)\n    return date\n\ndef call_llama(system_prompt, input):\n    template = f\"\"\"&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;{system_prompt}&lt;&lt;/SYS&gt;&gt;{input}[/INST]\"\"\"\n    client = boto3.client(service_name='bedrock-runtime',region_name='us-west-2')\n    body = json.dumps({\n        \"prompt\": template,\n        \"temperature\": 0.,\n        \"top_p\": 0.9,\n        \"max_gen_len\": 2048,\n    })\n    response = client.invoke_model(\n        body=body,\n        modelId='us.meta.llama3-2-90b-instruct-v1:0',\n        accept='application/json',\n        contentType='application/json'\n    )\n    response_body = json.loads(response['body'].read())\n    return response_body\n\ndef call_claude(system_prompt, input):\n    client = boto3.client(service_name='bedrock-runtime',region_name='us-west-2')\n    body=json.dumps(\n        {\n            \"anthropic_version\": \"bedrock-2023-05-31\",\n            \"max_tokens\": 2048,\n            \"messages\": [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": system_prompt + '\\n' + input,\n                    }\n                    ]\n                }\n                ]\n        }  \n    )  \n\n    \n    response = client.invoke_model(body=body, \n                                   modelId='anthropic.claude-3-5-sonnet-20241022-v2:0',\n                                   contentType='application/json',\n                                   accept='application/json')\n    response_body = json.loads(response.get('body').read())\n   \n    return response_body"
  },
  {
    "objectID": "notebook/sales_prediction.html#load-dataset",
    "href": "notebook/sales_prediction.html#load-dataset",
    "title": "Predict Zero-inflated and Long-tailed Outcomes",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport random\nfrom ucimlrepo import fetch_ucirepo \nimport boto3\nimport json\nfrom tqdm.auto import tqdm\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom autogluon.tabular import TabularDataset, TabularPredictor\nfrom sklearn.metrics import (\n    mean_squared_error, mean_absolute_error, r2_score, median_absolute_error,\n    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n)\nfrom scipy.stats import pearsonr\n\ndef calculate_regression_metrics(y_true, y_pred):\n    return {\n        'root_mean_squared_error': np.sqrt(mean_squared_error(y_true, y_pred)),\n        'mean_squared_error': mean_squared_error(y_true, y_pred),\n        'mean_absolute_error': mean_absolute_error(y_true, y_pred),\n        'r2': r2_score(y_true, y_pred),\n        'pearsonr': pearsonr(y_true, y_pred)[0],  # Pearson correlation coefficient\n        'median_absolute_error': median_absolute_error(y_true, y_pred)\n    }\n\ndef caluclate_classification_metrics(y_true, y_pred):\n    return {\n        'accuracy': accuracy_score(y_true, y_pred),\n        'precision': precision_score(y_true, y_pred, average='weighted'),\n        'recall': recall_score(y_true, y_pred, average='weighted'),\n        'f1_score': f1_score(y_true, y_pred, average='weighted'),\n        'confusion_matrix': confusion_matrix(y_true, y_pred)\n    }\n\ndef string_to_yearmon(date):\n    date = date.split()\n    date = date[0].split('/') + date[1].split(':')\n    date = date[2] + '-' + date[0].zfill(2) #+ '-' + date[1].zfill(2) + ' ' + date[3].zfill(2) + ':' + date[4].zfill(2)\n    return date\n\ndef call_llama(system_prompt, input):\n    template = f\"\"\"&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;{system_prompt}&lt;&lt;/SYS&gt;&gt;{input}[/INST]\"\"\"\n    client = boto3.client(service_name='bedrock-runtime',region_name='us-west-2')\n    body = json.dumps({\n        \"prompt\": template,\n        \"temperature\": 0.,\n        \"top_p\": 0.9,\n        \"max_gen_len\": 2048,\n    })\n    response = client.invoke_model(\n        body=body,\n        modelId='us.meta.llama3-2-90b-instruct-v1:0',\n        accept='application/json',\n        contentType='application/json'\n    )\n    response_body = json.loads(response['body'].read())\n    return response_body\n\ndef call_claude(system_prompt, input):\n    client = boto3.client(service_name='bedrock-runtime',region_name='us-west-2')\n    body=json.dumps(\n        {\n            \"anthropic_version\": \"bedrock-2023-05-31\",\n            \"max_tokens\": 2048,\n            \"messages\": [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": system_prompt + '\\n' + input,\n                    }\n                    ]\n                }\n                ]\n        }  \n    )  \n\n    \n    response = client.invoke_model(body=body, \n                                   modelId='anthropic.claude-3-5-sonnet-20241022-v2:0',\n                                   contentType='application/json',\n                                   accept='application/json')\n    response_body = json.loads(response.get('body').read())\n   \n    return response_body"
  },
  {
    "objectID": "notebook/sales_prediction.html#clean-dataset",
    "href": "notebook/sales_prediction.html#clean-dataset",
    "title": "Predict Zero-inflated and Long-tailed Outcomes",
    "section": "Clean Dataset",
    "text": "Clean Dataset\n\nonline_retail = fetch_ucirepo(id=352) \ntransaction_df = online_retail['data']['original']\ntransaction_df.shape\n\n(541909, 8)\n\n\n\n#create yearmon for train-valid split\ntransaction_df['yearmon'] = transaction_df.InvoiceDate.map(string_to_yearmon)\n\n#get rid of transactions without cid\ntransaction_df = transaction_df[~transaction_df.CustomerID.isna()].reset_index(drop=True)\n\n#fill in unknown descriptions\ntransaction_df.Description = transaction_df.Description.fillna('UNKNOWN')\n\n#convert customer id to string\ntransaction_df['CustomerID'] = transaction_df['CustomerID'].map(lambda x: str(int(x)))\n\ntransaction_df.shape\n\n(406829, 9)\n\n\n\n#check if still na\ntransaction_df.isna().mean()\n\nInvoiceNo      0.0\nStockCode      0.0\nDescription    0.0\nQuantity       0.0\nInvoiceDate    0.0\nUnitPrice      0.0\nCustomerID     0.0\nCountry        0.0\nyearmon        0.0\ndtype: float64\n\n\n\n#simplify by filtering unit price and quantity to be non-zero (get rid of discounts, cancellations, etc)\ntransaction_df = transaction_df[(transaction_df.UnitPrice&gt;0)&\\\n                                (transaction_df.Quantity&gt;0)].reset_index(drop=True)\n#add sales\ntransaction_df['Sales'] = transaction_df.UnitPrice * transaction_df.Quantity\ntransaction_df.shape\n\n(397884, 10)"
  },
  {
    "objectID": "notebook/sales_prediction.html#outcome",
    "href": "notebook/sales_prediction.html#outcome",
    "title": "Predict Zero-inflated and Long-tailed Outcomes",
    "section": "Outcome",
    "text": "Outcome\n\nfeature_period = {'start': '2011-01', 'end': '2011-09'}\noutcome_period = {'start': '2011-10', 'end': '2011-12'}\n\nfeature_transaction = transaction_df[(transaction_df.yearmon&gt;=feature_period['start'])&\\\n                                      (transaction_df.yearmon&lt;=feature_period['end'])]\noutcome_transaction = transaction_df[(transaction_df.yearmon&gt;=outcome_period['start'])&\\\n                                      (transaction_df.yearmon&lt;=outcome_period['end'])]\nfeature_transaction.shape, outcome_transaction.shape\n\n((240338, 10), (131389, 10))\n\n\n\n#aggregate sales during outcome period\noutcome_sales = outcome_transaction.groupby('CustomerID').Sales.sum().reset_index()\noutcome_sales\n\n\n\n\n\n\n\n\nCustomerID\nSales\n\n\n\n\n0\n12347\n1519.14\n\n\n1\n12349\n1757.55\n\n\n2\n12352\n311.73\n\n\n3\n12356\n58.35\n\n\n4\n12357\n6207.67\n\n\n...\n...\n...\n\n\n2555\n18276\n335.86\n\n\n2556\n18277\n110.38\n\n\n2557\n18282\n77.84\n\n\n2558\n18283\n974.21\n\n\n2559\n18287\n1072.00\n\n\n\n\n2560 rows Ã 2 columns\n\n\n\n\n#aggregate sales during feature period\nfeature_sales = feature_transaction.groupby('CustomerID').Sales.sum().reset_index()\nfeature_sales\n\n\n\n\n\n\n\n\nCustomerID\nSales\n\n\n\n\n0\n12346\n77183.60\n\n\n1\n12347\n2079.07\n\n\n2\n12348\n904.44\n\n\n3\n12350\n334.40\n\n\n4\n12352\n2194.31\n\n\n...\n...\n...\n\n\n3433\n18280\n180.60\n\n\n3434\n18281\n80.82\n\n\n3435\n18282\n100.21\n\n\n3436\n18283\n1120.67\n\n\n3437\n18287\n765.28\n\n\n\n\n3438 rows Ã 2 columns\n\n\n\n\n#merge to get TargetSales including those who spent during feature period but not during outcome (zeroes)\noutcome_df = feature_sales[['CustomerID']].merge(outcome_sales, on='CustomerID', how='left')\noutcome_df['Sales'] = outcome_df['Sales'].fillna(0)\noutcome_df.columns = ['CustomerID', 'TargetSales']\noutcome_df\n\n\n\n\n\n\n\n\nCustomerID\nTargetSales\n\n\n\n\n0\n12346\n0.00\n\n\n1\n12347\n1519.14\n\n\n2\n12348\n0.00\n\n\n3\n12350\n0.00\n\n\n4\n12352\n311.73\n\n\n...\n...\n...\n\n\n3433\n18280\n0.00\n\n\n3434\n18281\n0.00\n\n\n3435\n18282\n77.84\n\n\n3436\n18283\n974.21\n\n\n3437\n18287\n1072.00\n\n\n\n\n3438 rows Ã 2 columns\n\n\n\n\n#confirm zero-inflated, long-tailed\noutcome_df.TargetSales.describe(percentiles=[i/10 for i in range(10)])\n\ncount      3438.000000\nmean        666.245829\nstd        4016.843037\nmin           0.000000\n0%            0.000000\n10%           0.000000\n20%           0.000000\n30%           0.000000\n40%           0.000000\n50%         102.005000\n60%         263.006000\n70%         425.790000\n80%         705.878000\n90%        1273.611000\nmax      168469.600000\nName: TargetSales, dtype: float64\n\n\n\n#confirm zero-inflated, long-tailed\noutcome_df[outcome_df.TargetSales&lt;=10_000].TargetSales.hist(bins=100)"
  },
  {
    "objectID": "notebook/sales_prediction.html#feature",
    "href": "notebook/sales_prediction.html#feature",
    "title": "Predict Zero-inflated and Long-tailed Outcomes",
    "section": "Feature",
    "text": "Feature\n\nClassify Description into Category\n\nfeature_transaction.Description.nunique()\n\n3548\n\n\n\nGet Category\n\ndescriptions = feature_transaction.Description.unique().tolist()\nprint(descriptions[:5])\n\n#randomize descriptions with seed 112 to get which categories we should use\nnp.random.seed(112)\nrandom_descriptions = np.random.choice(descriptions, 1000, replace=False)\nprint(random_descriptions[:5])\n\n['JUMBO BAG PINK POLKADOT', 'BLUE POLKADOT WRAP', 'RED RETROSPOT WRAP ', 'RECYCLING BAG RETROSPOT ', 'RED RETROSPOT SHOPPER BAG']\n['MODERN FLORAL STATIONERY SET' 'PURPLE BERTIE GLASS BEAD BAG CHARM'\n 'PARTY INVITES SPACEMAN' 'MONTANA DIAMOND CLUSTER EARRINGS'\n 'SKULLS  DESIGN  COTTON TOTE BAG']\n\n\n\n# res = call_llama(\n#     'You are a product categorization assistant at a retail website.',\n#     'Given the following product descriptions, come up with a few product categories they should be classified into.'+'\\n'.join(random_descriptions)\n#     )\n\n# print(res['generation'])\n\n\n# res\n\n\n# res = call_claude(\n#     'You are a product categorization assistant at a retail website.',\n#     'Given the following product descriptions, come up with a few product categories they should be classified into.'+'\\n'.join(random_descriptions)\n#     )\n\n# print(res['content'][0]['text'])\n\n\n# res\n\nLLaMA 3.2 90B Output:\n&lt;&lt;SYS&gt;&gt;Based on the product descriptions, I would categorize them into the following categories:\n\n1. Home Decor:\n    * Wall art\n    * Decorative items (e.g. vases, figurines, etc.)\n    * Lighting (e.g. candles, lanterns, etc.)\n    * Textiles (e.g. throw pillows, blankets, etc.)\n2. Kitchen and Dining:\n    * Cookware and utensils\n    * Tableware (e.g. plates, cups, etc.)\n    * Kitchen decor (e.g. signs, magnets, etc.)\n    * Food and drink items (e.g. tea, coffee, etc.)\n3. Fashion and Accessories:\n    * Jewelry (e.g. necklaces, earrings, etc.)\n    * Handbags and wallets\n    * Clothing and accessories (e.g. scarves, hats, etc.)\n4. Stationery and Gifts:\n    * Cards and gift wrap\n    * Stationery (e.g. notebooks, pens, etc.)\n    * Gift items (e.g. mugs, keychains, etc.)\n5. Toys and Games:\n    * Toys (e.g. stuffed animals, puzzles, etc.)\n    * Games and puzzles\n6. Seasonal and Holiday:\n    * Christmas decorations and gifts\n    * Easter decorations and gifts\n    * Other seasonal items (e.g. Halloween, etc.)\n7. Personal Care and Wellness:\n    * Beauty and personal care items (e.g. skincare, haircare, etc.)\n    * Wellness and self-care items (e.g. essential oils, etc.)\n8. Outdoor and Garden:\n    * Garden decor and accessories\n    * Outdoor furniture and decor\n    * Gardening tools and supplies\n\nNote that some products may fit into multiple categories, but I have assigned them to the one that seems most relevant.\nClaude 3.5 v2 Output\nBased on these product descriptions, I would suggest the following main product categories:\n\n1. Home Decor\n- Candle holders\n- Picture frames\n- Wall art & signs\n- Clocks\n- Cushions & covers\n- Storage items\n- Decorative objects\n\n2. Jewelry & Accessories\n- Necklaces\n- Bracelets\n- Earrings\n- Hair accessories\n- Bag charms\n- Key rings\n\n3. Garden & Outdoor\n- Plant pots\n- Garden tools\n- Outdoor decorations\n- Bird houses\n- Garden markers\n\n4. Kitchen & Dining\n- Tea sets\n- Mugs\n- Kitchen storage\n- Cutlery\n- Baking accessories\n- Tea towels\n\n5. Stationery & Paper Goods\n- Notebooks\n- Gift wrap\n- Cards\n- Paper decorations\n- Writing sets\n\n6. Party & Celebrations\n- Party supplies\n- Gift bags\n- Christmas decorations\n- Easter items\n- Birthday items\n\n7. Children's Items\n- Toys\n- Children's tableware\n- School supplies\n- Kids' accessories\n\n8. Fashion Accessories\n- Bags\n- Purses\n- Scarves\n- Travel accessories\n\n9. Bath & Beauty\n- Bathroom accessories\n- Toiletry bags\n- Beauty items\n\n10. Lighting\n- Lamps\n- String lights\n- Tea lights\n- Lanterns\n\nThese categories cover the main types of products in the list while providing logical groupings for customers to browse.\n\ncategories = [\n    'Home Decor',\n    'Kitchen and Dining',\n    'Fashion Accessories',\n    'Stationary and Gifts',\n    'Toys and Games',\n    'Seasonal and Holiday',\n    'Personal Care and Wellness',\n    'Outdoor and Garden',   \n]\n\nlen(categories)\n\n8\n\n\n\n\nAnnotate Category to Description\n\n# #loop through descriptions in batches of batch_size\n# res_texts = []\n# batch_size = 100\n# for i in tqdm(range(0, len(descriptions), batch_size)):\n#     batch = descriptions[i:i+batch_size]\n#     d = \"\\n\".join(batch)\n#     inp = f'''Categorize the following product descriptions into {\", \".join(categories)} or Others, if they do not fall into any. \n# Only answer in the following format:\n\n# \"product description of product #1\"|\"product category classified into\"\n# \"product description of product #2\"|\"product category classified into\"\n# ...\n# \"product description of product #n\"|\"product category classified into\"\n\n# Here are the product descriptions:\n# {d}\n# '''\n#     while True:\n#         res = call_claude('You are a product categorizer at a retail website', inp)\n#         # if res['generation_token_count'] &gt; 1: #for llama\n#         if res['usage']['output_tokens'] &gt; 1:\n#             break\n#         else:\n#             print('Retrying...')\n#             time.sleep(2)\n#     res_text = res['content'][0]['text'].strip().split('\\n')\n#         #for llama\n#         # .replace('[SYS]','').replace('&lt;&lt;SYS&gt;&gt;','')\\\n#         # .replace('[/SYS]','').replace('&lt;&lt;/SYS&gt;&gt;','')\\\n#     if res_text!='':\n#         res_texts.extend(res_text)\n\n\n# with open('../data/sales_prediction/product_description_category.csv','w') as f:\n#     f.write('\"product_description\"|\"category\"\\n')\n#     for i in res_texts:\n#         f.write(f'{i}\\n')\n\n\nproduct_description_category = pd.read_csv('../data/sales_prediction/product_description_category.csv',\n                                           sep='|')\n\n#clean product_description\nproduct_description_category['Description'] = descriptions\nproduct_description_category.category.value_counts(normalize=True)\n\ncategory\nHome Decor                    0.328636\nKitchen and Dining            0.195885\nFashion Accessories           0.138670\nStationary and Gifts          0.116122\nSeasonal and Holiday          0.087373\nPersonal Care and Wellness    0.047351\nToys and Games                0.045096\nOutdoor and Garden            0.032976\nOthers                        0.007892\nName: proportion, dtype: float64\n\n\n\nfeature_transaction_cat = feature_transaction.merge(product_description_category,\n                                                    how='inner',\n                                                    on = 'Description',)\nfeature_transaction.shape, feature_transaction_cat.shape\n\n((240338, 10), (240338, 12))\n\n\n\n\n\nRFM\n\n#convert invoice date to datetime\nfeature_transaction_cat['InvoiceDate'] = pd.to_datetime(feature_transaction_cat['InvoiceDate'])\n\n# last date in feature set\ncurrent_date = feature_transaction_cat['InvoiceDate'].max()\n\n#rfm\ncustomer_features = feature_transaction_cat.groupby('CustomerID').agg({\n    'InvoiceDate': [\n        ('recency', lambda x: (current_date - x.max()).days),\n        ('first_purchase_date', 'min'),\n        ('purchase_day', 'nunique'),\n    ],\n    'InvoiceNo': [('nb_invoice', 'nunique')],\n    'Sales': [\n        ('total_sales', 'sum')\n    ],\n    'StockCode': [('nb_product', 'nunique')],\n    'category': [('nb_category', 'nunique')]\n}).reset_index()\n\n# Flatten column names\ncustomer_features.columns = [\n    'CustomerID',\n    'recency',\n    'first_purchase_date',\n    'purchase_day',\n    'nb_invoice',\n    'total_sales',\n    'nb_product',\n    'nb_category'\n]\n\n\n#almost always one purchase a day\n(customer_features.purchase_day==customer_features.nb_invoice).mean()\n\n0.977021524141943\n\n\n\ncustomer_features['customer_lifetime'] = (current_date - customer_features['first_purchase_date']).dt.days\ncustomer_features['avg_purchase_frequency'] = customer_features['customer_lifetime'] / customer_features['purchase_day']\ncustomer_features['avg_purchase_value'] = customer_features['total_sales'] / customer_features['purchase_day']\n\n\n\nCategory Preference\n\n#category preference\ncategory_sales = feature_transaction_cat.pivot_table(\n    values='Sales', \n    index='CustomerID', \n    columns='category', \n    aggfunc='sum', \n    fill_value=0\n)\ncategory_sales.columns = [i.lower().replace(' ','_') for i in category_sales.columns]\ncustomer_features = customer_features.merge(category_sales, on='CustomerID', how='left')\n\ntotal_sales = customer_features['total_sales']\nfor col in category_sales.columns:\n    percentage_col = f'per_{col}'\n    customer_features[percentage_col] = customer_features[col] / total_sales\n\n\n#make sure the categories are not too sparse\n(customer_features.iloc[:,-9:]==0).mean()\n\nper_fashion_accessories           0.409831\nper_home_decor                    0.081734\nper_kitchen_and_dining            0.122455\nper_others                        0.765561\nper_outdoor_and_garden            0.507853\nper_personal_care_and_wellness    0.448226\nper_seasonal_and_holiday          0.369401\nper_stationary_and_gifts          0.305410\nper_toys_and_games                0.487202\ndtype: float64\n\n\n\n\nPutting Them All Together\n\nselected_features = [\n 'recency',\n 'purchase_day',\n 'total_sales',\n 'nb_product',\n 'nb_category',\n 'customer_lifetime',\n 'avg_purchase_frequency',\n 'avg_purchase_value',\n 'per_fashion_accessories',\n 'per_home_decor',\n 'per_kitchen_and_dining',\n 'per_others',\n 'per_outdoor_and_garden',\n 'per_personal_care_and_wellness',\n 'per_seasonal_and_holiday',\n 'per_stationary_and_gifts',\n 'per_toys_and_games']\n\noutcome_variable = 'TargetSales'\n\n\ncustomer_features = customer_features[[ 'CustomerID']+selected_features]\ncustomer_features.head()\n\n\n\n\n\n\n\n\nCustomerID\nrecency\npurchase_day\ntotal_sales\nnb_product\nnb_category\ncustomer_lifetime\navg_purchase_frequency\navg_purchase_value\nper_fashion_accessories\nper_home_decor\nper_kitchen_and_dining\nper_others\nper_outdoor_and_garden\nper_personal_care_and_wellness\nper_seasonal_and_holiday\nper_stationary_and_gifts\nper_toys_and_games\n\n\n\n\n0\n12346\n255\n1\n77183.60\n1\n1\n255\n255.000000\n77183.600000\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n1\n12347\n59\n4\n2079.07\n65\n7\n247\n61.750000\n519.767500\n0.145834\n0.204168\n0.294021\n0.000000\n0.005628\n0.147614\n0.000000\n0.073013\n0.129721\n\n\n2\n12348\n5\n3\n904.44\n10\n4\n248\n82.666667\n301.480000\n0.000000\n0.000000\n0.000000\n0.132679\n0.000000\n0.825970\n0.018796\n0.022555\n0.000000\n\n\n3\n12350\n239\n1\n334.40\n17\n7\n239\n239.000000\n334.400000\n0.240431\n0.202751\n0.116926\n0.172548\n0.000000\n0.118421\n0.000000\n0.059211\n0.089713\n\n\n4\n12352\n2\n7\n2194.31\n47\n8\n226\n32.285714\n313.472857\n0.000000\n0.196531\n0.246187\n0.474090\n0.013535\n0.016680\n0.008066\n0.024404\n0.020508"
  },
  {
    "objectID": "notebook/sales_prediction.html#merge-features-and-outcome",
    "href": "notebook/sales_prediction.html#merge-features-and-outcome",
    "title": "Predict Zero-inflated and Long-tailed Outcomes",
    "section": "Merge Features and Outcome",
    "text": "Merge Features and Outcome\n\ncustomer_features.shape, outcome_df.shape\n\n((3438, 18), (3438, 2))\n\n\n\ndf = outcome_df.merge(customer_features, on='CustomerID').drop('CustomerID', axis=1)\ndf.shape\n\n(3438, 18)\n\n\n\n#correlations\ndf.iloc[:,1:].corr()\n\n\n\n\n\n\n\n\nrecency\npurchase_day\ntotal_sales\nnb_product\nnb_category\ncustomer_lifetime\navg_purchase_frequency\navg_purchase_value\nper_fashion_accessories\nper_home_decor\nper_kitchen_and_dining\nper_others\nper_outdoor_and_garden\nper_personal_care_and_wellness\nper_seasonal_and_holiday\nper_stationary_and_gifts\nper_toys_and_games\n\n\n\n\nrecency\n1.000000\n-0.299308\n-0.132344\n-0.287415\n-0.326772\n0.298853\n0.893973\n0.008823\n-0.020861\n0.022013\n0.057244\n-0.016069\n0.071268\n-0.082792\n-0.085681\n-0.017813\n-0.009686\n\n\npurchase_day\n-0.299308\n1.000000\n0.540253\n0.690345\n0.304621\n0.332109\n-0.331543\n0.027488\n0.030683\n0.018684\n0.025269\n0.004299\n-0.019992\n-0.035665\n-0.020392\n-0.045384\n-0.028187\n\n\ntotal_sales\n-0.132344\n0.540253\n1.000000\n0.400467\n0.137064\n0.156018\n-0.148762\n0.361138\n0.016511\n-0.013819\n0.047834\n0.006398\n-0.029353\n-0.011937\n-0.016724\n-0.029181\n-0.013139\n\n\nnb_product\n-0.287415\n0.690345\n0.400467\n1.000000\n0.555551\n0.265594\n-0.294923\n0.061039\n-0.003137\n-0.017516\n0.035615\n-0.006842\n-0.026371\n-0.005309\n-0.016586\n0.026716\n-0.010069\n\n\nnb_category\n-0.326772\n0.304621\n0.137064\n0.555551\n1.000000\n0.224232\n-0.321596\n0.019955\n0.004863\n-0.138372\n-0.039363\n0.055555\n0.041405\n0.075882\n0.015498\n0.152869\n0.111150\n\n\ncustomer_lifetime\n0.298853\n0.332109\n0.156018\n0.265594\n0.224232\n1.000000\n0.358431\n0.014933\n0.011220\n0.066111\n0.069175\n-0.019971\n0.029726\n-0.127865\n-0.120399\n-0.050320\n-0.036484\n\n\navg_purchase_frequency\n0.893973\n-0.331543\n-0.148762\n-0.294923\n-0.321596\n0.358431\n1.000000\n0.009157\n-0.016093\n0.027208\n0.037053\n-0.027413\n0.060369\n-0.070352\n-0.074799\n-0.000546\n-0.010612\n\n\navg_purchase_value\n0.008823\n0.027488\n0.361138\n0.061039\n0.019955\n0.014933\n0.009157\n1.000000\n-0.003187\n-0.056690\n0.076862\n0.015427\n-0.028884\n0.004225\n-0.000200\n-0.012729\n-0.002396\n\n\nper_fashion_accessories\n-0.020861\n0.030683\n0.016511\n-0.003137\n0.004863\n0.011220\n-0.016093\n-0.003187\n1.000000\n-0.254015\n-0.177775\n-0.010436\n-0.082834\n-0.038493\n-0.124719\n-0.068166\n-0.051486\n\n\nper_home_decor\n0.022013\n0.018684\n-0.013819\n-0.017516\n-0.138372\n0.066111\n0.027208\n-0.056690\n-0.254015\n1.000000\n-0.481983\n-0.155784\n-0.080637\n-0.158837\n-0.165964\n-0.262313\n-0.245759\n\n\nper_kitchen_and_dining\n0.057244\n0.025269\n0.047834\n0.035615\n-0.039363\n0.069175\n0.037053\n0.076862\n-0.177775\n-0.481983\n1.000000\n-0.013075\n-0.144698\n-0.117031\n-0.204235\n-0.173386\n-0.143931\n\n\nper_others\n-0.016069\n0.004299\n0.006398\n-0.006842\n0.055555\n-0.019971\n-0.027413\n0.015427\n-0.010436\n-0.155784\n-0.013075\n1.000000\n-0.062652\n0.014794\n-0.047940\n-0.033975\n-0.040421\n\n\nper_outdoor_and_garden\n0.071268\n-0.019992\n-0.029353\n-0.026371\n0.041405\n0.029726\n0.060369\n-0.028884\n-0.082834\n-0.080637\n-0.144698\n-0.062652\n1.000000\n-0.045639\n-0.077947\n-0.057297\n-0.001034\n\n\nper_personal_care_and_wellness\n-0.082792\n-0.035665\n-0.011937\n-0.005309\n0.075882\n-0.127865\n-0.070352\n0.004225\n-0.038493\n-0.158837\n-0.117031\n0.014794\n-0.045639\n1.000000\n-0.057926\n-0.025871\n-0.017022\n\n\nper_seasonal_and_holiday\n-0.085681\n-0.020392\n-0.016724\n-0.016586\n0.015498\n-0.120399\n-0.074799\n-0.000200\n-0.124719\n-0.165964\n-0.204235\n-0.047940\n-0.077947\n-0.057926\n1.000000\n-0.019418\n-0.042970\n\n\nper_stationary_and_gifts\n-0.017813\n-0.045384\n-0.029181\n0.026716\n0.152869\n-0.050320\n-0.000546\n-0.012729\n-0.068166\n-0.262313\n-0.173386\n-0.033975\n-0.057297\n-0.025871\n-0.019418\n1.000000\n0.172039\n\n\nper_toys_and_games\n-0.009686\n-0.028187\n-0.013139\n-0.010069\n0.111150\n-0.036484\n-0.010612\n-0.002396\n-0.051486\n-0.245759\n-0.143931\n-0.040421\n-0.001034\n-0.017022\n-0.042970\n0.172039\n1.000000\n\n\n\n\n\n\n\n\n#target and most predictive variable\ndf[df.TargetSales&lt;=25_000].plot.scatter(x='TargetSales',y='total_sales')"
  },
  {
    "objectID": "notebook/sales_prediction.html#train-test-splits",
    "href": "notebook/sales_prediction.html#train-test-splits",
    "title": "Predict Zero-inflated and Long-tailed Outcomes",
    "section": "Train-Test Splits",
    "text": "Train-Test Splits\n\n#split into train-valid sets\ntrain_df, test_df = train_test_split(df,\n                                      test_size=0.2, \n                                      random_state=112)\n\n\npd.concat([train_df.TargetSales.describe(percentiles=[i/10 for i in range(10)]).reset_index(),\ntest_df.TargetSales.describe(percentiles=[i/10 for i in range(10)]).reset_index(),], axis=1)\n\n\n\n\n\n\n\n\nindex\nTargetSales\nindex\nTargetSales\n\n\n\n\n0\ncount\n2750.000000\ncount\n688.000000\n\n\n1\nmean\n642.650436\nmean\n760.558808\n\n\n2\nstd\n4015.305436\nstd\n4024.524400\n\n\n3\nmin\n0.000000\nmin\n0.000000\n\n\n4\n0%\n0.000000\n0%\n0.000000\n\n\n5\n10%\n0.000000\n10%\n0.000000\n\n\n6\n20%\n0.000000\n20%\n0.000000\n\n\n7\n30%\n0.000000\n30%\n0.000000\n\n\n8\n40%\n0.000000\n40%\n0.000000\n\n\n9\n50%\n91.350000\n50%\n113.575000\n\n\n10\n60%\n260.308000\n60%\n277.836000\n\n\n11\n70%\n426.878000\n70%\n418.187000\n\n\n12\n80%\n694.164000\n80%\n759.582000\n\n\n13\n90%\n1272.997000\n90%\n1255.670000\n\n\n14\nmax\n168469.600000\nmax\n77099.380000"
  },
  {
    "objectID": "notebook/sales_prediction.html#baseline-regression",
    "href": "notebook/sales_prediction.html#baseline-regression",
    "title": "Predict Zero-inflated and Long-tailed Outcomes",
    "section": "Baseline Regression",
    "text": "Baseline Regression\n\npredictor = TabularPredictor(label='TargetSales').fit(train_df[selected_features + ['TargetSales']], \n                                                      presets='medium_quality',\n                                                      num_gpus=8,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20241122_052014\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.1.1\nPython Version:     3.9.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP Wed Oct 23 01:22:11 UTC 2024\nCPU Count:          64\nMemory Avail:       468.90 GB / 480.23 GB (97.6%)\nDisk Space Avail:   1553.14 GB / 1968.52 GB (78.9%)\n===================================================\nPresets specified: ['medium_quality']\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20241122_052014\"\nTrain Data Rows:    2750\nTrain Data Columns: 17\nLabel Column:       TargetSales\nAutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n    Label info (max, min, mean, stddev): (168469.6, 0.0, 642.65044, 4015.30544)\n    If 'regression' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\nProblem Type:       regression\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    480152.27 MB\n    Train Data (Original)  Memory Usage: 0.36 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    0.0s = Fit runtime\n    17 features in original data used to generate 17 features in processed data.\n    Train Data (Processed) Memory Usage: 0.36 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.06s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.18181818181818182, Train Rows: 2250, Val Rows: 500\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nExcluded models: ['FASTAI', 'KNN', 'NN_TORCH'] (Specified by `excluded_model_types`)\nFitting 7 L1 models ...\nFitting model: LightGBMXT ...\n    Training LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1\nWarning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:  pip uninstall lightgbm -y   pip install lightgbm --install-option=--gpu\n    -1003.3582   = Validation score   (-root_mean_squared_error)\n    0.6s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    Training LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1\nWarning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:  pip uninstall lightgbm -y   pip install lightgbm --install-option=--gpu\n    -928.536     = Validation score   (-root_mean_squared_error)\n    0.6s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestMSE ...\n    -887.6823    = Validation score   (-root_mean_squared_error)\n    0.8s     = Training   runtime\n    0.1s     = Validation runtime\nFitting model: CatBoost ...\n    Training CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n    Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n    -912.3919    = Validation score   (-root_mean_squared_error)\n    1.8s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesMSE ...\n    -974.1369    = Validation score   (-root_mean_squared_error)\n    0.7s     = Training   runtime\n    0.09s    = Validation runtime\nFitting model: XGBoost ...\n/home/charipol/miniconda3/lib/python3.9/site-packages/xgboost/core.py:160: UserWarning: [05:20:19] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/home/charipol/miniconda3/lib/python3.9/site-packages/xgboost/core.py:160: UserWarning: [05:20:20] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n    -816.8577    = Validation score   (-root_mean_squared_error)\n    1.0s     = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMLarge ...\n    Training LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1\nWarning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:  pip uninstall lightgbm -y   pip install lightgbm --install-option=--gpu\n    -969.4897    = Validation score   (-root_mean_squared_error)\n    1.61s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    Ensemble Weights: {'XGBoost': 0.478, 'RandomForestMSE': 0.304, 'LightGBMXT': 0.217}\n    -766.1795    = Validation score   (-root_mean_squared_error)\n    0.02s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 7.61s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 4475.4 rows/s (500 batch size)\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241122_052014\")\n\n\n\ntest_df['pred_baseline'] = predictor.predict(test_df[selected_features])\n\n\ncalculate_regression_metrics(test_df['TargetSales'], test_df['pred_baseline'])\n\n{'root_mean_squared_error': 3535.0456293403095,\n 'mean_squared_error': 12496547.601518024,\n 'mean_absolute_error': 733.2916506070869,\n 'r2': 0.22733254613537157,\n 'pearsonr': 0.4889487555329212,\n 'median_absolute_error': 351.8167266845703}"
  },
  {
    "objectID": "notebook/sales_prediction.html#regression-on-winsorized-outcome",
    "href": "notebook/sales_prediction.html#regression-on-winsorized-outcome",
    "title": "Predict Zero-inflated and Long-tailed Outcomes",
    "section": "Regression on Winsorized Outcome",
    "text": "Regression on Winsorized Outcome\n\noutlier_per = 0.99\noutlier_cap_train = train_df['TargetSales'].quantile(outlier_per)\noutlier_cap_train\n\n7180.805199999947\n\n\n\n#winsorize\ntrain_df['TargetSales_win'] = train_df['TargetSales'].map(lambda x: outlier_cap_train if x&gt; outlier_cap_train else x)\ntest_df['TargetSales_win'] = test_df['TargetSales'].map(lambda x: outlier_cap_train if x&gt; outlier_cap_train else x)\n\n\npredictor = TabularPredictor(label='TargetSales_win').fit(train_df[selected_features+['TargetSales_win']],\n                                                      presets='medium_quality',\n                                                      num_gpus=8,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20241122_052148\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.1.1\nPython Version:     3.9.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP Wed Oct 23 01:22:11 UTC 2024\nCPU Count:          64\nMemory Avail:       468.79 GB / 480.23 GB (97.6%)\nDisk Space Avail:   1553.06 GB / 1968.52 GB (78.9%)\n===================================================\nPresets specified: ['medium_quality']\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20241122_052148\"\nTrain Data Rows:    2750\nTrain Data Columns: 17\nLabel Column:       TargetSales_win\nAutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n    Label info (max, min, mean, stddev): (7180.805199999947, 0.0, 474.91365, 994.46633)\n    If 'regression' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\nProblem Type:       regression\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    480039.65 MB\n    Train Data (Original)  Memory Usage: 0.36 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    0.0s = Fit runtime\n    17 features in original data used to generate 17 features in processed data.\n    Train Data (Processed) Memory Usage: 0.36 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.06s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.18181818181818182, Train Rows: 2250, Val Rows: 500\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nExcluded models: ['FASTAI', 'KNN', 'NN_TORCH'] (Specified by `excluded_model_types`)\nFitting 7 L1 models ...\nFitting model: LightGBMXT ...\n    Training LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\nWarning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:  pip uninstall lightgbm -y   pip install lightgbm --install-option=--gpu\n[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1\n    -612.1193    = Validation score   (-root_mean_squared_error)\n    0.71s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    Training LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1\nWarning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:  pip uninstall lightgbm -y   pip install lightgbm --install-option=--gpu\n    -606.6121    = Validation score   (-root_mean_squared_error)\n    0.87s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestMSE ...\n    -594.4683    = Validation score   (-root_mean_squared_error)\n    0.8s     = Training   runtime\n    0.1s     = Validation runtime\nFitting model: CatBoost ...\n    Training CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n    Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n    -569.0171    = Validation score   (-root_mean_squared_error)\n    2.73s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesMSE ...\n    -594.7243    = Validation score   (-root_mean_squared_error)\n    0.72s    = Training   runtime\n    0.1s     = Validation runtime\nFitting model: XGBoost ...\n/home/charipol/miniconda3/lib/python3.9/site-packages/xgboost/core.py:160: UserWarning: [05:21:54] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/home/charipol/miniconda3/lib/python3.9/site-packages/xgboost/core.py:160: UserWarning: [05:21:55] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n    -587.4133    = Validation score   (-root_mean_squared_error)\n    1.11s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: LightGBMLarge ...\n    Training LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1\nWarning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:  pip uninstall lightgbm -y   pip install lightgbm --install-option=--gpu\n    -586.8863    = Validation score   (-root_mean_squared_error)\n    2.03s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    Ensemble Weights: {'CatBoost': 0.632, 'XGBoost': 0.263, 'RandomForestMSE': 0.105}\n    -563.7135    = Validation score   (-root_mean_squared_error)\n    0.02s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 9.55s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 4052.1 rows/s (500 batch size)\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241122_052148\")\n\n\n\ntest_df['pred_winsorized'] = predictor.predict(test_df[selected_features])\n\n\ncalculate_regression_metrics(test_df['TargetSales'], test_df['pred_winsorized'])\n\n{'root_mean_squared_error': 3621.2890030725234,\n 'mean_squared_error': 13113734.043773992,\n 'mean_absolute_error': 632.4055458614438,\n 'r2': 0.18917161624464252,\n 'pearsonr': 0.5745161754376499,\n 'median_absolute_error': 224.00707733154297}\n\n\n\ncalculate_regression_metrics(test_df['TargetSales_win'], test_df['pred_winsorized'])\n\n{'root_mean_squared_error': 677.156973523447,\n 'mean_squared_error': 458541.5667914344,\n 'mean_absolute_error': 381.3935720242338,\n 'r2': 0.6129909348661391,\n 'pearsonr': 0.7834868276613383,\n 'median_absolute_error': 223.31085205078125}"
  },
  {
    "objectID": "notebook/sales_prediction.html#log1p-regression",
    "href": "notebook/sales_prediction.html#log1p-regression",
    "title": "Predict Zero-inflated and Long-tailed Outcomes",
    "section": "Log1p Regression",
    "text": "Log1p Regression\n\n#log\ntrain_df['TargetSales_log1p'] = train_df['TargetSales'].map(np.log1p)\ntest_df['TargetSales_log1p'] = test_df['TargetSales'].map(np.log1p)\n\n\n#from zero-inflated to one-inflated\ntrain_df['TargetSales_log1p'].hist()\n\n\n\n\n\n\n\n\n\npredictor = TabularPredictor(label='TargetSales_log1p').fit(train_df[selected_features+['TargetSales_log1p']],\n                                                      presets='medium_quality',\n                                                      num_gpus=8,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20241122_052230\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.1.1\nPython Version:     3.9.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP Wed Oct 23 01:22:11 UTC 2024\nCPU Count:          64\nMemory Avail:       468.40 GB / 480.23 GB (97.5%)\nDisk Space Avail:   1552.98 GB / 1968.52 GB (78.9%)\n===================================================\nPresets specified: ['medium_quality']\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20241122_052230\"\nTrain Data Rows:    2750\nTrain Data Columns: 17\nLabel Column:       TargetSales_log1p\nAutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n    Label info (max, min, mean, stddev): (12.034516532838857, 0.0, 3.24748, 3.24795)\n    If 'regression' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\nProblem Type:       regression\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    479637.48 MB\n    Train Data (Original)  Memory Usage: 0.36 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    0.1s = Fit runtime\n    17 features in original data used to generate 17 features in processed data.\n    Train Data (Processed) Memory Usage: 0.36 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.09s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.18181818181818182, Train Rows: 2250, Val Rows: 500\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nExcluded models: ['FASTAI', 'KNN', 'NN_TORCH'] (Specified by `excluded_model_types`)\nFitting 7 L1 models ...\nFitting model: LightGBMXT ...\n    Training LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1\nWarning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:  pip uninstall lightgbm -y   pip install lightgbm --install-option=--gpu\n    -2.7983  = Validation score   (-root_mean_squared_error)\n    0.66s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    Training LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1\nWarning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:  pip uninstall lightgbm -y   pip install lightgbm --install-option=--gpu\n    -2.8188  = Validation score   (-root_mean_squared_error)\n    0.75s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestMSE ...\n    -2.8521  = Validation score   (-root_mean_squared_error)\n    0.88s    = Training   runtime\n    0.09s    = Validation runtime\nFitting model: CatBoost ...\n    Training CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n    Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n    -2.7665  = Validation score   (-root_mean_squared_error)\n    2.22s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesMSE ...\n    -2.8213  = Validation score   (-root_mean_squared_error)\n    0.66s    = Training   runtime\n    0.09s    = Validation runtime\nFitting model: XGBoost ...\n/home/charipol/miniconda3/lib/python3.9/site-packages/xgboost/core.py:160: UserWarning: [05:22:36] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/home/charipol/miniconda3/lib/python3.9/site-packages/xgboost/core.py:160: UserWarning: [05:22:37] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n    -2.827   = Validation score   (-root_mean_squared_error)\n    1.14s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMLarge ...\n    Training LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1\nWarning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:  pip uninstall lightgbm -y   pip install lightgbm --install-option=--gpu\n    -2.8638  = Validation score   (-root_mean_squared_error)\n    1.72s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    Ensemble Weights: {'CatBoost': 1.0}\n    -2.7665  = Validation score   (-root_mean_squared_error)\n    0.02s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 8.57s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 120249.5 rows/s (500 batch size)\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241122_052230\")\n\n\n\ntest_df['pred_log1p'] = predictor.predict(test_df[selected_features])\ntest_df['pred_log1p_expm1'] = test_df['pred_log1p'].map(np.expm1)\n\n\ncalculate_regression_metrics(test_df['TargetSales'], test_df['pred_log1p_expm1'])\n\n{'root_mean_squared_error': 3842.2757199198686,\n 'mean_squared_error': 14763082.707885744,\n 'mean_absolute_error': 649.181994311652,\n 'r2': 0.08719160756773026,\n 'pearsonr': 0.5800264158426427,\n 'median_absolute_error': 88.13328933411897}\n\n\n\ncalculate_regression_metrics(test_df['TargetSales_log1p'], test_df['pred_log1p'])\n\n{'root_mean_squared_error': 2.728486520952067,\n 'mean_squared_error': 7.444638695017115,\n 'mean_absolute_error': 2.4563614303989136,\n 'r2': 0.29819311146909255,\n 'pearsonr': 0.549298106584379,\n 'median_absolute_error': 2.3491433179177177}"
  },
  {
    "objectID": "notebook/sales_prediction.html#hurdle-regression",
    "href": "notebook/sales_prediction.html#hurdle-regression",
    "title": "Predict Zero-inflated and Long-tailed Outcomes",
    "section": "Hurdle Regression",
    "text": "Hurdle Regression\n\nBinary Classification\n\ntrain_df['has_purchase'] = train_df.TargetSales.map(lambda x: 1 if x&gt;0 else 0)\ntest_df['has_purchase'] = test_df.TargetSales.map(lambda x: 1 if x&gt;0 else 0)\n\n\ntrain_df['has_purchase'].mean(), test_df['has_purchase'].mean()\n\n(0.5141818181818182, 0.5305232558139535)\n\n\n\npredictor_cls = TabularPredictor(label='has_purchase').fit(train_df[selected_features+['has_purchase']],\n                                                      presets='medium_quality',\n                                                      num_gpus=8,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20241122_054503\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.1.1\nPython Version:     3.9.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP Wed Oct 23 01:22:11 UTC 2024\nCPU Count:          64\nMemory Avail:       467.80 GB / 480.23 GB (97.4%)\nDisk Space Avail:   1552.75 GB / 1968.52 GB (78.9%)\n===================================================\nPresets specified: ['medium_quality']\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20241122_054503\"\nTrain Data Rows:    2750\nTrain Data Columns: 17\nLabel Column:       has_purchase\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n    2 unique label values:  [1, 0]\n    If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\nProblem Type:       binary\nPreprocessing data ...\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    479022.72 MB\n    Train Data (Original)  Memory Usage: 0.36 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    0.0s = Fit runtime\n    17 features in original data used to generate 17 features in processed data.\n    Train Data (Processed) Memory Usage: 0.36 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.06s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.18181818181818182, Train Rows: 2250, Val Rows: 500\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nExcluded models: ['FASTAI', 'KNN', 'NN_TORCH'] (Specified by `excluded_model_types`)\nFitting 9 L1 models ...\nFitting model: LightGBMXT ...\n    Training LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\nWarning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:  pip uninstall lightgbm -y   pip install lightgbm --install-option=--gpu\n[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1\n    0.684    = Validation score   (accuracy)\n    0.75s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    Training LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1\nWarning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:  pip uninstall lightgbm -y   pip install lightgbm --install-option=--gpu\n    0.668    = Validation score   (accuracy)\n    0.81s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestGini ...\n    0.652    = Validation score   (accuracy)\n    0.92s    = Training   runtime\n    0.11s    = Validation runtime\nFitting model: RandomForestEntr ...\n    0.668    = Validation score   (accuracy)\n    0.92s    = Training   runtime\n    0.11s    = Validation runtime\nFitting model: CatBoost ...\n    Training CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n    Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n    0.688    = Validation score   (accuracy)\n    3.9s     = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesGini ...\n    0.69     = Validation score   (accuracy)\n    0.89s    = Training   runtime\n    0.12s    = Validation runtime\nFitting model: ExtraTreesEntr ...\n    0.664    = Validation score   (accuracy)\n    0.88s    = Training   runtime\n    0.1s     = Validation runtime\nFitting model: XGBoost ...\n/home/charipol/miniconda3/lib/python3.9/site-packages/xgboost/core.py:160: UserWarning: [05:45:13] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/home/charipol/miniconda3/lib/python3.9/site-packages/xgboost/core.py:160: UserWarning: [05:45:14] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n    0.658    = Validation score   (accuracy)\n    1.06s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMLarge ...\n    Training LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1\nWarning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:  pip uninstall lightgbm -y   pip install lightgbm --install-option=--gpu\n    0.67     = Validation score   (accuracy)\n    2.09s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    Ensemble Weights: {'ExtraTreesGini': 0.714, 'LightGBMXT': 0.214, 'CatBoost': 0.071}\n    0.698    = Validation score   (accuracy)\n    0.14s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 13.2s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 4174.4 rows/s (500 batch size)\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241122_054503\")\n\n\n\ntest_df['pred_binary'] = predictor_cls.predict(test_df[selected_features])\n\n\ncaluclate_classification_metrics(test_df['has_purchase'], test_df['pred_binary'])\n\n{'accuracy': 0.6875,\n 'precision': 0.6881282151076215,\n 'recall': 0.6875,\n 'f1_score': 0.6876965304761199,\n 'confusion_matrix': array([[220, 103],\n        [112, 253]])}\n\n\n\n\nRegression on Non-Zero Outcome\n\ntrain_df_nonzero = train_df[train_df.has_purchase==1].reset_index(drop=True)\ntest_df_nonzero = test_df[test_df.has_purchase==1].reset_index(drop=True)\n\ntrain_df_nonzero.shape, test_df_nonzero.shape\n\n((1414, 21), (365, 26))\n\n\n\n#log\ntrain_df_nonzero['TargetSales_log'] = train_df_nonzero['TargetSales'].map(np.log)\ntest_df_nonzero['TargetSales_log'] = test_df_nonzero['TargetSales'].map(np.log)\n\n\ntrain_df_nonzero['TargetSales_log'].hist()\n\n\n\n\n\n\n\n\n\npredictor_reg = TabularPredictor(label='TargetSales_log').fit(train_df_nonzero[selected_features+['TargetSales_log']],\n                                                      presets='medium_quality',\n                                                      num_gpus=8,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20241122_054607\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.1.1\nPython Version:     3.9.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP Wed Oct 23 01:22:11 UTC 2024\nCPU Count:          64\nMemory Avail:       467.78 GB / 480.23 GB (97.4%)\nDisk Space Avail:   1552.65 GB / 1968.52 GB (78.9%)\n===================================================\nPresets specified: ['medium_quality']\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20241122_054607\"\nTrain Data Rows:    1414\nTrain Data Columns: 17\nLabel Column:       TargetSales_log\nAutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n    Label info (max, min, mean, stddev): (12.034510597067465, 2.2975725511705014, 6.31275, 1.06643)\n    If 'regression' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\nProblem Type:       regression\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    479007.09 MB\n    Train Data (Original)  Memory Usage: 0.18 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    0.0s = Fit runtime\n    17 features in original data used to generate 17 features in processed data.\n    Train Data (Processed) Memory Usage: 0.18 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.06s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 1131, Val Rows: 283\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nExcluded models: ['FASTAI', 'KNN', 'NN_TORCH'] (Specified by `excluded_model_types`)\nFitting 7 L1 models ...\nFitting model: LightGBMXT ...\n    Training LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1\nWarning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:  pip uninstall lightgbm -y   pip install lightgbm --install-option=--gpu\n    -0.7585  = Validation score   (-root_mean_squared_error)\n    0.77s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBM ...\n    Training LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1\nWarning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:  pip uninstall lightgbm -y   pip install lightgbm --install-option=--gpu\n    -0.7376  = Validation score   (-root_mean_squared_error)\n    0.73s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: RandomForestMSE ...\n    -0.7427  = Validation score   (-root_mean_squared_error)\n    0.67s    = Training   runtime\n    0.1s     = Validation runtime\nFitting model: CatBoost ...\n    Training CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n    Warning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n    -0.7276  = Validation score   (-root_mean_squared_error)\n    3.23s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesMSE ...\n    -0.7481  = Validation score   (-root_mean_squared_error)\n    0.7s     = Training   runtime\n    0.09s    = Validation runtime\nFitting model: XGBoost ...\n/home/charipol/miniconda3/lib/python3.9/site-packages/xgboost/core.py:160: UserWarning: [05:46:13] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/home/charipol/miniconda3/lib/python3.9/site-packages/xgboost/core.py:160: UserWarning: [05:46:14] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n    -0.7365  = Validation score   (-root_mean_squared_error)\n    1.09s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: LightGBMLarge ...\n    Training LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n[LightGBM] [Fatal] GPU Tree Learner was not enabled in this build.\nPlease recompile with CMake option -DUSE_GPU=1\nWarning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:  pip uninstall lightgbm -y   pip install lightgbm --install-option=--gpu\n    -0.775   = Validation score   (-root_mean_squared_error)\n    2.48s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    Ensemble Weights: {'CatBoost': 0.6, 'XGBoost': 0.25, 'LightGBM': 0.15}\n    -0.7242  = Validation score   (-root_mean_squared_error)\n    0.02s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 10.21s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 13543.8 rows/s (283 batch size)\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241122_054607\")\n\n\n\ntest_df_nonzero['pred_log'] = predictor_reg.predict(test_df_nonzero[selected_features])\ntest_df_nonzero['pred_log_exp'] = test_df_nonzero['pred_log'].map(np.exp)\n\ntest_df['pred_log'] = predictor_reg.predict(test_df[selected_features])\ntest_df['pred_log_exp'] = test_df['pred_log'].map(np.exp)\n\n\ncalculate_regression_metrics(test_df_nonzero['TargetSales'], test_df_nonzero['pred_log_exp'])\n\n{'root_mean_squared_error': 4432.526276333969,\n 'mean_squared_error': 19647289.190391082,\n 'mean_absolute_error': 884.6138819058123,\n 'r2': 0.3344550185098929,\n 'pearsonr': 0.6474654951795362,\n 'median_absolute_error': 239.09978802417265}\n\n\n\ntest_df['pred_hurdle'] = test_df.pred_binary * test_df.pred_log_exp\ncalculate_regression_metrics(test_df['TargetSales'], test_df['pred_hurdle'])\n\n{'root_mean_squared_error': 3246.258960109551,\n 'mean_squared_error': 10538197.236091545,\n 'mean_absolute_error': 589.3096792867335,\n 'r2': 0.34841827628096245,\n 'pearsonr': 0.6497011871197678,\n 'median_absolute_error': 197.7694728626407}\n\n\n\n\nDuanâs Method\n\ntrain_df_nonzero['pred_log'] = predictor_reg.predict(train_df_nonzero[selected_features])\ntrain_df_nonzero['pred_log_exp'] = train_df_nonzero['pred_log'].map(np.exp)\n\nsmearing_factor = np.mean(np.exp(train_df_nonzero['TargetSales_log'] - train_df_nonzero['pred_log']))\nsmearing_factor\n\n1.3715872927403208\n\n\n\ntest_df['pred_log_exp_corrected'] = test_df['pred_log_exp'] * smearing_factor\ntest_df['pred_hurdle_corrected'] = test_df.pred_binary * test_df.pred_log_exp_corrected\ncalculate_regression_metrics(test_df['TargetSales'], test_df['pred_hurdle_corrected'])\n\n{'root_mean_squared_error': 3096.3423338211273,\n 'mean_squared_error': 9587335.848212866,\n 'mean_absolute_error': 641.2781956415758,\n 'r2': 0.4072104860158461,\n 'pearsonr': 0.6497011871197678,\n 'median_absolute_error': 269.04780476878705}"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Charin and I am a senior applied scientist at Amazon. My main focus is on estimating heterogeneity effects for customer targeting and personalization, using causal inference techniques and counterfactuals generated by large language models. This is a collection of technical writings I find useful."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "chariblog - technical writings in applied science",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n2024-10-27\n\n\nHarlow Malloc\n\n\n\n\n\n\nNo matching items"
  }
]