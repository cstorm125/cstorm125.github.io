[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "notebook/sales_prediction.html",
    "href": "notebook/sales_prediction.html",
    "title": "Predict Zero-inflated and Long-tailed Outcomes",
    "section": "",
    "text": "This notebook details how to predict a real-number outcome that is zero-inflated and long-tailed such as sales prediction in retail. We provide baseline regression, regression trained using winsorized outcome, regression trained on log(y+1) outcome, and hurdle regression with and without Duan’s method."
  },
  {
    "objectID": "notebook/sales_prediction.html#import",
    "href": "notebook/sales_prediction.html#import",
    "title": "Predict Zero-inflated and Long-tailed Outcomes",
    "section": "Import",
    "text": "Import\n\nimport pandas as pd\nimport numpy as np\nimport random\nfrom ucimlrepo import fetch_ucirepo \nimport boto3\nimport json\nfrom tqdm.auto import tqdm\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom autogluon.tabular import TabularDataset, TabularPredictor\nfrom sklearn.metrics import (\n    mean_squared_error, mean_absolute_error, r2_score, median_absolute_error,\n    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n)\n\nfrom scipy.stats import pearsonr, wasserstein_distance\n\ndef calculate_regression_metrics(y_true, y_pred):\n    return {\n        'root_mean_squared_error': np.sqrt(mean_squared_error(y_true, y_pred)),\n        'mean_squared_error': mean_squared_error(y_true, y_pred),\n        'mean_absolute_error': mean_absolute_error(y_true, y_pred),\n        'r2': r2_score(y_true, y_pred),\n        'pearsonr': pearsonr(y_true, y_pred)[0],  # Pearson correlation coefficient\n        'median_absolute_error': median_absolute_error(y_true, y_pred),\n        'earths_mover_distance': wasserstein_distance(y_true, y_pred)\n    }\n\ndef caluclate_classification_metrics(y_true, y_pred):\n    return {\n        'accuracy': accuracy_score(y_true, y_pred),\n        'precision': precision_score(y_true, y_pred, average='weighted'),\n        'recall': recall_score(y_true, y_pred, average='weighted'),\n        'f1_score': f1_score(y_true, y_pred, average='weighted'),\n        'confusion_matrix': confusion_matrix(y_true, y_pred)\n    }\n\ndef string_to_yearmon(date):\n    date = date.split()\n    date = date[0].split('/') + date[1].split(':')\n    date = date[2] + '-' + date[0].zfill(2) #+ '-' + date[1].zfill(2) + ' ' + date[3].zfill(2) + ':' + date[4].zfill(2)\n    return date\n\ndef call_llama(system_prompt, input):\n    template = f\"\"\"&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;{system_prompt}&lt;&lt;/SYS&gt;&gt;{input}[/INST]\"\"\"\n    client = boto3.client(service_name='bedrock-runtime',region_name='us-west-2')\n    body = json.dumps({\n        \"prompt\": template,\n        \"temperature\": 0.,\n        \"top_p\": 0.9,\n        \"max_gen_len\": 2048,\n    })\n    response = client.invoke_model(\n        body=body,\n        modelId='us.meta.llama3-2-90b-instruct-v1:0',\n        accept='application/json',\n        contentType='application/json'\n    )\n    response_body = json.loads(response['body'].read())\n    return response_body\n\ndef call_claude(system_prompt, input):\n    client = boto3.client(service_name='bedrock-runtime',region_name='us-west-2')\n    body=json.dumps(\n        {\n            \"anthropic_version\": \"bedrock-2023-05-31\",\n            \"max_tokens\": 2048,\n            \"messages\": [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": system_prompt + '\\n' + input,\n                    }\n                    ]\n                }\n                ]\n        }  \n    )  \n\n    \n    response = client.invoke_model(body=body, \n                                   modelId='anthropic.claude-3-5-sonnet-20241022-v2:0',\n                                   contentType='application/json',\n                                   accept='application/json')\n    response_body = json.loads(response.get('body').read())\n   \n    return response_body"
  },
  {
    "objectID": "notebook/sales_prediction.html#dataset",
    "href": "notebook/sales_prediction.html#dataset",
    "title": "Predict Zero-inflated and Long-tailed Outcomes",
    "section": "Dataset",
    "text": "Dataset\nWe use the UCI Online Retail dataset, which are transactions from a UK-based, non-store online retail from 2010-12-01 and 2011-12-09. We perform the following data processing:\n\nRemove transactions without CustomerID; from 541,909 to 406,829 transactions\nFilter out transactions where either UnitPrice or Quantity is less than zero; from 406,829 to 397,884 transactions\nFill in missing product Description with value UNKNOWN.\n\n\nonline_retail = fetch_ucirepo(id=352) \ntransaction_df = online_retail['data']['original']\ntransaction_df.shape\n\n(541909, 8)\n\n\n\n#create yearmon for train-valid split\ntransaction_df['yearmon'] = transaction_df.InvoiceDate.map(string_to_yearmon)\n\n#get rid of transactions without cid\ntransaction_df = transaction_df[~transaction_df.CustomerID.isna()].reset_index(drop=True)\n\n#fill in unknown descriptions\ntransaction_df.Description = transaction_df.Description.fillna('UNKNOWN')\n\n#convert customer id to string\ntransaction_df['CustomerID'] = transaction_df['CustomerID'].map(lambda x: str(int(x)))\n\ntransaction_df.shape\n\n(406829, 9)\n\n\n\n#check if still na\ntransaction_df.isna().mean()\n\nInvoiceNo      0.0\nStockCode      0.0\nDescription    0.0\nQuantity       0.0\nInvoiceDate    0.0\nUnitPrice      0.0\nCustomerID     0.0\nCountry        0.0\nyearmon        0.0\ndtype: float64\n\n\n\n#simplify by filtering unit price and quantity to be non-zero (get rid of discounts, cancellations, etc)\ntransaction_df = transaction_df[(transaction_df.UnitPrice&gt;0)&\\\n                                (transaction_df.Quantity&gt;0)].reset_index(drop=True)\n#add sales\ntransaction_df['Sales'] = transaction_df.UnitPrice * transaction_df.Quantity\ntransaction_df.shape\n\n(397884, 10)"
  },
  {
    "objectID": "notebook/sales_prediction.html#problem-formulation-and-outcome",
    "href": "notebook/sales_prediction.html#problem-formulation-and-outcome",
    "title": "Predict Zero-inflated and Long-tailed Outcomes",
    "section": "Problem Formulation and Outcome",
    "text": "Problem Formulation and Outcome\nWe formulate the problem as predicting the sales (TargetSales) during Q4 2011 for each customers who bought at least one item during Q1-Q3 2011. Note that we are interested in predicting the actual sales number per customer as accurately as possible; this is common for marketing use cases such as determining what spend threshold to give each customer in a promotion, customer targeting for upselling, detecting early signs of churns, and so on.\nWe transform the transaction dataset into a customer-level dataset where we calculate features using transactions between 2011-01 to 2011-09 and outcome using transactions between 2011-10 to 2011-12, summing Quantity times UnitPrice. We left-join the customers in feature set to outcome set. This will result in the zero-inflated nature of the outcome as not all customers will come back in Q4. The distribution of non-zero sales is naturally long-tailed with a few customers having extraordinarily high amount of sales in Q4. This resulted in a customer-level dataset with 3,438 customers.\n\nfeature_period = {'start': '2011-01', 'end': '2011-09'}\noutcome_period = {'start': '2011-10', 'end': '2011-12'}\n\nfeature_transaction = transaction_df[(transaction_df.yearmon&gt;=feature_period['start'])&\\\n                                      (transaction_df.yearmon&lt;=feature_period['end'])]\noutcome_transaction = transaction_df[(transaction_df.yearmon&gt;=outcome_period['start'])&\\\n                                      (transaction_df.yearmon&lt;=outcome_period['end'])]\nfeature_transaction.shape, outcome_transaction.shape\n\n((240338, 10), (131389, 10))\n\n\n\n#aggregate sales during outcome period\noutcome_sales = outcome_transaction.groupby('CustomerID').Sales.sum().reset_index()\noutcome_sales\n\n\n\n\n\n\n\n\nCustomerID\nSales\n\n\n\n\n0\n12347\n1519.14\n\n\n1\n12349\n1757.55\n\n\n2\n12352\n311.73\n\n\n3\n12356\n58.35\n\n\n4\n12357\n6207.67\n\n\n...\n...\n...\n\n\n2555\n18276\n335.86\n\n\n2556\n18277\n110.38\n\n\n2557\n18282\n77.84\n\n\n2558\n18283\n974.21\n\n\n2559\n18287\n1072.00\n\n\n\n\n2560 rows × 2 columns\n\n\n\n\n#aggregate sales during feature period\nfeature_sales = feature_transaction.groupby('CustomerID').Sales.sum().reset_index()\nfeature_sales\n\n\n\n\n\n\n\n\nCustomerID\nSales\n\n\n\n\n0\n12346\n77183.60\n\n\n1\n12347\n2079.07\n\n\n2\n12348\n904.44\n\n\n3\n12350\n334.40\n\n\n4\n12352\n2194.31\n\n\n...\n...\n...\n\n\n3433\n18280\n180.60\n\n\n3434\n18281\n80.82\n\n\n3435\n18282\n100.21\n\n\n3436\n18283\n1120.67\n\n\n3437\n18287\n765.28\n\n\n\n\n3438 rows × 2 columns\n\n\n\n\n#merge to get TargetSales including those who spent during feature period but not during outcome (zeroes)\noutcome_df = feature_sales[['CustomerID']].merge(outcome_sales, on='CustomerID', how='left')\noutcome_df['Sales'] = outcome_df['Sales'].fillna(0)\noutcome_df.columns = ['CustomerID', 'TargetSales']\noutcome_df\n\n\n\n\n\n\n\n\nCustomerID\nTargetSales\n\n\n\n\n0\n12346\n0.00\n\n\n1\n12347\n1519.14\n\n\n2\n12348\n0.00\n\n\n3\n12350\n0.00\n\n\n4\n12352\n311.73\n\n\n...\n...\n...\n\n\n3433\n18280\n0.00\n\n\n3434\n18281\n0.00\n\n\n3435\n18282\n77.84\n\n\n3436\n18283\n974.21\n\n\n3437\n18287\n1072.00\n\n\n\n\n3438 rows × 2 columns\n\n\n\n\n#confirm zero-inflated, long-tailed\noutcome_df.TargetSales.describe(percentiles=[i/10 for i in range(10)])\n\ncount      3438.000000\nmean        666.245829\nstd        4016.843037\nmin           0.000000\n0%            0.000000\n10%           0.000000\n20%           0.000000\n30%           0.000000\n40%           0.000000\n50%         102.005000\n60%         263.006000\n70%         425.790000\n80%         705.878000\n90%        1273.611000\nmax      168469.600000\nName: TargetSales, dtype: float64\n\n\n\n#confirm zero-inflated, long-tailed\noutcome_df[outcome_df.TargetSales&lt;=10_000].TargetSales.hist(bins=100)"
  },
  {
    "objectID": "notebook/sales_prediction.html#feature",
    "href": "notebook/sales_prediction.html#feature",
    "title": "Predict Zero-inflated and Long-tailed Outcomes",
    "section": "Feature",
    "text": "Feature\nWe represent a customer using traditional RFM features namely recency of purchase, purchase days, total sales, number of distinct products purchased, number of distinct category purchased, customer tenure within 2011, average purchase frequency, average purchase value, and percentage of purchase across all 9 categories. This is based on data from Q1-Q3 2011.\nSince the UCI Online Retail dataset does not have a category but only contains descriptions over 3,000 items, we use LLaMA 3.2 90B to infer categories based on randomly selected 1,000 descriptions. This is to make the category preference representation for each customer, which is more tractable than including features about all 3,000+ items. After that, we use the same model to label a category for each description. The categories are:\n\nHome Decor\nKitchen and Dining\nFashion Accessories\nStationary and Gifts\nToys and Games\nSeasonal and Holiday\nPersonal Care and Wellness\nOutdoor and Garden\nOthers\n\n\nClassify Description into Category\n\nfeature_transaction.Description.nunique()\n\n3548\n\n\n\nGet Category\n\ndescriptions = feature_transaction.Description.unique().tolist()\nprint(descriptions[:5])\n\n#randomize descriptions with seed 112 to get which categories we should use\nnp.random.seed(112)\nrandom_descriptions = np.random.choice(descriptions, 1000, replace=False)\nprint(random_descriptions[:5])\n\n['JUMBO BAG PINK POLKADOT', 'BLUE POLKADOT WRAP', 'RED RETROSPOT WRAP ', 'RECYCLING BAG RETROSPOT ', 'RED RETROSPOT SHOPPER BAG']\n['MODERN FLORAL STATIONERY SET' 'PURPLE BERTIE GLASS BEAD BAG CHARM'\n 'PARTY INVITES SPACEMAN' 'MONTANA DIAMOND CLUSTER EARRINGS'\n 'SKULLS  DESIGN  COTTON TOTE BAG']\n\n\n\n# res = call_llama(\n#     'You are a product categorization assistant at a retail website.',\n#     'Given the following product descriptions, come up with a few product categories they should be classified into.'+'\\n'.join(random_descriptions)\n#     )\n\n# print(res['generation'])\n\n\n# res\n\n\n# res = call_claude(\n#     'You are a product categorization assistant at a retail website.',\n#     'Given the following product descriptions, come up with a few product categories they should be classified into.'+'\\n'.join(random_descriptions)\n#     )\n\n# print(res['content'][0]['text'])\n\n\n# res\n\nLLaMA 3.2 90B Output:\n&lt;&lt;SYS&gt;&gt;Based on the product descriptions, I would categorize them into the following categories:\n\n1. Home Decor:\n    * Wall art\n    * Decorative items (e.g. vases, figurines, etc.)\n    * Lighting (e.g. candles, lanterns, etc.)\n    * Textiles (e.g. throw pillows, blankets, etc.)\n2. Kitchen and Dining:\n    * Cookware and utensils\n    * Tableware (e.g. plates, cups, etc.)\n    * Kitchen decor (e.g. signs, magnets, etc.)\n    * Food and drink items (e.g. tea, coffee, etc.)\n3. Fashion and Accessories:\n    * Jewelry (e.g. necklaces, earrings, etc.)\n    * Handbags and wallets\n    * Clothing and accessories (e.g. scarves, hats, etc.)\n4. Stationery and Gifts:\n    * Cards and gift wrap\n    * Stationery (e.g. notebooks, pens, etc.)\n    * Gift items (e.g. mugs, keychains, etc.)\n5. Toys and Games:\n    * Toys (e.g. stuffed animals, puzzles, etc.)\n    * Games and puzzles\n6. Seasonal and Holiday:\n    * Christmas decorations and gifts\n    * Easter decorations and gifts\n    * Other seasonal items (e.g. Halloween, etc.)\n7. Personal Care and Wellness:\n    * Beauty and personal care items (e.g. skincare, haircare, etc.)\n    * Wellness and self-care items (e.g. essential oils, etc.)\n8. Outdoor and Garden:\n    * Garden decor and accessories\n    * Outdoor furniture and decor\n    * Gardening tools and supplies\n\nNote that some products may fit into multiple categories, but I have assigned them to the one that seems most relevant.\nClaude 3.5 v2 Output\nBased on these product descriptions, I would suggest the following main product categories:\n\n1. Home Decor\n- Candle holders\n- Picture frames\n- Wall art & signs\n- Clocks\n- Cushions & covers\n- Storage items\n- Decorative objects\n\n2. Jewelry & Accessories\n- Necklaces\n- Bracelets\n- Earrings\n- Hair accessories\n- Bag charms\n- Key rings\n\n3. Garden & Outdoor\n- Plant pots\n- Garden tools\n- Outdoor decorations\n- Bird houses\n- Garden markers\n\n4. Kitchen & Dining\n- Tea sets\n- Mugs\n- Kitchen storage\n- Cutlery\n- Baking accessories\n- Tea towels\n\n5. Stationery & Paper Goods\n- Notebooks\n- Gift wrap\n- Cards\n- Paper decorations\n- Writing sets\n\n6. Party & Celebrations\n- Party supplies\n- Gift bags\n- Christmas decorations\n- Easter items\n- Birthday items\n\n7. Children's Items\n- Toys\n- Children's tableware\n- School supplies\n- Kids' accessories\n\n8. Fashion Accessories\n- Bags\n- Purses\n- Scarves\n- Travel accessories\n\n9. Bath & Beauty\n- Bathroom accessories\n- Toiletry bags\n- Beauty items\n\n10. Lighting\n- Lamps\n- String lights\n- Tea lights\n- Lanterns\n\nThese categories cover the main types of products in the list while providing logical groupings for customers to browse.\n\ncategories = [\n    'Home Decor',\n    'Kitchen and Dining',\n    'Fashion Accessories',\n    'Stationary and Gifts',\n    'Toys and Games',\n    'Seasonal and Holiday',\n    'Personal Care and Wellness',\n    'Outdoor and Garden',   \n]\n\nlen(categories)\n\n8\n\n\n\n\nAnnotate Category to Description\n\n# #loop through descriptions in batches of batch_size\n# res_texts = []\n# batch_size = 100\n# for i in tqdm(range(0, len(descriptions), batch_size)):\n#     batch = descriptions[i:i+batch_size]\n#     d = \"\\n\".join(batch)\n#     inp = f'''Categorize the following product descriptions into {\", \".join(categories)} or Others, if they do not fall into any. \n# Only answer in the following format:\n\n# \"product description of product #1\"|\"product category classified into\"\n# \"product description of product #2\"|\"product category classified into\"\n# ...\n# \"product description of product #n\"|\"product category classified into\"\n\n# Here are the product descriptions:\n# {d}\n# '''\n#     while True:\n#         res = call_claude('You are a product categorizer at a retail website', inp)\n#         # if res['generation_token_count'] &gt; 1: #for llama\n#         if res['usage']['output_tokens'] &gt; 1:\n#             break\n#         else:\n#             print('Retrying...')\n#             time.sleep(2)\n#     res_text = res['content'][0]['text'].strip().split('\\n')\n#         #for llama\n#         # .replace('[SYS]','').replace('&lt;&lt;SYS&gt;&gt;','')\\\n#         # .replace('[/SYS]','').replace('&lt;&lt;/SYS&gt;&gt;','')\\\n#     if res_text!='':\n#         res_texts.extend(res_text)\n\n\n# with open('../data/sales_prediction/product_description_category.csv','w') as f:\n#     f.write('\"product_description\"|\"category\"\\n')\n#     for i in res_texts:\n#         f.write(f'{i}\\n')\n\n\nproduct_description_category = pd.read_csv('../data/sales_prediction/product_description_category.csv',\n                                           sep='|')\n\n#clean product_description\nproduct_description_category['Description'] = descriptions\nproduct_description_category.category.value_counts(normalize=True)\n\ncategory\nHome Decor                    0.328636\nKitchen and Dining            0.195885\nFashion Accessories           0.138670\nStationary and Gifts          0.116122\nSeasonal and Holiday          0.087373\nPersonal Care and Wellness    0.047351\nToys and Games                0.045096\nOutdoor and Garden            0.032976\nOthers                        0.007892\nName: proportion, dtype: float64\n\n\n\nfeature_transaction_cat = feature_transaction.merge(product_description_category,\n                                                    how='inner',\n                                                    on = 'Description',)\nfeature_transaction.shape, feature_transaction_cat.shape\n\n((240338, 10), (240338, 12))\n\n\n\n\n\nRFM\n\n#convert invoice date to datetime\nfeature_transaction_cat['InvoiceDate'] = pd.to_datetime(feature_transaction_cat['InvoiceDate'])\n\n# last date in feature set\ncurrent_date = feature_transaction_cat['InvoiceDate'].max()\n\n#rfm\ncustomer_features = feature_transaction_cat.groupby('CustomerID').agg({\n    'InvoiceDate': [\n        ('recency', lambda x: (current_date - x.max()).days),\n        ('first_purchase_date', 'min'),\n        ('purchase_day', 'nunique'),\n    ],\n    'InvoiceNo': [('nb_invoice', 'nunique')],\n    'Sales': [\n        ('total_sales', 'sum')\n    ],\n    'StockCode': [('nb_product', 'nunique')],\n    'category': [('nb_category', 'nunique')]\n}).reset_index()\n\n# Flatten column names\ncustomer_features.columns = [\n    'CustomerID',\n    'recency',\n    'first_purchase_date',\n    'purchase_day',\n    'nb_invoice',\n    'total_sales',\n    'nb_product',\n    'nb_category'\n]\n\n\n#almost always one purchase a day\n(customer_features.purchase_day==customer_features.nb_invoice).mean()\n\n0.977021524141943\n\n\n\ncustomer_features['customer_lifetime'] = (current_date - customer_features['first_purchase_date']).dt.days\ncustomer_features['avg_purchase_frequency'] = customer_features['customer_lifetime'] / customer_features['purchase_day']\ncustomer_features['avg_purchase_value'] = customer_features['total_sales'] / customer_features['purchase_day']\n\n\n\nCategory Preference\n\n#category preference\ncategory_sales = feature_transaction_cat.pivot_table(\n    values='Sales', \n    index='CustomerID', \n    columns='category', \n    aggfunc='sum', \n    fill_value=0\n)\ncategory_sales.columns = [i.lower().replace(' ','_') for i in category_sales.columns]\ncustomer_features = customer_features.merge(category_sales, on='CustomerID', how='left')\n\ntotal_sales = customer_features['total_sales']\nfor col in category_sales.columns:\n    percentage_col = f'per_{col}'\n    customer_features[percentage_col] = customer_features[col] / total_sales\n\n\n#make sure the categories are not too sparse\n(customer_features.iloc[:,-9:]==0).mean()\n\nper_fashion_accessories           0.409831\nper_home_decor                    0.081734\nper_kitchen_and_dining            0.122455\nper_others                        0.765561\nper_outdoor_and_garden            0.507853\nper_personal_care_and_wellness    0.448226\nper_seasonal_and_holiday          0.369401\nper_stationary_and_gifts          0.305410\nper_toys_and_games                0.487202\ndtype: float64\n\n\n\n\nPutting Them All Together\n\nselected_features = [\n 'recency',\n 'purchase_day',\n 'total_sales',\n 'nb_product',\n 'nb_category',\n 'customer_lifetime',\n 'avg_purchase_frequency',\n 'avg_purchase_value',\n 'per_fashion_accessories',\n 'per_home_decor',\n 'per_kitchen_and_dining',\n 'per_others',\n 'per_outdoor_and_garden',\n 'per_personal_care_and_wellness',\n 'per_seasonal_and_holiday',\n 'per_stationary_and_gifts',\n 'per_toys_and_games']\n\noutcome_variable = 'TargetSales'\n\n\ncustomer_features = customer_features[[ 'CustomerID']+selected_features]\ncustomer_features.head()\n\n\n\n\n\n\n\n\nCustomerID\nrecency\npurchase_day\ntotal_sales\nnb_product\nnb_category\ncustomer_lifetime\navg_purchase_frequency\navg_purchase_value\nper_fashion_accessories\nper_home_decor\nper_kitchen_and_dining\nper_others\nper_outdoor_and_garden\nper_personal_care_and_wellness\nper_seasonal_and_holiday\nper_stationary_and_gifts\nper_toys_and_games\n\n\n\n\n0\n12346\n255\n1\n77183.60\n1\n1\n255\n255.000000\n77183.600000\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n1\n12347\n59\n4\n2079.07\n65\n7\n247\n61.750000\n519.767500\n0.145834\n0.204168\n0.294021\n0.000000\n0.005628\n0.147614\n0.000000\n0.073013\n0.129721\n\n\n2\n12348\n5\n3\n904.44\n10\n4\n248\n82.666667\n301.480000\n0.000000\n0.000000\n0.000000\n0.132679\n0.000000\n0.825970\n0.018796\n0.022555\n0.000000\n\n\n3\n12350\n239\n1\n334.40\n17\n7\n239\n239.000000\n334.400000\n0.240431\n0.202751\n0.116926\n0.172548\n0.000000\n0.118421\n0.000000\n0.059211\n0.089713\n\n\n4\n12352\n2\n7\n2194.31\n47\n8\n226\n32.285714\n313.472857\n0.000000\n0.196531\n0.246187\n0.474090\n0.013535\n0.016680\n0.008066\n0.024404\n0.020508"
  },
  {
    "objectID": "notebook/sales_prediction.html#merge-features-and-outcome",
    "href": "notebook/sales_prediction.html#merge-features-and-outcome",
    "title": "Predict Zero-inflated and Long-tailed Outcomes",
    "section": "Merge Features and Outcome",
    "text": "Merge Features and Outcome\n\ncustomer_features.shape, outcome_df.shape\n\n((3438, 18), (3438, 2))\n\n\n\ndf = outcome_df.merge(customer_features, on='CustomerID').drop('CustomerID', axis=1)\ndf.shape\n\n(3438, 18)\n\n\n\n#correlations\ndf.iloc[:,1:].corr()\n\n\n\n\n\n\n\n\nrecency\npurchase_day\ntotal_sales\nnb_product\nnb_category\ncustomer_lifetime\navg_purchase_frequency\navg_purchase_value\nper_fashion_accessories\nper_home_decor\nper_kitchen_and_dining\nper_others\nper_outdoor_and_garden\nper_personal_care_and_wellness\nper_seasonal_and_holiday\nper_stationary_and_gifts\nper_toys_and_games\n\n\n\n\nrecency\n1.000000\n-0.299308\n-0.132344\n-0.287415\n-0.326772\n0.298853\n0.893973\n0.008823\n-0.020861\n0.022013\n0.057244\n-0.016069\n0.071268\n-0.082792\n-0.085681\n-0.017813\n-0.009686\n\n\npurchase_day\n-0.299308\n1.000000\n0.540253\n0.690345\n0.304621\n0.332109\n-0.331543\n0.027488\n0.030683\n0.018684\n0.025269\n0.004299\n-0.019992\n-0.035665\n-0.020392\n-0.045384\n-0.028187\n\n\ntotal_sales\n-0.132344\n0.540253\n1.000000\n0.400467\n0.137064\n0.156018\n-0.148762\n0.361138\n0.016511\n-0.013819\n0.047834\n0.006398\n-0.029353\n-0.011937\n-0.016724\n-0.029181\n-0.013139\n\n\nnb_product\n-0.287415\n0.690345\n0.400467\n1.000000\n0.555551\n0.265594\n-0.294923\n0.061039\n-0.003137\n-0.017516\n0.035615\n-0.006842\n-0.026371\n-0.005309\n-0.016586\n0.026716\n-0.010069\n\n\nnb_category\n-0.326772\n0.304621\n0.137064\n0.555551\n1.000000\n0.224232\n-0.321596\n0.019955\n0.004863\n-0.138372\n-0.039363\n0.055555\n0.041405\n0.075882\n0.015498\n0.152869\n0.111150\n\n\ncustomer_lifetime\n0.298853\n0.332109\n0.156018\n0.265594\n0.224232\n1.000000\n0.358431\n0.014933\n0.011220\n0.066111\n0.069175\n-0.019971\n0.029726\n-0.127865\n-0.120399\n-0.050320\n-0.036484\n\n\navg_purchase_frequency\n0.893973\n-0.331543\n-0.148762\n-0.294923\n-0.321596\n0.358431\n1.000000\n0.009157\n-0.016093\n0.027208\n0.037053\n-0.027413\n0.060369\n-0.070352\n-0.074799\n-0.000546\n-0.010612\n\n\navg_purchase_value\n0.008823\n0.027488\n0.361138\n0.061039\n0.019955\n0.014933\n0.009157\n1.000000\n-0.003187\n-0.056690\n0.076862\n0.015427\n-0.028884\n0.004225\n-0.000200\n-0.012729\n-0.002396\n\n\nper_fashion_accessories\n-0.020861\n0.030683\n0.016511\n-0.003137\n0.004863\n0.011220\n-0.016093\n-0.003187\n1.000000\n-0.254015\n-0.177775\n-0.010436\n-0.082834\n-0.038493\n-0.124719\n-0.068166\n-0.051486\n\n\nper_home_decor\n0.022013\n0.018684\n-0.013819\n-0.017516\n-0.138372\n0.066111\n0.027208\n-0.056690\n-0.254015\n1.000000\n-0.481983\n-0.155784\n-0.080637\n-0.158837\n-0.165964\n-0.262313\n-0.245759\n\n\nper_kitchen_and_dining\n0.057244\n0.025269\n0.047834\n0.035615\n-0.039363\n0.069175\n0.037053\n0.076862\n-0.177775\n-0.481983\n1.000000\n-0.013075\n-0.144698\n-0.117031\n-0.204235\n-0.173386\n-0.143931\n\n\nper_others\n-0.016069\n0.004299\n0.006398\n-0.006842\n0.055555\n-0.019971\n-0.027413\n0.015427\n-0.010436\n-0.155784\n-0.013075\n1.000000\n-0.062652\n0.014794\n-0.047940\n-0.033975\n-0.040421\n\n\nper_outdoor_and_garden\n0.071268\n-0.019992\n-0.029353\n-0.026371\n0.041405\n0.029726\n0.060369\n-0.028884\n-0.082834\n-0.080637\n-0.144698\n-0.062652\n1.000000\n-0.045639\n-0.077947\n-0.057297\n-0.001034\n\n\nper_personal_care_and_wellness\n-0.082792\n-0.035665\n-0.011937\n-0.005309\n0.075882\n-0.127865\n-0.070352\n0.004225\n-0.038493\n-0.158837\n-0.117031\n0.014794\n-0.045639\n1.000000\n-0.057926\n-0.025871\n-0.017022\n\n\nper_seasonal_and_holiday\n-0.085681\n-0.020392\n-0.016724\n-0.016586\n0.015498\n-0.120399\n-0.074799\n-0.000200\n-0.124719\n-0.165964\n-0.204235\n-0.047940\n-0.077947\n-0.057926\n1.000000\n-0.019418\n-0.042970\n\n\nper_stationary_and_gifts\n-0.017813\n-0.045384\n-0.029181\n0.026716\n0.152869\n-0.050320\n-0.000546\n-0.012729\n-0.068166\n-0.262313\n-0.173386\n-0.033975\n-0.057297\n-0.025871\n-0.019418\n1.000000\n0.172039\n\n\nper_toys_and_games\n-0.009686\n-0.028187\n-0.013139\n-0.010069\n0.111150\n-0.036484\n-0.010612\n-0.002396\n-0.051486\n-0.245759\n-0.143931\n-0.040421\n-0.001034\n-0.017022\n-0.042970\n0.172039\n1.000000\n\n\n\n\n\n\n\n\n#target and most predictive variable\ndf[df.TargetSales&lt;=25_000].plot.scatter(x='TargetSales',y='total_sales')"
  },
  {
    "objectID": "notebook/sales_prediction.html#train-test-splits",
    "href": "notebook/sales_prediction.html#train-test-splits",
    "title": "Predict Zero-inflated and Long-tailed Outcomes",
    "section": "Train-Test Splits",
    "text": "Train-Test Splits\nWe randomly split the dataset into train and test sets at 80/20 ratio. We also confirm the distribution of TargetSales is similar across percentiles and only different at the upper end.\n\n#split into train-valid sets\ntrain_df, test_df = train_test_split(df,\n                                      test_size=0.2, \n                                      random_state=112)\n\n\npd.concat([train_df.TargetSales.describe(percentiles=[i/10 for i in range(10)]).reset_index(),\ntest_df.TargetSales.describe(percentiles=[i/10 for i in range(10)]).reset_index(),], axis=1)\n\n\n\n\n\n\n\n\nindex\nTargetSales\nindex\nTargetSales\n\n\n\n\n0\ncount\n2750.000000\ncount\n688.000000\n\n\n1\nmean\n642.650436\nmean\n760.558808\n\n\n2\nstd\n4015.305436\nstd\n4024.524400\n\n\n3\nmin\n0.000000\nmin\n0.000000\n\n\n4\n0%\n0.000000\n0%\n0.000000\n\n\n5\n10%\n0.000000\n10%\n0.000000\n\n\n6\n20%\n0.000000\n20%\n0.000000\n\n\n7\n30%\n0.000000\n30%\n0.000000\n\n\n8\n40%\n0.000000\n40%\n0.000000\n\n\n9\n50%\n91.350000\n50%\n113.575000\n\n\n10\n60%\n260.308000\n60%\n277.836000\n\n\n11\n70%\n426.878000\n70%\n418.187000\n\n\n12\n80%\n694.164000\n80%\n759.582000\n\n\n13\n90%\n1272.997000\n90%\n1255.670000\n\n\n14\nmax\n168469.600000\nmax\n77099.380000"
  },
  {
    "objectID": "notebook/sales_prediction.html#baseline-regression",
    "href": "notebook/sales_prediction.html#baseline-regression",
    "title": "Predict Zero-inflated and Long-tailed Outcomes",
    "section": "Baseline Regression",
    "text": "Baseline Regression\nThe most naive solution is to predict TargetSales based on the features. We use a stacked ensemble of LightGBM, CatBoost, XGBoost, Random Forest and Extra Trees via AutoGluon. We train with good_quality preset, stated to be “Stronger than any other AutoML Framework”, for speedy training and inference but feel free to try more performant option. We exclude the neural-network models as they require further preprocessing of the features.\nWe use an industry-grade, non-parametric model to be as close to a real use case as possible and make a point that our methodology works not only in a toy-dataset setup.\n\npreset = 'good_quality'\n\n\npredictor = TabularPredictor(label='TargetSales').fit(train_df[selected_features + ['TargetSales']], \n                                                      presets=preset,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20241124_120921\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.1.1\nPython Version:     3.9.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP Wed Oct 23 01:22:11 UTC 2024\nCPU Count:          64\nMemory Avail:       469.51 GB / 480.23 GB (97.8%)\nDisk Space Avail:   1541.39 GB / 1968.52 GB (78.3%)\n===================================================\nPresets specified: ['good_quality']\nSetting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\nStack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\nNote: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n    You can avoid this risk by setting `save_bag_folds=True`.\nDyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n    This is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n    Running DyStack for up to 900s of the 3600s of remaining time (25%).\n        Context path: \"AutogluonModels/ag-20241124_120921/ds_sub_fit/sub_fit_ho\"\n(_dystack pid=67249)    -3921.7042   = Validation score   (-root_mean_squared_error)\n(_dystack pid=67249)    1.87s    = Training   runtime\n(_dystack pid=67249)    0.02s    = Validation runtime\n(_dystack pid=67249) Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 588.58s of the 887.25s of remaining time.\n(_dystack pid=67249)    -4516.1192   = Validation score   (-root_mean_squared_error)\n(_dystack pid=67249)    1.13s    = Training   runtime\n(_dystack pid=67249)    0.18s    = Validation runtime\n(_dystack pid=67249) Fitting model: CatBoost_BAG_L1 ... Training model for up to 587.13s of the 885.8s of remaining time.\n(_dystack pid=67249)    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n(_dystack pid=73545) Running DyStack sub-fit ...\n(_dystack pid=73545) Beginning AutoGluon training ... Time limit = 900s\n(_dystack pid=73545) AutoGluon will save models to \"AutogluonModels/ag-20241124_120921/ds_sub_fit/sub_fit_ho\"\n(_dystack pid=73545) Train Data Rows:    2444\n(_dystack pid=73545) Train Data Columns: 17\n(_dystack pid=73545) Label Column:       TargetSales\n(_dystack pid=73545) Problem Type:       regression\n(_dystack pid=73545) Preprocessing data ...\n(_dystack pid=73545) Using Feature Generators to preprocess the data ...\n(_dystack pid=73545) Fitting AutoMLPipelineFeatureGenerator...\n(_dystack pid=73545)    Available Memory:                    481823.62 MB\n(_dystack pid=73545)    Train Data (Original)  Memory Usage: 0.32 MB (0.0% of available memory)\n(_dystack pid=73545)    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n(_dystack pid=73545)    Stage 1 Generators:\n(_dystack pid=73545)        Fitting AsTypeFeatureGenerator...\n(_dystack pid=73545)    Stage 2 Generators:\n(_dystack pid=73545)        Fitting FillNaFeatureGenerator...\n(_dystack pid=73545)    Stage 3 Generators:\n(_dystack pid=73545)        Fitting IdentityFeatureGenerator...\n(_dystack pid=73545)    Stage 4 Generators:\n(_dystack pid=73545)        Fitting DropUniqueFeatureGenerator...\n(_dystack pid=73545)    Stage 5 Generators:\n(_dystack pid=73545)        Fitting DropDuplicatesFeatureGenerator...\n(_dystack pid=73545)    Types of features in original data (raw dtype, special dtypes):\n(_dystack pid=73545)        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n(_dystack pid=73545)        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n(_dystack pid=73545)    Types of features in processed data (raw dtype, special dtypes):\n(_dystack pid=73545)        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n(_dystack pid=73545)        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n(_dystack pid=73545)    0.0s = Fit runtime\n(_dystack pid=73545)    17 features in original data used to generate 17 features in processed data.\n(_dystack pid=73545)    Train Data (Processed) Memory Usage: 0.32 MB (0.0% of available memory)\n(_dystack pid=73545) Data preprocessing and feature engineering runtime = 0.05s ...\n(_dystack pid=73545) AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n(_dystack pid=73545)    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n(_dystack pid=73545)    To change this, specify the eval_metric parameter of Predictor()\n(_dystack pid=73545) User-specified model hyperparameters to be fit:\n(_dystack pid=73545) {\n(_dystack pid=73545)    'NN_TORCH': {},\n(_dystack pid=73545)    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n(_dystack pid=73545)    'CAT': {},\n(_dystack pid=73545)    'XGB': {},\n(_dystack pid=73545)    'FASTAI': {},\n(_dystack pid=73545)    'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n(_dystack pid=73545)    'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n(_dystack pid=73545) }\n(_dystack pid=73545) AutoGluon will fit 2 stack levels (L1 to L2) ...\n(_dystack pid=73545) Excluded models: ['NN_TORCH', 'FASTAI'] (Specified by `excluded_model_types`)\n(_dystack pid=73545) Fitting 7 L1 models ...\n(_dystack pid=67249)    -3857.3111   = Validation score   (-root_mean_squared_error)\n(_dystack pid=67249)    3.26s    = Training   runtime\n(_dystack pid=67249)    0.03s    = Validation runtime\n(_dystack pid=67249) Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 580.37s of the 879.04s of remaining time. [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\n(_dystack pid=73545)    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n(_dystack pid=67249)    -3901.1815   = Validation score   (-root_mean_squared_error)\n(_dystack pid=67249)    0.93s    = Training   runtime\n(_dystack pid=67249)    0.19s    = Validation runtime\n(_dystack pid=67249)    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n(_dystack pid=67249) Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 573.08s of the 871.75s of remaining time. [repeated 3x across cluster]\n(_dystack pid=67249)    -3941.3599   = Validation score   (-root_mean_squared_error) [repeated 2x across cluster]\n(_dystack pid=67249)    3.08s    = Training   runtime [repeated 2x across cluster]\n(_dystack pid=67249)    0.03s    = Validation runtime [repeated 2x across cluster]\n(_dystack pid=67249)    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%) [repeated 2x across cluster]\n(_dystack pid=73545) Fitting model: CatBoost_BAG_L1 ... Training model for up to 581.75s of the 881.89s of remaining time. [repeated 2x across cluster]\n(_dystack pid=73545)    -4516.1791   = Validation score   (-root_mean_squared_error) [repeated 2x across cluster]\n(_dystack pid=73545)    1.07s    = Training   runtime [repeated 2x across cluster]\n(_dystack pid=73545)    0.18s    = Validation runtime [repeated 2x across cluster]\n(_dystack pid=73545)    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n(_dystack pid=67249)    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n(_dystack pid=73545) Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 572.92s of the 873.05s of remaining time. [repeated 2x across cluster]\n(_dystack pid=73545)    -3857.3111   = Validation score   (-root_mean_squared_error) [repeated 2x across cluster]\n(_dystack pid=73545)    4.27s    = Training   runtime [repeated 2x across cluster]\n(_dystack pid=73545)    0.03s    = Validation runtime [repeated 2x across cluster]\n(_dystack pid=73545)    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n(_dystack pid=73545) Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 565.61s of the 865.75s of remaining time. [repeated 3x across cluster]\n(_dystack pid=73545)    -3941.3599   = Validation score   (-root_mean_squared_error) [repeated 3x across cluster]\n(_dystack pid=73545)    3.7s     = Training   runtime [repeated 3x across cluster]\n(_dystack pid=73545)    0.03s    = Validation runtime [repeated 3x across cluster]\n(_dystack pid=67249)    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n(_dystack pid=73545)    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n\n\n(_ray_fit pid=83699) [1000] valid_set's rmse: 959.321\n\n\n(_dystack pid=73545)    Ensemble Weights: {'CatBoost_BAG_L1': 0.6, 'ExtraTreesMSE_BAG_L1': 0.36, 'LightGBM_BAG_L1': 0.04}\n(_dystack pid=73545) Excluded models: ['NN_TORCH', 'FASTAI'] (Specified by `excluded_model_types`)\n(_dystack pid=73545) Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 856.2s of remaining time. [repeated 2x across cluster]\n(_dystack pid=73545)    -3835.4224   = Validation score   (-root_mean_squared_error) [repeated 3x across cluster]\n(_dystack pid=73545)    0.02s    = Training   runtime [repeated 3x across cluster]\n(_dystack pid=73545)    0.0s     = Validation runtime [repeated 3x across cluster]\n(_dystack pid=67249)    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.01%)\n(_dystack pid=73545) Fitting 7 L2 models ...\n(_dystack pid=73545)    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n(_dystack pid=73545)    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n(_dystack pid=73545) Fitting model: LightGBM_BAG_L2 ... Training model for up to 841.91s of the 841.91s of remaining time. [repeated 2x across cluster]\n(_dystack pid=73545)    -3941.7891   = Validation score   (-root_mean_squared_error)\n(_dystack pid=73545)    7.98s    = Training   runtime\n(_dystack pid=73545)    0.02s    = Validation runtime\n\n\n(_ray_fit pid=88182) [1000] valid_set's rmse: 4314.99\n\n\n(_dystack pid=67249)    -3922.8881   = Validation score   (-root_mean_squared_error)\n(_dystack pid=67249)    22.72s   = Training   runtime\n(_dystack pid=67249)    0.06s    = Validation runtime\n(_dystack pid=67249) Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 520.2s of the 818.87s of remaining time.\n(_dystack pid=67249)    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n(_dystack pid=73545) Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 833.79s of the 833.78s of remaining time.\n(_dystack pid=73545) Fitting model: CatBoost_BAG_L2 ... Training model for up to 832.34s of the 832.34s of remaining time.\n\n\n(_ray_fit pid=88728) [1000] valid_set's rmse: 1586.71\n\n\n(_dystack pid=67249)    -4025.1199   = Validation score   (-root_mean_squared_error) [repeated 3x across cluster]\n(_dystack pid=67249)    3.46s    = Training   runtime [repeated 3x across cluster]\n(_dystack pid=67249)    0.05s    = Validation runtime [repeated 3x across cluster]\n(_dystack pid=73545)    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n(_dystack pid=67249) Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 514.28s of the 812.95s of remaining time.\n(_dystack pid=67249)    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.01%)\n(_dystack pid=73545) Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 824.34s of the 824.33s of remaining time.\n(_dystack pid=73545) Fitting model: XGBoost_BAG_L2 ... Training model for up to 823.02s of the 823.01s of remaining time.\n(_dystack pid=73545)    -3952.2022   = Validation score   (-root_mean_squared_error) [repeated 2x across cluster]\n(_dystack pid=73545)    0.96s    = Training   runtime [repeated 2x across cluster]\n(_dystack pid=73545)    0.26s    = Validation runtime [repeated 2x across cluster]\n(_dystack pid=73545)    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n(_dystack pid=67249) Fitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 504.41s of the 803.08s of remaining time.\n(_dystack pid=67249) Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 503.17s of the 801.84s of remaining time.\n(_dystack pid=67249)    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n(_dystack pid=67249)    -3899.225    = Validation score   (-root_mean_squared_error) [repeated 2x across cluster]\n(_dystack pid=67249)    0.94s    = Training   runtime [repeated 2x across cluster]\n(_dystack pid=67249)    0.19s    = Validation runtime [repeated 2x across cluster]\n(_dystack pid=67249) Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 497.79s of the 796.46s of remaining time. [repeated 2x across cluster]\n(_dystack pid=67249)    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.01%) [repeated 2x across cluster]\n(_dystack pid=67249)    -3852.1469   = Validation score   (-root_mean_squared_error) [repeated 2x across cluster]\n(_dystack pid=67249)    3.13s    = Training   runtime [repeated 2x across cluster]\n(_dystack pid=67249)    0.03s    = Validation runtime [repeated 2x across cluster]\n(_dystack pid=73545) Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 805.79s of remaining time.\n(_dystack pid=73545)    -3912.6409   = Validation score   (-root_mean_squared_error)\n(_dystack pid=73545)    5.91s    = Training   runtime\n(_dystack pid=73545)    0.02s    = Validation runtime\n(_dystack pid=73545)    Ensemble Weights: {'CatBoost_BAG_L1': 0.36, 'ExtraTreesMSE_BAG_L1': 0.32, 'LightGBM_BAG_L2': 0.32}\n(_dystack pid=73545)    -3823.1639   = Validation score   (-root_mean_squared_error)\n(_dystack pid=73545)    0.03s    = Training   runtime\n(_dystack pid=73545)    0.0s     = Validation runtime\n(_dystack pid=73545) AutoGluon training complete, total runtime = 94.26s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1745.1 rows/s (306 batch size)\n(_dystack pid=73545) Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n(_dystack pid=73545) Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n(_dystack pid=73545)    Models trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n(_dystack pid=73545)    This process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n(_dystack pid=73545)    To learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n(_dystack pid=73545) Fitting 1 L1 models ...\n(_dystack pid=73545) Fitting model: LightGBMXT_BAG_L1_FULL ...\n(_dystack pid=73545)    1.76s    = Training   runtime\n(_dystack pid=73545) Fitting 1 L1 models ...\n(_dystack pid=73545) Fitting model: LightGBM_BAG_L1_FULL ...\n(_dystack pid=73545)    0.4s     = Training   runtime\n(_dystack pid=73545) Fitting model: RandomForestMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n(_dystack pid=73545)    1.07s    = Training   runtime\n(_dystack pid=73545)    0.18s    = Validation runtime\n(_dystack pid=73545) Fitting 1 L1 models ...\n(_dystack pid=73545) Fitting model: CatBoost_BAG_L1_FULL ...\n(_dystack pid=73545)    1.05s    = Training   runtime\n(_dystack pid=73545) Fitting model: ExtraTreesMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n(_dystack pid=73545)    0.97s    = Training   runtime\n(_dystack pid=73545)    0.18s    = Validation runtime\n(_dystack pid=73545) Fitting 1 L1 models ...\n(_dystack pid=73545) Fitting model: XGBoost_BAG_L1_FULL ...\n(_dystack pid=73545)    0.67s    = Training   runtime\n(_dystack pid=73545) Fitting 1 L1 models ...\n(_dystack pid=73545) Fitting model: LightGBMLarge_BAG_L1_FULL ...\n(_dystack pid=73545)    0.82s    = Training   runtime\n(_dystack pid=73545) Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n(_dystack pid=73545)    Ensemble Weights: {'CatBoost_BAG_L1': 0.6, 'ExtraTreesMSE_BAG_L1': 0.36, 'LightGBM_BAG_L1': 0.04}\n(_dystack pid=73545)    0.02s    = Training   runtime\n(_dystack pid=73545) Fitting 1 L2 models ...\n(_dystack pid=73545) Fitting model: LightGBMXT_BAG_L2_FULL ...\n(_dystack pid=73545)    0.48s    = Training   runtime\n(_dystack pid=73545) Fitting 1 L2 models ...\n(_dystack pid=73545) Fitting model: LightGBM_BAG_L2_FULL ...\n(_dystack pid=73545)    0.55s    = Training   runtime\n(_dystack pid=73545) Fitting model: RandomForestMSE_BAG_L2_FULL | Skipping fit via cloning parent ...\n(_dystack pid=73545)    1.19s    = Training   runtime\n(_dystack pid=73545)    0.19s    = Validation runtime\n(_dystack pid=73545) Fitting 1 L2 models ...\n(_dystack pid=73545) Fitting model: CatBoost_BAG_L2_FULL ...\n(_dystack pid=73545)    0.2s     = Training   runtime\n(_dystack pid=73545) Fitting model: ExtraTreesMSE_BAG_L2_FULL | Skipping fit via cloning parent ...\n(_dystack pid=73545)    0.96s    = Training   runtime\n(_dystack pid=73545)    0.26s    = Validation runtime\n(_dystack pid=73545) Fitting 1 L2 models ...\n(_dystack pid=73545) Fitting model: XGBoost_BAG_L2_FULL ...\n(_dystack pid=73545)    0.25s    = Training   runtime\n(_dystack pid=73545) Fitting 1 L2 models ...\n(_dystack pid=73545) Fitting model: LightGBMLarge_BAG_L2_FULL ...\n(_dystack pid=73545)    0.62s    = Training   runtime\n(_dystack pid=73545) Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n(_dystack pid=73545)    Ensemble Weights: {'CatBoost_BAG_L1': 0.36, 'ExtraTreesMSE_BAG_L1': 0.32, 'LightGBM_BAG_L2': 0.32}\n(_dystack pid=73545)    0.03s    = Training   runtime\n(_dystack pid=73545) Updated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\n(_dystack pid=73545) Refit complete, total runtime = 7.65s ... Best model: \"WeightedEnsemble_L3_FULL\"\n(_dystack pid=73545) TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241124_120921/ds_sub_fit/sub_fit_ho\")\n(_dystack pid=73545) Deleting DyStack predictor artifacts (clean_up_fits=True) ...\nLeaderboard on holdout data (DyStack):\n                          model  score_holdout    score_val              eval_metric  pred_time_test  pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0          CatBoost_BAG_L1_FULL    -803.899801 -3857.311112  root_mean_squared_error        0.008284            NaN  1.053437                 0.008284                     NaN           1.053437            1       True          4\n1      WeightedEnsemble_L2_FULL    -813.259482 -3835.422447  root_mean_squared_error        0.145661            NaN  2.445404                 0.003759                     NaN           0.019572            2       True          8\n2          CatBoost_BAG_L2_FULL    -838.233626 -3904.774934  root_mean_squared_error        0.331961            NaN  6.940521                 0.009136                     NaN           0.200137            2       True         12\n3   RandomForestMSE_BAG_L1_FULL    -847.825565 -4516.179095  root_mean_squared_error        0.143293       0.178818  1.065305                 0.143293                0.178818           1.065305            1       True          3\n4     ExtraTreesMSE_BAG_L2_FULL    -890.912998 -3952.202176  root_mean_squared_error        0.447699            NaN  7.702393                 0.124875                0.262680           0.962008            2       True         13\n5     ExtraTreesMSE_BAG_L1_FULL    -922.896541 -3900.303809  root_mean_squared_error        0.129396       0.175367  0.970753                 0.129396                0.175367           0.970753            1       True          5\n6      WeightedEnsemble_L3_FULL    -977.887954 -3823.163850  root_mean_squared_error        0.333488            NaN  7.317170                 0.003488                     NaN           0.030095            3       True         16\n7          LightGBM_BAG_L1_FULL   -1086.123687 -3921.704247  root_mean_squared_error        0.004222            NaN  0.401642                 0.004222                     NaN           0.401642            1       True          2\n8   RandomForestMSE_BAG_L2_FULL   -1090.066132 -4525.205744  root_mean_squared_error        0.484758            NaN  7.932426                 0.161933                0.186784           1.192042            2       True         11\n9        LightGBMXT_BAG_L1_FULL   -1230.340360 -3990.480139  root_mean_squared_error        0.004649            NaN  1.756547                 0.004649                     NaN           1.756547            1       True          1\n10       LightGBMXT_BAG_L2_FULL   -1234.815155 -3941.789134  root_mean_squared_error        0.328251            NaN  7.219311                 0.005426                     NaN           0.478927            2       True          9\n11    LightGBMLarge_BAG_L1_FULL   -1345.024278 -3912.540001  root_mean_squared_error        0.009380            NaN  0.821679                 0.009380                     NaN           0.821679            1       True          7\n12    LightGBMLarge_BAG_L2_FULL   -1640.347524 -3912.640942  root_mean_squared_error        0.333141            NaN  7.357616                 0.010317                     NaN           0.617232            2       True         15\n13         LightGBM_BAG_L2_FULL   -1743.255667 -3894.707823  root_mean_squared_error        0.330000            NaN  7.287075                 0.007175                     NaN           0.546691            2       True         10\n14          XGBoost_BAG_L1_FULL   -2245.433966 -3941.359884  root_mean_squared_error        0.023602            NaN  0.671022                 0.023602                     NaN           0.671022            1       True          6\n15          XGBoost_BAG_L2_FULL   -2454.083373 -3929.201875  root_mean_squared_error        0.343434            NaN  6.986505                 0.020609                     NaN           0.246121            2       True         14\n    0    = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n    107s     = DyStack   runtime |  3493s    = Remaining runtime\nStarting main fit with num_stack_levels=0.\n    For future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\nBeginning AutoGluon training ... Time limit = 3493s\nAutoGluon will save models to \"AutogluonModels/ag-20241124_120921\"\nTrain Data Rows:    2750\nTrain Data Columns: 17\nLabel Column:       TargetSales\nProblem Type:       regression\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    480556.52 MB\n    Train Data (Original)  Memory Usage: 0.36 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    0.0s = Fit runtime\n    17 features in original data used to generate 17 features in processed data.\n    Train Data (Processed) Memory Usage: 0.36 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.06s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n}\nExcluded models: ['FASTAI', 'NN_TORCH'] (Specified by `excluded_model_types`)\nFitting 7 L1 models ...\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 3493.16s of the 3493.16s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -3713.1197   = Validation score   (-root_mean_squared_error)\n    6.39s    = Training   runtime\n    0.04s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ... Training model for up to 3484.28s of the 3484.27s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -3635.1505   = Validation score   (-root_mean_squared_error)\n    5.52s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: RandomForestMSE_BAG_L1 ... Training model for up to 3476.36s of the 3476.36s of remaining time.\n    -4135.0334   = Validation score   (-root_mean_squared_error)\n    1.09s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ... Training model for up to 3475.01s of the 3475.0s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -3669.0125   = Validation score   (-root_mean_squared_error)\n    13.22s   = Training   runtime\n    0.03s    = Validation runtime\nFitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 3459.47s of the 3459.47s of remaining time.\n    -3678.3921   = Validation score   (-root_mean_squared_error)\n    0.63s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ... Training model for up to 3458.58s of the 3458.58s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -3785.5048   = Validation score   (-root_mean_squared_error)\n    2.13s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ... Training model for up to 3454.21s of the 3454.2s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -3704.5742   = Validation score   (-root_mean_squared_error)\n    3.52s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 3445.96s of remaining time.\n    Ensemble Weights: {'LightGBM_BAG_L1': 0.5, 'ExtraTreesMSE_BAG_L1': 0.35, 'CatBoost_BAG_L1': 0.15}\n    -3608.5561   = Validation score   (-root_mean_squared_error)\n    0.02s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 47.31s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 4567.8 rows/s (344 batch size)\nAutomatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\nRefitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n    Models trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n    This process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n    To learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\nFitting 1 L1 models ...\nFitting model: LightGBMXT_BAG_L1_FULL ...\n    1.62s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBM_BAG_L1_FULL ...\n    0.52s    = Training   runtime\nFitting model: RandomForestMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    1.09s    = Training   runtime\n    0.18s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: CatBoost_BAG_L1_FULL ...\n    0.95s    = Training   runtime\nFitting model: ExtraTreesMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.63s    = Training   runtime\n    0.18s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: XGBoost_BAG_L1_FULL ...\n    0.78s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBMLarge_BAG_L1_FULL ...\n    0.58s    = Training   runtime\nFitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n    Ensemble Weights: {'LightGBM_BAG_L1': 0.5, 'ExtraTreesMSE_BAG_L1': 0.35, 'CatBoost_BAG_L1': 0.15}\n    0.02s    = Training   runtime\nUpdated best model to \"WeightedEnsemble_L2_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L2_FULL\" for predict() and predict_proba().\nRefit complete, total runtime = 4.86s ... Best model: \"WeightedEnsemble_L2_FULL\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241124_120921\")\n\n\n\ntest_df['pred_baseline'] = predictor.predict(test_df[selected_features])\n\n\nmetric_baseline = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_baseline'])\nmetric_baseline['model'] = 'baseline'\nmetric_baseline\n\n{'root_mean_squared_error': 3162.478744240967,\n 'mean_squared_error': 10001271.807775924,\n 'mean_absolute_error': 715.6442657130541,\n 'r2': 0.3816166296854987,\n 'pearsonr': 0.6190719671013133,\n 'median_absolute_error': 232.98208312988282,\n 'earths_mover_distance': 287.77728784026124,\n 'model': 'baseline'}"
  },
  {
    "objectID": "notebook/sales_prediction.html#regression-on-winsorized-outcome",
    "href": "notebook/sales_prediction.html#regression-on-winsorized-outcome",
    "title": "Predict Zero-inflated and Long-tailed Outcomes",
    "section": "Regression on Winsorized Outcome",
    "text": "Regression on Winsorized Outcome\nOne possible approach to deal with long-tailed outcome is to train on a winsorized outcome. This may lead to better performance when tested on a winsorized outcome but not so much on original outcome.\n\noutlier_per = 0.99\noutlier_cap_train = train_df['TargetSales'].quantile(outlier_per)\noutlier_cap_train\n\n7180.805199999947\n\n\n\n#winsorize\ntrain_df['TargetSales_win'] = train_df['TargetSales'].map(lambda x: outlier_cap_train if x&gt; outlier_cap_train else x)\ntest_df['TargetSales_win'] = test_df['TargetSales'].map(lambda x: outlier_cap_train if x&gt; outlier_cap_train else x)\n\n\npredictor = TabularPredictor(label='TargetSales_win').fit(train_df[selected_features+['TargetSales_win']],\n                                                      presets=preset,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20241124_121246\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.1.1\nPython Version:     3.9.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP Wed Oct 23 01:22:11 UTC 2024\nCPU Count:          64\nMemory Avail:       468.66 GB / 480.23 GB (97.6%)\nDisk Space Avail:   1540.88 GB / 1968.52 GB (78.3%)\n===================================================\nPresets specified: ['good_quality']\nSetting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\nStack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\nNote: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n    You can avoid this risk by setting `save_bag_folds=True`.\nDyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n    This is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n    Running DyStack for up to 900s of the 3600s of remaining time (25%).\n        Context path: \"AutogluonModels/ag-20241124_121246/ds_sub_fit/sub_fit_ho\"\nLeaderboard on holdout data (DyStack):\n                          model  score_holdout   score_val              eval_metric  pred_time_test  pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0           XGBoost_BAG_L2_FULL    -673.227470 -706.566643  root_mean_squared_error        0.276127            NaN  6.175230                 0.012955                     NaN           0.249267            2       True         14\n1          CatBoost_BAG_L2_FULL    -685.276375 -679.668006  root_mean_squared_error        0.269301            NaN  6.563540                 0.006129                     NaN           0.637577            2       True         12\n2     ExtraTreesMSE_BAG_L2_FULL    -686.432329 -688.280166  root_mean_squared_error        0.380708            NaN  6.801457                 0.117536                0.173927           0.875495            2       True         13\n3      WeightedEnsemble_L2_FULL    -687.292057 -677.748155  root_mean_squared_error        0.142632            NaN  3.476852                 0.004268                     NaN           0.034524            2       True          8\n4          CatBoost_BAG_L1_FULL    -688.830702 -682.216238  root_mean_squared_error        0.010283            NaN  0.828954                 0.010283                     NaN           0.828954            1       True          4\n5   RandomForestMSE_BAG_L2_FULL    -690.155342 -702.819447  root_mean_squared_error        0.395053            NaN  6.983567                 0.131881                0.173440           1.057604            2       True         11\n6     LightGBMLarge_BAG_L2_FULL    -699.457560 -701.790157  root_mean_squared_error        0.272846            NaN  6.558122                 0.009674                     NaN           0.632160            2       True         15\n7      WeightedEnsemble_L3_FULL    -699.646914 -664.915201  root_mean_squared_error        0.297722            NaN  7.544165                 0.004164                     NaN           0.043211            3       True         16\n8   RandomForestMSE_BAG_L1_FULL    -700.107179 -708.557877  root_mean_squared_error        0.112145       0.175746  1.039714                 0.112145                0.175746           1.039714            1       True          3\n9     ExtraTreesMSE_BAG_L1_FULL    -701.853556 -688.997247  root_mean_squared_error        0.109873       0.171419  0.868997                 0.109873                0.171419           0.868997            1       True          5\n10          XGBoost_BAG_L1_FULL    -717.776000 -710.501170  root_mean_squared_error        0.014828            NaN  0.205235                 0.014828                     NaN           0.205235            1       True          6\n11       LightGBMXT_BAG_L2_FULL    -723.560168 -701.334719  root_mean_squared_error        0.269142            NaN  6.225563                 0.005970                     NaN           0.299601            2       True          9\n12         LightGBM_BAG_L1_FULL    -726.112842 -700.802863  root_mean_squared_error        0.002233            NaN  0.244877                 0.002233                     NaN           0.244877            1       True          2\n13         LightGBM_BAG_L2_FULL    -728.829307 -669.578190  root_mean_squared_error        0.293558            NaN  7.500954                 0.030386                     NaN           1.574992            2       True         10\n14       LightGBMXT_BAG_L1_FULL    -733.594747 -704.073534  root_mean_squared_error        0.003381            NaN  1.539141                 0.003381                     NaN           1.539141            1       True          1\n15    LightGBMLarge_BAG_L1_FULL    -766.964045 -715.782974  root_mean_squared_error        0.010430            NaN  1.199044                 0.010430                     NaN           1.199044            1       True          7\n    0    = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n    206s     = DyStack   runtime |  3394s    = Remaining runtime\nStarting main fit with num_stack_levels=0.\n    For future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\nBeginning AutoGluon training ... Time limit = 3394s\nAutoGluon will save models to \"AutogluonModels/ag-20241124_121246\"\nTrain Data Rows:    2750\nTrain Data Columns: 17\nLabel Column:       TargetSales_win\nProblem Type:       regression\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    480760.98 MB\n    Train Data (Original)  Memory Usage: 0.36 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    0.1s = Fit runtime\n    17 features in original data used to generate 17 features in processed data.\n    Train Data (Processed) Memory Usage: 0.36 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.07s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n}\nExcluded models: ['FASTAI', 'NN_TORCH'] (Specified by `excluded_model_types`)\nFitting 7 L1 models ...\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 3394.08s of the 3394.08s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -710.5609    = Validation score   (-root_mean_squared_error)\n    4.51s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ... Training model for up to 3382.41s of the 3382.4s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -696.6213    = Validation score   (-root_mean_squared_error)\n    7.34s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: RandomForestMSE_BAG_L1 ... Training model for up to 3370.06s of the 3370.05s of remaining time.\n    -706.2702    = Validation score   (-root_mean_squared_error)\n    0.86s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ... Training model for up to 3368.94s of the 3368.94s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -668.1395    = Validation score   (-root_mean_squared_error)\n    10.27s   = Training   runtime\n    0.03s    = Validation runtime\nFitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 3356.05s of the 3356.05s of remaining time.\n    -688.8913    = Validation score   (-root_mean_squared_error)\n    0.67s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ... Training model for up to 3355.13s of the 3355.13s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -699.3326    = Validation score   (-root_mean_squared_error)\n    2.05s    = Training   runtime\n    0.04s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ... Training model for up to 3350.76s of the 3350.75s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -714.8496    = Validation score   (-root_mean_squared_error)\n    4.96s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 3340.67s of remaining time.\n    Ensemble Weights: {'CatBoost_BAG_L1': 0.833, 'XGBoost_BAG_L1': 0.125, 'ExtraTreesMSE_BAG_L1': 0.042}\n    -667.3394    = Validation score   (-root_mean_squared_error)\n    0.03s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 53.54s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 3887.5 rows/s (344 batch size)\nAutomatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\nRefitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n    Models trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n    This process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n    To learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\nFitting 1 L1 models ...\nFitting model: LightGBMXT_BAG_L1_FULL ...\n    0.5s     = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBM_BAG_L1_FULL ...\n    0.57s    = Training   runtime\nFitting model: RandomForestMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.86s    = Training   runtime\n    0.18s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: CatBoost_BAG_L1_FULL ...\n    0.46s    = Training   runtime\nFitting model: ExtraTreesMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.67s    = Training   runtime\n    0.18s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: XGBoost_BAG_L1_FULL ...\n    0.14s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBMLarge_BAG_L1_FULL ...\n    0.83s    = Training   runtime\nFitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n    Ensemble Weights: {'CatBoost_BAG_L1': 0.833, 'XGBoost_BAG_L1': 0.125, 'ExtraTreesMSE_BAG_L1': 0.042}\n    0.03s    = Training   runtime\nUpdated best model to \"WeightedEnsemble_L2_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L2_FULL\" for predict() and predict_proba().\nRefit complete, total runtime = 2.91s ... Best model: \"WeightedEnsemble_L2_FULL\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241124_121246\")\n\n\n\ntest_df['pred_winsorized'] = predictor.predict(test_df[selected_features])\n\n\nmetric_winsorized = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_winsorized'])\nmetric_winsorized['model'] = 'winsorized'\nmetric_winsorized\n\n{'root_mean_squared_error': 3623.576377551195,\n 'mean_squared_error': 13130305.76394704,\n 'mean_absolute_error': 627.7880071099414,\n 'r2': 0.18814697894155963,\n 'pearsonr': 0.5757989413256978,\n 'median_absolute_error': 219.62248107910156,\n 'earths_mover_distance': 432.1288432991232,\n 'model': 'winsorized'}\n\n\n\ncalculate_regression_metrics(test_df['TargetSales_win'], test_df['pred_winsorized'])\n\n{'root_mean_squared_error': 673.4846433338375,\n 'mean_squared_error': 453581.5648065064,\n 'mean_absolute_error': 376.77603327273135,\n 'r2': 0.6171771763549553,\n 'pearsonr': 0.7865724180212539,\n 'median_absolute_error': 218.8311004638672,\n 'earths_mover_distance': 181.1168694619127}"
  },
  {
    "objectID": "notebook/sales_prediction.html#log1p-regression",
    "href": "notebook/sales_prediction.html#log1p-regression",
    "title": "Predict Zero-inflated and Long-tailed Outcomes",
    "section": "Log1p Regression",
    "text": "Log1p Regression\nLog transformation handles long-tailed distribution and is especially useful for certain models since the transformed distribution is roughly normal. However, it cannot handle zero-valued outcome and oftentimes scientists end up adding 1 to the outcome (so often that numpy even has a function for it). This not only introduces bias to the prediction, but also does not solve the zero-inflation as it becomes one-inflation instead.\n\n#log\ntrain_df['TargetSales_log1p'] = train_df['TargetSales'].map(np.log1p)\ntest_df['TargetSales_log1p'] = test_df['TargetSales'].map(np.log1p)\n\n\n#from zero-inflated to one-inflated\ntrain_df['TargetSales_log1p'].hist()\n\n\n\n\n\n\n\n\n\npredictor = TabularPredictor(label='TargetSales_log1p').fit(train_df[selected_features+['TargetSales_log1p']],\n                                                      presets=preset,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20241124_121709\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.1.1\nPython Version:     3.9.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP Wed Oct 23 01:22:11 UTC 2024\nCPU Count:          64\nMemory Avail:       468.82 GB / 480.23 GB (97.6%)\nDisk Space Avail:   1540.26 GB / 1968.52 GB (78.2%)\n===================================================\nPresets specified: ['good_quality']\nSetting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\nStack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\nNote: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n    You can avoid this risk by setting `save_bag_folds=True`.\nDyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n    This is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n    Running DyStack for up to 900s of the 3600s of remaining time (25%).\n        Context path: \"AutogluonModels/ag-20241124_121709/ds_sub_fit/sub_fit_ho\"\nLeaderboard on holdout data (DyStack):\n                          model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0          CatBoost_BAG_L1_FULL      -2.885523  -2.780750  root_mean_squared_error        0.004735            NaN   0.658652                 0.004735                     NaN           0.658652            1       True          4\n1      WeightedEnsemble_L2_FULL      -2.894191  -2.775644  root_mean_squared_error        0.148396            NaN   3.401193                 0.003478                     NaN           0.020115            2       True          8\n2   RandomForestMSE_BAG_L2_FULL      -2.894937  -2.808770  root_mean_squared_error        0.419389            NaN   7.133266                 0.115585                0.183146           1.895508            2       True         11\n3     ExtraTreesMSE_BAG_L2_FULL      -2.896741  -2.777983  root_mean_squared_error        0.429382            NaN   6.105324                 0.125578                0.178113           0.867565            2       True         13\n4        LightGBMXT_BAG_L1_FULL      -2.908863  -2.782297  root_mean_squared_error        0.003623            NaN   1.791187                 0.003623                     NaN           1.791187            1       True          1\n5          CatBoost_BAG_L2_FULL      -2.922107  -2.759026  root_mean_squared_error        0.311817            NaN   8.303950                 0.008013                     NaN           3.066191            2       True         12\n6          LightGBM_BAG_L2_FULL      -2.931031  -2.759814  root_mean_squared_error        0.314463            NaN   6.115426                 0.010659                     NaN           0.877667            2       True         10\n7           XGBoost_BAG_L2_FULL      -2.938193  -2.790059  root_mean_squared_error        0.316005            NaN   5.407094                 0.012201                     NaN           0.169335            2       True         14\n8      WeightedEnsemble_L3_FULL      -2.942265  -2.685363  root_mean_squared_error        0.452081            NaN  16.144942                 0.005700                     NaN           0.031429            3       True         16\n9     ExtraTreesMSE_BAG_L1_FULL      -2.946022  -2.815757  root_mean_squared_error        0.136560       0.182462   0.931239                 0.136560                0.182462           0.931239            1       True          5\n10         LightGBM_BAG_L1_FULL      -2.953480  -2.813496  root_mean_squared_error        0.002491            NaN   0.215871                 0.002491                     NaN           0.215871            1       True          2\n11          XGBoost_BAG_L1_FULL      -2.972277  -2.836214  root_mean_squared_error        0.018836            NaN   0.154601                 0.018836                     NaN           0.154601            1       True          6\n12    LightGBMLarge_BAG_L2_FULL      -2.977587  -2.794323  root_mean_squared_error        0.350646            NaN   9.701969                 0.046842                     NaN           4.464211            2       True         15\n13  RandomForestMSE_BAG_L1_FULL      -2.985264  -2.831375  root_mean_squared_error        0.131284       0.177366   1.045549                 0.131284                0.177366           1.045549            1       True          3\n14       LightGBMXT_BAG_L2_FULL      -2.995407  -2.694352  root_mean_squared_error        0.376679            NaN  10.602300                 0.072875                     NaN           5.364541            2       True          9\n15    LightGBMLarge_BAG_L1_FULL      -3.050660  -2.862792  root_mean_squared_error        0.006275            NaN   0.440660                 0.006275                     NaN           0.440660            1       True          7\n    0    = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n    234s     = DyStack   runtime |  3366s    = Remaining runtime\nStarting main fit with num_stack_levels=0.\n    For future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\nBeginning AutoGluon training ... Time limit = 3366s\nAutoGluon will save models to \"AutogluonModels/ag-20241124_121709\"\nTrain Data Rows:    2750\nTrain Data Columns: 17\nLabel Column:       TargetSales_log1p\nProblem Type:       regression\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    480394.63 MB\n    Train Data (Original)  Memory Usage: 0.36 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    0.0s = Fit runtime\n    17 features in original data used to generate 17 features in processed data.\n    Train Data (Processed) Memory Usage: 0.36 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.06s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n}\nExcluded models: ['FASTAI', 'NN_TORCH'] (Specified by `excluded_model_types`)\nFitting 7 L1 models ...\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 3365.79s of the 3365.79s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -2.7861  = Validation score   (-root_mean_squared_error)\n    3.27s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ... Training model for up to 3356.74s of the 3356.74s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -2.8189  = Validation score   (-root_mean_squared_error)\n    9.06s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: RandomForestMSE_BAG_L1 ... Training model for up to 3331.23s of the 3331.23s of remaining time.\n    -2.8468  = Validation score   (-root_mean_squared_error)\n    0.99s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ... Training model for up to 3329.97s of the 3329.97s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -2.7963  = Validation score   (-root_mean_squared_error)\n    4.65s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 3322.89s of the 3322.89s of remaining time.\n    -2.8191  = Validation score   (-root_mean_squared_error)\n    0.71s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ... Training model for up to 3321.92s of the 3321.91s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -2.8365  = Validation score   (-root_mean_squared_error)\n    4.65s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ... Training model for up to 3314.98s of the 3314.98s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -2.8667  = Validation score   (-root_mean_squared_error)\n    8.01s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 3304.75s of remaining time.\n    Ensemble Weights: {'LightGBMXT_BAG_L1': 0.714, 'CatBoost_BAG_L1': 0.143, 'ExtraTreesMSE_BAG_L1': 0.143}\n    -2.7845  = Validation score   (-root_mean_squared_error)\n    0.02s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 61.18s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 5534.0 rows/s (344 batch size)\nAutomatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\nRefitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n    Models trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n    This process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n    To learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\nFitting 1 L1 models ...\nFitting model: LightGBMXT_BAG_L1_FULL ...\n    0.33s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBM_BAG_L1_FULL ...\n    0.28s    = Training   runtime\nFitting model: RandomForestMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.99s    = Training   runtime\n    0.18s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: CatBoost_BAG_L1_FULL ...\n    0.18s    = Training   runtime\nFitting model: ExtraTreesMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.71s    = Training   runtime\n    0.19s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: XGBoost_BAG_L1_FULL ...\n    0.11s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBMLarge_BAG_L1_FULL ...\n    0.62s    = Training   runtime\nFitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n    Ensemble Weights: {'LightGBMXT_BAG_L1': 0.714, 'CatBoost_BAG_L1': 0.143, 'ExtraTreesMSE_BAG_L1': 0.143}\n    0.02s    = Training   runtime\nUpdated best model to \"WeightedEnsemble_L2_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L2_FULL\" for predict() and predict_proba().\nRefit complete, total runtime = 1.9s ... Best model: \"WeightedEnsemble_L2_FULL\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241124_121709\")\n\n\n\ntest_df['pred_log1p'] = predictor.predict(test_df[selected_features])\ntest_df['pred_log1p_expm1'] = test_df['pred_log1p'].map(np.expm1)\n\n\nmetric_log1p = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_log1p_expm1'])\nmetric_log1p['model'] = 'log1p'\nmetric_log1p\n\n{'root_mean_squared_error': 3725.342295894091,\n 'mean_squared_error': 13878175.221577456,\n 'mean_absolute_error': 618.9768466651894,\n 'r2': 0.14190585634701047,\n 'pearsonr': 0.5817166874396966,\n 'median_absolute_error': 89.55495441784018,\n 'earths_mover_distance': 581.0494444960044,\n 'model': 'log1p'}\n\n\n\ncalculate_regression_metrics(test_df['TargetSales_log1p'], test_df['pred_log1p'])\n\n{'root_mean_squared_error': 2.720047847858299,\n 'mean_squared_error': 7.398660294638562,\n 'mean_absolute_error': 2.418601533469381,\n 'r2': 0.30252750020590236,\n 'pearsonr': 0.5507740732825224,\n 'median_absolute_error': 2.349368453025818,\n 'earths_mover_distance': 1.8552344547363062}"
  },
  {
    "objectID": "notebook/sales_prediction.html#hurdle-regression",
    "href": "notebook/sales_prediction.html#hurdle-regression",
    "title": "Predict Zero-inflated and Long-tailed Outcomes",
    "section": "Hurdle Regression",
    "text": "Hurdle Regression\nHurdle regression is a two-stage approach that handles zero inflation by first having a classification model to predict if the outcome is zero or not, then a regression model, trained only on examples with actual non-zero outcomes, to fit a log-transformed outcome. When retransforming the predictions from log to non-log numbers, we perform correction of underestimation using Duan’s method. During inference time, we multiply the predictions from the classification and regression model.\n\nBinary Classification\n\ntrain_df['has_purchase'] = train_df.TargetSales.map(lambda x: 1 if x&gt;0 else 0)\ntest_df['has_purchase'] = test_df.TargetSales.map(lambda x: 1 if x&gt;0 else 0)\n\n\ntrain_df['has_purchase'].mean(), test_df['has_purchase'].mean()\n\n(0.5141818181818182, 0.5305232558139535)\n\n\n\npredictor_cls = TabularPredictor(label='has_purchase').fit(train_df[selected_features+['has_purchase']],\n                                                      presets=preset,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20241124_122312\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.1.1\nPython Version:     3.9.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP Wed Oct 23 01:22:11 UTC 2024\nCPU Count:          64\nMemory Avail:       468.17 GB / 480.23 GB (97.5%)\nDisk Space Avail:   1539.67 GB / 1968.52 GB (78.2%)\n===================================================\nPresets specified: ['good_quality']\nSetting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\nStack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\nNote: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n    You can avoid this risk by setting `save_bag_folds=True`.\nDyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n    This is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n    Running DyStack for up to 900s of the 3600s of remaining time (25%).\n        Context path: \"AutogluonModels/ag-20241124_122312/ds_sub_fit/sub_fit_ho\"\nLeaderboard on holdout data (DyStack):\n                           model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0           LightGBM_BAG_L1_FULL       0.699346   0.686989    accuracy        0.001610            NaN   0.128000                 0.001610                     NaN           0.128000            1       True          2\n1           CatBoost_BAG_L1_FULL       0.699346   0.698036    accuracy        0.004140            NaN   0.612854                 0.004140                     NaN           0.612854            1       True          5\n2       WeightedEnsemble_L2_FULL       0.699346   0.698036    accuracy        0.006433            NaN   0.782266                 0.002294                     NaN           0.169412            2       True         10\n3     ExtraTreesEntr_BAG_L1_FULL       0.692810   0.675123    accuracy        0.129808       0.286071   1.857026                 0.129808                0.286071           1.857026            1       True          7\n4         LightGBMXT_BAG_L2_FULL       0.692810   0.701718    accuracy        0.517301            NaN   9.294753                 0.004494                     NaN           0.198776            2       True         11\n5           CatBoost_BAG_L2_FULL       0.692810   0.707447    accuracy        0.518521            NaN   9.654956                 0.005714                     NaN           0.558980            2       True         15\n6       WeightedEnsemble_L3_FULL       0.692810   0.707856    accuracy        0.520624            NaN   9.972980                 0.002103                     NaN           0.318024            3       True         20\n7     ExtraTreesGini_BAG_L1_FULL       0.689542   0.673077    accuracy        0.120208       0.276335   1.145981                 0.120208                0.276335           1.145981            1       True          6\n8   RandomForestEntr_BAG_L1_FULL       0.683007   0.654255    accuracy        0.116106       0.235400   1.176533                 0.116106                0.235400           1.176533            1       True          4\n9     ExtraTreesGini_BAG_L2_FULL       0.679739   0.684534    accuracy        0.633551            NaN  10.122717                 0.120743                0.182851           1.026740            2       True         16\n10  RandomForestGini_BAG_L2_FULL       0.679739   0.684943    accuracy        0.651521            NaN  11.043631                 0.138714                0.257494           1.947655            2       True         13\n11          LightGBM_BAG_L2_FULL       0.676471   0.706219    accuracy        0.517315            NaN   9.297191                 0.004508                     NaN           0.201214            2       True         12\n12    ExtraTreesEntr_BAG_L2_FULL       0.676471   0.679624    accuracy        0.633967            NaN  10.118144                 0.121159                0.181766           1.022167            2       True         17\n13           XGBoost_BAG_L1_FULL       0.673203   0.687398    accuracy        0.009487            NaN   0.136529                 0.009487                     NaN           0.136529            1       True          8\n14           XGBoost_BAG_L2_FULL       0.673203   0.702946    accuracy        0.531083            NaN   9.225766                 0.018275                     NaN           0.129789            2       True         18\n15  RandomForestEntr_BAG_L2_FULL       0.673203   0.684534    accuracy        0.630382            NaN  10.803487                 0.117574                0.290613           1.707510            2       True         14\n16        LightGBMXT_BAG_L1_FULL       0.666667   0.694354    accuracy        0.002703            NaN   1.306679                 0.002703                     NaN           1.306679            1       True          1\n17     LightGBMLarge_BAG_L1_FULL       0.663399   0.673895    accuracy        0.010920            NaN   0.880971                 0.010920                     NaN           0.880971            1       True          9\n18  RandomForestGini_BAG_L1_FULL       0.660131   0.666121    accuracy        0.117827       0.183592   1.851404                 0.117827                0.183592           1.851404            1       True          3\n19     LightGBMLarge_BAG_L2_FULL       0.656863   0.704992    accuracy        0.522204            NaN   9.949596                 0.009396                     NaN           0.853620            2       True         19\n    0    = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n    105s     = DyStack   runtime |  3495s    = Remaining runtime\nStarting main fit with num_stack_levels=0.\n    For future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\nBeginning AutoGluon training ... Time limit = 3495s\nAutoGluon will save models to \"AutogluonModels/ag-20241124_122312\"\nTrain Data Rows:    2750\nTrain Data Columns: 17\nLabel Column:       has_purchase\nProblem Type:       binary\nPreprocessing data ...\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    480528.71 MB\n    Train Data (Original)  Memory Usage: 0.36 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    0.0s = Fit runtime\n    17 features in original data used to generate 17 features in processed data.\n    Train Data (Processed) Memory Usage: 0.36 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.06s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n}\nExcluded models: ['FASTAI', 'NN_TORCH'] (Specified by `excluded_model_types`)\nFitting 9 L1 models ...\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 3494.55s of the 3494.54s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    0.6964   = Validation score   (accuracy)\n    2.11s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ... Training model for up to 3490.18s of the 3490.18s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    0.6884   = Validation score   (accuracy)\n    1.94s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: RandomForestGini_BAG_L1 ... Training model for up to 3486.01s of the 3486.0s of remaining time.\n    0.6615   = Validation score   (accuracy)\n    0.92s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: RandomForestEntr_BAG_L1 ... Training model for up to 3484.83s of the 3484.82s of remaining time.\n    0.6644   = Validation score   (accuracy)\n    0.97s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ... Training model for up to 3483.61s of the 3483.6s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    0.6935   = Validation score   (accuracy)\n    1.88s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 3479.43s of the 3479.43s of remaining time.\n    0.6738   = Validation score   (accuracy)\n    0.97s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 3478.19s of the 3478.19s of remaining time.\n    0.6716   = Validation score   (accuracy)\n    0.88s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ... Training model for up to 3477.03s of the 3477.02s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    0.6789   = Validation score   (accuracy)\n    1.81s    = Training   runtime\n    0.04s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ... Training model for up to 3473.0s of the 3473.0s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    0.6738   = Validation score   (accuracy)\n    5.19s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 3465.55s of remaining time.\n    Ensemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n    0.6964   = Validation score   (accuracy)\n    0.18s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 29.27s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 25514.6 rows/s (344 batch size)\nAutomatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\nRefitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n    Models trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n    This process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n    To learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\nFitting 1 L1 models ...\nFitting model: LightGBMXT_BAG_L1_FULL ...\n    0.25s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBM_BAG_L1_FULL ...\n    0.26s    = Training   runtime\nFitting model: RandomForestGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.92s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: RandomForestEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.97s    = Training   runtime\n    0.19s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: CatBoost_BAG_L1_FULL ...\n    0.1s     = Training   runtime\nFitting model: ExtraTreesGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.97s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: ExtraTreesEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.88s    = Training   runtime\n    0.19s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: XGBoost_BAG_L1_FULL ...\n    0.12s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBMLarge_BAG_L1_FULL ...\n    1.04s    = Training   runtime\nFitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n    Ensemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n    0.18s    = Training   runtime\nUpdated best model to \"LightGBMXT_BAG_L1_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"LightGBMXT_BAG_L1_FULL\" for predict() and predict_proba().\nRefit complete, total runtime = 2.24s ... Best model: \"LightGBMXT_BAG_L1_FULL\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241124_122312\")\n\n\n\ntest_df['pred_binary'] = predictor_cls.predict(test_df[selected_features])\n\n\ncaluclate_classification_metrics(test_df['has_purchase'], test_df['pred_binary'])\n\n{'accuracy': 0.6918604651162791,\n 'precision': 0.6941069004479309,\n 'recall': 0.6918604651162791,\n 'f1_score': 0.6921418829824787,\n 'confusion_matrix': array([[229,  94],\n        [118, 247]])}\n\n\n\n\nRegression on Non-Zero Outcome\n\ntrain_df_nonzero = train_df[train_df.has_purchase==1].reset_index(drop=True)\ntest_df_nonzero = test_df[test_df.has_purchase==1].reset_index(drop=True)\n\ntrain_df_nonzero.shape, test_df_nonzero.shape\n\n((1414, 21), (365, 26))\n\n\n\n#log\ntrain_df_nonzero['TargetSales_log'] = train_df_nonzero['TargetSales'].map(np.log)\ntest_df_nonzero['TargetSales_log'] = test_df_nonzero['TargetSales'].map(np.log)\n\n\ntrain_df_nonzero['TargetSales_log'].hist()\n\n\n\n\n\n\n\n\n\npredictor_reg = TabularPredictor(label='TargetSales_log').fit(train_df_nonzero[selected_features+['TargetSales_log']],\n                                                      presets=preset,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20241124_122529\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.1.1\nPython Version:     3.9.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP Wed Oct 23 01:22:11 UTC 2024\nCPU Count:          64\nMemory Avail:       468.95 GB / 480.23 GB (97.7%)\nDisk Space Avail:   1540.91 GB / 1968.52 GB (78.3%)\n===================================================\nPresets specified: ['good_quality']\nSetting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\nStack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\nNote: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n    You can avoid this risk by setting `save_bag_folds=True`.\nDyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n    This is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n    Running DyStack for up to 900s of the 3600s of remaining time (25%).\n        Context path: \"AutogluonModels/ag-20241124_122529/ds_sub_fit/sub_fit_ho\"\nLeaderboard on holdout data (DyStack):\n                          model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0     ExtraTreesMSE_BAG_L1_FULL      -0.839346  -0.781522  root_mean_squared_error        0.111404       0.145174  0.801402                 0.111404                0.145174           0.801402            1       True          5\n1     ExtraTreesMSE_BAG_L2_FULL      -0.853589  -0.787303  root_mean_squared_error        0.433190            NaN  8.210388                 0.141008                0.142385           0.826072            2       True         13\n2      WeightedEnsemble_L3_FULL      -0.857556  -0.768871  root_mean_squared_error        0.326593            NaN  8.044827                 0.005474                     NaN           0.025574            3       True         16\n3   RandomForestMSE_BAG_L1_FULL      -0.861175  -0.804629  root_mean_squared_error        0.117929       0.148952  0.930058                 0.117929                0.148952           0.930058            1       True          3\n4      WeightedEnsemble_L2_FULL      -0.862117  -0.769820  root_mean_squared_error        0.139917            NaN  1.970585                 0.004211                     NaN           0.016667            2       True          8\n5        LightGBMXT_BAG_L1_FULL      -0.864283  -0.782882  root_mean_squared_error        0.002619            NaN  1.420255                 0.002619                     NaN           1.420255            1       True          1\n6          CatBoost_BAG_L2_FULL      -0.866735  -0.782409  root_mean_squared_error        0.300920            NaN  7.581485                 0.008738                     NaN           0.197169            2       True         12\n7   RandomForestMSE_BAG_L2_FULL      -0.867588  -0.801697  root_mean_squared_error        0.419654            NaN  8.309190                 0.127472                0.146134           0.924873            2       True         11\n8        LightGBMXT_BAG_L2_FULL      -0.867632  -0.795484  root_mean_squared_error        0.296052            NaN  7.538164                 0.003871                     NaN           0.153847            2       True          9\n9           XGBoost_BAG_L2_FULL      -0.867990  -0.801243  root_mean_squared_error        0.314637            NaN  7.687288                 0.022455                     NaN           0.302972            2       True         14\n10         LightGBM_BAG_L2_FULL      -0.869884  -0.797155  root_mean_squared_error        0.298664            NaN  7.716282                 0.006483                     NaN           0.331965            2       True         10\n11         LightGBM_BAG_L1_FULL      -0.872511  -0.786431  root_mean_squared_error        0.002023            NaN  0.156078                 0.002023                     NaN           0.156078            1       True          2\n12         CatBoost_BAG_L1_FULL      -0.876613  -0.774745  root_mean_squared_error        0.006800            NaN  0.826834                 0.006800                     NaN           0.826834            1       True          4\n13          XGBoost_BAG_L1_FULL      -0.908105  -0.786510  root_mean_squared_error        0.015479            NaN  0.169605                 0.015479                     NaN           0.169605            1       True          6\n14    LightGBMLarge_BAG_L2_FULL      -0.929002  -0.811482  root_mean_squared_error        0.299922            NaN  8.012576                 0.007740                     NaN           0.628259            2       True         15\n15    LightGBMLarge_BAG_L1_FULL      -0.943116  -0.811385  root_mean_squared_error        0.035927            NaN  3.080085                 0.035927                     NaN           3.080085            1       True          7\n    1    = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n    89s  = DyStack   runtime |  3511s    = Remaining runtime\nStarting main fit with num_stack_levels=1.\n    For future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\nBeginning AutoGluon training ... Time limit = 3511s\nAutoGluon will save models to \"AutogluonModels/ag-20241124_122529\"\nTrain Data Rows:    1414\nTrain Data Columns: 17\nLabel Column:       TargetSales_log\nProblem Type:       regression\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    479873.40 MB\n    Train Data (Original)  Memory Usage: 0.18 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    0.0s = Fit runtime\n    17 features in original data used to generate 17 features in processed data.\n    Train Data (Processed) Memory Usage: 0.18 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.05s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n}\nAutoGluon will fit 2 stack levels (L1 to L2) ...\nExcluded models: ['FASTAI', 'NN_TORCH'] (Specified by `excluded_model_types`)\nFitting 7 L1 models ...\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 2339.78s of the 3510.54s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -0.7909  = Validation score   (-root_mean_squared_error)\n    1.84s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ... Training model for up to 2335.74s of the 3506.5s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -0.7919  = Validation score   (-root_mean_squared_error)\n    1.76s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: RandomForestMSE_BAG_L1 ... Training model for up to 2331.7s of the 3502.46s of remaining time.\n    -0.8074  = Validation score   (-root_mean_squared_error)\n    0.81s    = Training   runtime\n    0.15s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ... Training model for up to 2330.65s of the 3501.42s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -0.783   = Validation score   (-root_mean_squared_error)\n    1.91s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 2326.5s of the 3497.26s of remaining time.\n    -0.7902  = Validation score   (-root_mean_squared_error)\n    0.6s     = Training   runtime\n    0.15s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ... Training model for up to 2325.65s of the 3496.41s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -0.8026  = Validation score   (-root_mean_squared_error)\n    1.7s     = Training   runtime\n    0.04s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2321.75s of the 3492.52s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -0.8166  = Validation score   (-root_mean_squared_error)\n    3.45s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 3486.77s of remaining time.\n    Ensemble Weights: {'CatBoost_BAG_L1': 0.5, 'ExtraTreesMSE_BAG_L1': 0.25, 'LightGBM_BAG_L1': 0.188, 'LightGBMXT_BAG_L1': 0.062}\n    -0.78    = Validation score   (-root_mean_squared_error)\n    0.03s    = Training   runtime\n    0.0s     = Validation runtime\nExcluded models: ['FASTAI', 'NN_TORCH'] (Specified by `excluded_model_types`)\nFitting 7 L2 models ...\nFitting model: LightGBMXT_BAG_L2 ... Training model for up to 3486.68s of the 3486.67s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -0.7869  = Validation score   (-root_mean_squared_error)\n    3.01s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBM_BAG_L2 ... Training model for up to 3481.42s of the 3481.41s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -0.7932  = Validation score   (-root_mean_squared_error)\n    2.46s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: RandomForestMSE_BAG_L2 ... Training model for up to 3476.66s of the 3476.65s of remaining time.\n    -0.8097  = Validation score   (-root_mean_squared_error)\n    0.7s     = Training   runtime\n    0.15s    = Validation runtime\nFitting model: CatBoost_BAG_L2 ... Training model for up to 3475.71s of the 3475.7s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -0.7847  = Validation score   (-root_mean_squared_error)\n    5.42s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 3468.03s of the 3468.01s of remaining time.\n    -0.795   = Validation score   (-root_mean_squared_error)\n    0.62s    = Training   runtime\n    0.15s    = Validation runtime\nFitting model: XGBoost_BAG_L2 ... Training model for up to 3467.14s of the 3467.13s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -0.8102  = Validation score   (-root_mean_squared_error)\n    2.35s    = Training   runtime\n    0.04s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L2 ... Training model for up to 3462.54s of the 3462.53s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -0.8107  = Validation score   (-root_mean_squared_error)\n    11.53s   = Training   runtime\n    0.05s    = Validation runtime\nFitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 3448.66s of remaining time.\n    Ensemble Weights: {'CatBoost_BAG_L1': 0.333, 'LightGBMXT_BAG_L2': 0.25, 'LightGBM_BAG_L2': 0.167, 'LightGBM_BAG_L1': 0.083, 'ExtraTreesMSE_BAG_L1': 0.083, 'LightGBMLarge_BAG_L2': 0.083}\n    -0.7746  = Validation score   (-root_mean_squared_error)\n    0.03s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 62.02s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 728.5 rows/s (177 batch size)\nAutomatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\nRefitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n    Models trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n    This process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n    To learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\nFitting 1 L1 models ...\nFitting model: LightGBMXT_BAG_L1_FULL ...\n    0.35s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBM_BAG_L1_FULL ...\n    0.32s    = Training   runtime\nFitting model: RandomForestMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.81s    = Training   runtime\n    0.15s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: CatBoost_BAG_L1_FULL ...\n    0.29s    = Training   runtime\nFitting model: ExtraTreesMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.6s     = Training   runtime\n    0.15s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: XGBoost_BAG_L1_FULL ...\n    0.1s     = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBMLarge_BAG_L1_FULL ...\n    0.66s    = Training   runtime\nFitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n    Ensemble Weights: {'CatBoost_BAG_L1': 0.5, 'ExtraTreesMSE_BAG_L1': 0.25, 'LightGBM_BAG_L1': 0.188, 'LightGBMXT_BAG_L1': 0.062}\n    0.03s    = Training   runtime\nFitting 1 L2 models ...\nFitting model: LightGBMXT_BAG_L2_FULL ...\n    0.76s    = Training   runtime\nFitting 1 L2 models ...\nFitting model: LightGBM_BAG_L2_FULL ...\n    0.45s    = Training   runtime\nFitting model: RandomForestMSE_BAG_L2_FULL | Skipping fit via cloning parent ...\n    0.7s     = Training   runtime\n    0.15s    = Validation runtime\nFitting 1 L2 models ...\nFitting model: CatBoost_BAG_L2_FULL ...\n    0.34s    = Training   runtime\nFitting model: ExtraTreesMSE_BAG_L2_FULL | Skipping fit via cloning parent ...\n    0.62s    = Training   runtime\n    0.15s    = Validation runtime\nFitting 1 L2 models ...\nFitting model: XGBoost_BAG_L2_FULL ...\n    0.18s    = Training   runtime\nFitting 1 L2 models ...\nFitting model: LightGBMLarge_BAG_L2_FULL ...\n    2.46s    = Training   runtime\nFitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n    Ensemble Weights: {'CatBoost_BAG_L1': 0.333, 'LightGBMXT_BAG_L2': 0.25, 'LightGBM_BAG_L2': 0.167, 'LightGBM_BAG_L1': 0.083, 'ExtraTreesMSE_BAG_L1': 0.083, 'LightGBMLarge_BAG_L2': 0.083}\n    0.03s    = Training   runtime\nUpdated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\nRefit complete, total runtime = 6.67s ... Best model: \"WeightedEnsemble_L3_FULL\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241124_122529\")\n\n\n\ntest_df_nonzero['pred_log'] = predictor_reg.predict(test_df_nonzero[selected_features])\ntest_df_nonzero['pred_log_exp'] = test_df_nonzero['pred_log'].map(np.exp)\n\ntest_df['pred_log'] = predictor_reg.predict(test_df[selected_features])\ntest_df['pred_log_exp'] = test_df['pred_log'].map(np.exp)\n\n\ncalculate_regression_metrics(test_df_nonzero['TargetSales'], test_df_nonzero['pred_log_exp'])\n\n{'root_mean_squared_error': 4330.443144695726,\n 'mean_squared_error': 18752737.82944221,\n 'mean_absolute_error': 880.0418223064565,\n 'r2': 0.3647576298877435,\n 'pearsonr': 0.6756393928483335,\n 'median_absolute_error': 243.0658528752748,\n 'earths_mover_distance': 546.7166312173882}\n\n\n\ntest_df['pred_hurdle'] = test_df.pred_binary * test_df.pred_log_exp\n\n\nmetric_hurdle = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_hurdle'])\nmetric_hurdle['model'] = 'hurdle'\nmetric_hurdle\n\n{'root_mean_squared_error': 3171.760744960863,\n 'mean_squared_error': 10060066.22327469,\n 'mean_absolute_error': 584.9162934881963,\n 'r2': 0.3779813431428882,\n 'pearsonr': 0.6769697889999318,\n 'median_absolute_error': 199.1780137692856,\n 'earths_mover_distance': 286.381442541919,\n 'model': 'hurdle'}\n\n\n\n\nDuan’s Method\n\ntrain_df_nonzero['pred_log'] = predictor_reg.predict(train_df_nonzero[selected_features])\ntrain_df_nonzero['pred_log_exp'] = train_df_nonzero['pred_log'].map(np.exp)\n\nsmearing_factor = np.mean(np.exp(train_df_nonzero['TargetSales_log'] - train_df_nonzero['pred_log']))\nsmearing_factor\n\n1.2280991653046711\n\n\n\ntest_df['pred_log_exp_corrected'] = test_df['pred_log_exp'] * smearing_factor\ntest_df['pred_hurdle_corrected'] = test_df.pred_binary * test_df.pred_log_exp_corrected\n\n\nmetric_hurdle_corrected = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_hurdle_corrected'])\nmetric_hurdle_corrected['model'] = 'hurdle_corrected'\nmetric_hurdle_corrected\n\n{'root_mean_squared_error': 3055.3207868281233,\n 'mean_squared_error': 9334985.110424023,\n 'mean_absolute_error': 613.3946643257099,\n 'r2': 0.42281345159207295,\n 'pearsonr': 0.6769697889999318,\n 'median_absolute_error': 232.55557358084502,\n 'earths_mover_distance': 241.61839859133218,\n 'model': 'hurdle_corrected'}"
  },
  {
    "objectID": "notebook/sales_prediction.html#evaluation",
    "href": "notebook/sales_prediction.html#evaluation",
    "title": "Predict Zero-inflated and Long-tailed Outcomes",
    "section": "Evaluation",
    "text": "Evaluation\nWe can see that the hurdle_corrected method performs best across all metrics except for 1) mean absolute error where it performs about 5% worse than hurdle method without the correction and 2) median absolute error where it only performs better than baseline regression; pearson’s correlation is tied between the two Hurdle methods by definition since we multiply Duan’s smearing factor to hurdle predictions to get hurdle_corrected.\n\nmetric_df = pd.DataFrame([metric_baseline,\n                       metric_winsorized,\n                       metric_log1p,\n                       metric_hurdle,\n                       metric_hurdle_corrected,])\n\n\n\nrank_df = metric_df.copy()\nfor col in metric_df.columns.tolist()[:-1]:\n    if col in ['r2', 'pearsonr']:\n        rank_df[f'{col}_rank'] = rank_df[col].rank(ascending=False)\n    else:\n        rank_df[f'{col}_rank'] = rank_df[col].rank(ascending=True)\nrank_df = rank_df.drop(metric_df.columns.tolist()[:-1], axis=1)\nrank_df['avg_rank'] = rank_df.iloc[:,1:].mean(axis=1)\nrank_df.transpose()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nmodel\nbaseline\nwinsorized\nlog1p\nhurdle\nhurdle_corrected\n\n\nroot_mean_squared_error_rank\n2.0\n4.0\n5.0\n3.0\n1.0\n\n\nmean_squared_error_rank\n2.0\n4.0\n5.0\n3.0\n1.0\n\n\nmean_absolute_error_rank\n5.0\n4.0\n3.0\n1.0\n2.0\n\n\nr2_rank\n2.0\n4.0\n5.0\n3.0\n1.0\n\n\npearsonr_rank\n3.0\n5.0\n4.0\n1.5\n1.5\n\n\nmedian_absolute_error_rank\n5.0\n3.0\n1.0\n2.0\n4.0\n\n\nearths_mover_distance_rank\n3.0\n4.0\n5.0\n2.0\n1.0\n\n\navg_rank\n3.142857\n4.0\n4.0\n2.214286\n1.642857\n\n\n\n\n\n\n\n\nmetric_df.transpose()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nroot_mean_squared_error\n3162.478744\n3623.576378\n3725.342296\n3171.760745\n3055.320787\n\n\nmean_squared_error\n10001271.807776\n13130305.763947\n13878175.221577\n10060066.223275\n9334985.110424\n\n\nmean_absolute_error\n715.644266\n627.788007\n618.976847\n584.916293\n613.394664\n\n\nr2\n0.381617\n0.188147\n0.141906\n0.377981\n0.422813\n\n\npearsonr\n0.619072\n0.575799\n0.581717\n0.67697\n0.67697\n\n\nmedian_absolute_error\n232.982083\n219.622481\n89.554954\n199.178014\n232.555574\n\n\nearths_mover_distance\n287.777288\n432.128843\n581.049444\n286.381443\n241.618399\n\n\nmodel\nbaseline\nwinsorized\nlog1p\nhurdle\nhurdle_corrected\n\n\n\n\n\n\n\n\nWhy hurdle Outperforms hurdle_corrected in MAE?\nDuan’s method adjusts for underestimation from retransformation of log outcome. This could lead to smaller extreme errors but more less extreme ones. We verify this hypothesis by comparing mean absolute error before and after transformation for errors originally under and over 99th percentile. We confirm that is the case for this problem.\n\nerr_hurdle = (test_df['TargetSales'] - test_df['pred_hurdle']).abs()\nerr_hurdle_corrected = (test_df['TargetSales'] - test_df['pred_hurdle_corrected']).abs()\n\n\nerr_hurdle.describe(percentiles=[.25, .5, .75, .9, .95, .99]) \n\ncount      688.000000\nmean       584.916293\nstd       3119.628924\nmin          0.000000\n25%          0.000000\n50%        199.178014\n75%        475.603446\n90%        862.530026\n95%       1237.540954\n99%       6763.777844\nmax      55731.205996\ndtype: float64\n\n\n\nerr_hurdle[err_hurdle&lt;6763.777844].mean(),\\\nerr_hurdle[err_hurdle&gt;6763.777844].mean(),\n\n(355.4918014848842, 22904.641872667555)\n\n\n\nerr_hurdle_corrected[err_hurdle&lt;6763.777844].mean(),\\\nerr_hurdle_corrected[err_hurdle&gt;6763.777844].mean(),\n\n(392.7718802742851, 22076.839798471465)\n\n\n\ntest_df['pred_hurdle_corrected'].plot.kde()\n\n\n\n\n\n\n\n\n\ntest_df['pred_hurdle_corrected'].plot.kde()\ntest_df['pred_hurdle'].plot.kde()\n# test_df['TargetSales'].plot.kde()\n\n\n\n\n\n\n\n\n\n\nWhy log1p Performs So Much Better than Others in MedAE?\nIt is for similar reasons that hurdle outperforms hurdle_corrected in MedAE; however, log1p performs twice better than other approaches, especially the Hurdle models which should be modeling the non-zero outcomes in the same manner. This is because Hurdle models depend not only on the regression but the classification model. We can see that if the classification model were perfect (instead of the current f1 = 0.69), other metrics also improved but not nearly as drastic as MedAE and MAE.\n\ntest_df['pred_hurdle_corrected_perfect_cls'] = test_df.has_purchase * test_df.pred_log_exp_corrected\nmetric_hurdle_corrected_perfect_cls = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_hurdle_corrected_perfect_cls'])\nmetric_hurdle_corrected_perfect_cls['model'] = 'hurdle_corrected_perfect_cls'\n\nmetric_df2 = pd.DataFrame([metric_baseline,\n                       metric_winsorized,\n                       metric_log1p,\n                       metric_hurdle,\n                       metric_hurdle_corrected,\n                       metric_hurdle_corrected_perfect_cls,])\nmetric_df2.transpose()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\nroot_mean_squared_error\n3162.478744\n3623.576378\n3725.342296\n3171.760745\n3055.320787\n3030.854831\n\n\nmean_squared_error\n10001271.807776\n13130305.763947\n13878175.221577\n10060066.223275\n9334985.110424\n9186081.006625\n\n\nmean_absolute_error\n715.644266\n627.788007\n618.976847\n584.916293\n613.394664\n479.558294\n\n\nr2\n0.381617\n0.188147\n0.141906\n0.377981\n0.422813\n0.43202\n\n\npearsonr\n0.619072\n0.575799\n0.581717\n0.67697\n0.67697\n0.687639\n\n\nmedian_absolute_error\n232.982083\n219.622481\n89.554954\n199.178014\n232.555574\n34.991964\n\n\nearths_mover_distance\n287.777288\n432.128843\n581.049444\n286.381443\n241.618399\n234.587018\n\n\nmodel\nbaseline\nwinsorized\nlog1p\nhurdle\nhurdle_corrected\nhurdle_corrected_perfect_cls"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Charin and I am a senior applied scientist at Amazon. My main focus is on estimating heterogeneity effects for customer targeting and personalization, using causal inference techniques and counterfactuals generated by large language models. This is a collection of technical writings I find useful."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "chariblog - technical writings in applied science",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n2024-10-27\n\n\nHarlow Malloc\n\n\n\n\n\n\nNo matching items"
  }
]