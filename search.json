[
  {
    "objectID": "posts/interesting_career/index.html",
    "href": "posts/interesting_career/index.html",
    "title": "How to Have a Robustly Interesting Career in Data Science",
    "section": "",
    "text": "I have been a data scientistüë®‚Äçüíª ever since around the time HBR popularized the term in its job sexiness article. My run started out of necessity, since back then it was very difficult to get a job when you were an Econ graduate with an eclectic mix of skills in statistics and programming; suddenly it became one of the most sought-after skill combinations on the market. It has been about a decade since. Lately, I have been asked about how to have a successful career in the field. Success is quite the subjective term, but let me try to formulate my thoughts on how to keep it interesting doing what I love in spite of external circumstances.\n\n\n\nfeatured_image\n\n\nObjective Function As any half decent scientist would do, let us begin at the desirable end state and work backwards. I have always optimized for business impact. I relish in seeing my models bring joy to customers, efficiency to selling partners, and productivity to fellow builders. Something as simple as seeing a widget on the home screen powered by one of my models makes my day. Thankfully, my data products have consistently delivered at least double-digit millions of dollars in value (cost savings and/or top-line uplifts) annually. And this keeps me sane in the midst of all the [insert-your-hype-keyword] FOMO. The key is to find an objective function that really matters to you, and not fall for the vanity metrics. For instance, I have written some research papers and did some fun open source projects, but a lot of these feel too abstract to quantify. If I were to try to optimize for them, I would likely end up chasing after citations, Github stars, or other arbitrary numbers. I would overfit them and the activities of writing papers and contributing to open source projects themselves would become secondary to the vanity metrics. This will not be the case if writing great research papers is something that really matters to you. You need to find an objective function that correlates almost perfectly with the joy you experience as a scientist.\nBuild with Stakes As you move to fulfill your objectives, it is important that you do so by building concrete products with high enough stakes. I strongly believe this is the only way to level up as a scientist. The anti-thesis to this is to get stuck in the learning loop; you keep taking online courses, overfitting toy datasets, hunting for certificates, and you wonder why your career is not going anywhere while the snakeoil vendors keep buying new sports cars. It is because these learning materials when taken in excess only serve to make you feel good about yourself. Yes, you learned something new and yes, you might have built some capstone projects. But these have virtually no consequence if you fail. In fact, they are structured in a way that it is more difficult for you to fail. You stop thinking for yourself and just enjoy the pseudo-intellectual force feeding. It comes as no surprise that you need to think for yourself to grow as a scientist. The stakes do not have to be monetary. It can be anything that matters if you fail to complete your tasks. Enter a competition that evaluates scientific prowess (avoid slide show contests), create an open source project that helps with your hobbies, or most likely propose a project at work that affects your compensation/promotion evaluation. Conduct experiments, write production codes, and document everything either as technical reports or research papers. Hold yourself to the highest standards.\nFind The Right Party The most important person in your career is your mentor. Ideally, you want to find the person whose objective function aligns well with yours and have a clear track records of building with stakes. At different stages, you might have more than one for different aspects such as one for business and another for research, or for different perspectives. But if I were to be honest, you would be extremely lucky to find one at all. Your mentor should not only be the person you aspire to be, but also someone you think you would have a chance to overcome in a fair fight one day. Once you have found such person, you would usually be surrounded by good company. Humans are social animals and no matter how hard you try to follow your objective function and hold yourself to the highest standards, it will be almost impossible without people with similar mentality by your side. This is the most luck-dependent component. Be grateful if you can find the right party and be the party people would like to join one day.\nIntegrity Every action has a price. We are in a privileged position to have a skillset that often determines the outcome of the business and rarely people question. You could gain a small but decisive advantage almost scotch-free; keep randomizing the seeds in validation splits and/or model initialization to get a marginally good result in offline evaluation to justify a launch, slice and dice the control and treatment groups to get a statistically significant result, pick a seemingly strong model that is out-of-domain to compare with your specifically finetuned model and call your model SoTA, the list goes on. But you should never do any of this, not only because you are a good person who does not want to lose sleep, but because you will eventually pay the price. I can guarantee there is always a price. Any useful scientist will call you out on any of the examples I gave; if not, time and repeated online experiments will expose your fudging of the numbers. Your choice is to keep job hopping before this happens and keep the career Ponzi scheme alive. Or simply be an honest scientist, enjoys the scientific process, learns from your mistakes, and grow.\nSide Quests Whatever you do, there is always a room for side quests. Your main quest is to satisfy your objective function, but as any good optimization method, exploration is needed to ensure a robust solution. A side quest is an excuse to try the new technology you have been raring to get your hands on, a low-stake confidence builder, and most importantly a great way to remind us how much we love the craft. These are things I hold closest to my heart as I navigate the fast-paced, uncertainty-filled landscape of data science. I have conducted a few dozens of model validation experiments and each one is never less anxiety-inducing than the last. We are in the business of results and results can be brutal. In these turbulent economic conditions, I hope these templates can be useful to you as much as they were to me in surviving some unpleasant situations that may be beyond our control and continuing to do the things we love.\nBe safe from scammers and snakeoil vendors. And I will see you around."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "chariblog - technical writings in applied science",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nPredict How Much A Customer Will Spend\n\n\n\n\n\n\nretail\n\n\nzero-inflated\n\n\nlong/fat-tailed\n\n\nhurdle\n\n\n\n\n\n\n\n\n\n2024-11-25\n\n\ncstorm125\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Have a Robustly Interesting Career in Data Science\n\n\n\n\n\n\ncareer\n\n\n\n\n\n\n\n\n\n2024-11-23\n\n\ncstorm125\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Charin and I am a senior applied scientist at Amazon. My main focus is on estimating heterogeneity effects for customer targeting and personalization, using causal inference techniques and counterfactuals generated by large language models. This is a collection of technical writings I find useful."
  },
  {
    "objectID": "notebook/sales_prediction.html",
    "href": "notebook/sales_prediction.html",
    "title": "Predict Zero-inflated and Long/fat-tailed Outcomes",
    "section": "",
    "text": "This notebook details how to predict a real-number outcome that is zero-inflated and long/fat-tailed such as sales prediction in retail. We provide baseline regression, regression trained using winsorized outcome, regression trained on log(y+1) outcome, and hurdle regression with and without Duan‚Äôs method."
  },
  {
    "objectID": "notebook/sales_prediction.html#import",
    "href": "notebook/sales_prediction.html#import",
    "title": "Predict Zero-inflated and Long/fat-tailed Outcomes",
    "section": "Import",
    "text": "Import\n\nimport pandas as pd\nimport numpy as np\nimport random\nfrom ucimlrepo import fetch_ucirepo \nimport boto3\nimport json\nfrom tqdm.auto import tqdm\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom autogluon.tabular import TabularDataset, TabularPredictor\nfrom sklearn.metrics import (\n    mean_squared_error, mean_absolute_error, r2_score, median_absolute_error,\n    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n)\n\nfrom scipy.stats import pearsonr, spearmanr, wasserstein_distance\n\ndef calculate_regression_metrics(y_true, y_pred):\n    return {\n        'root_mean_squared_error': np.sqrt(mean_squared_error(y_true, y_pred)),\n        'mean_squared_error': mean_squared_error(y_true, y_pred),\n        'mean_absolute_error': mean_absolute_error(y_true, y_pred),\n        'r2': r2_score(y_true, y_pred),\n        'pearsonr': pearsonr(y_true, y_pred)[0],  \n        'spearmanr': spearmanr(y_true, y_pred)[0],\n        'median_absolute_error': median_absolute_error(y_true, y_pred),\n        'earths_mover_distance': wasserstein_distance(y_true, y_pred)\n    }\n\ndef caluclate_classification_metrics(y_true, y_pred):\n    return {\n        'accuracy': accuracy_score(y_true, y_pred),\n        'precision': precision_score(y_true, y_pred, average='weighted'),\n        'recall': recall_score(y_true, y_pred, average='weighted'),\n        'f1_score': f1_score(y_true, y_pred, average='weighted'),\n        'confusion_matrix': confusion_matrix(y_true, y_pred)\n    }\n\ndef string_to_yearmon(date):\n    date = date.split()\n    date = date[0].split('/') + date[1].split(':')\n    date = date[2] + '-' + date[0].zfill(2) #+ '-' + date[1].zfill(2) + ' ' + date[3].zfill(2) + ':' + date[4].zfill(2)\n    return date\n\ndef call_llama(system_prompt, input):\n    template = f\"\"\"&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;{system_prompt}&lt;&lt;/SYS&gt;&gt;{input}[/INST]\"\"\"\n    client = boto3.client(service_name='bedrock-runtime',region_name='us-west-2')\n    body = json.dumps({\n        \"prompt\": template,\n        \"temperature\": 0.,\n        \"top_p\": 0.9,\n        \"max_gen_len\": 2048,\n    })\n    response = client.invoke_model(\n        body=body,\n        modelId='us.meta.llama3-2-90b-instruct-v1:0',\n        accept='application/json',\n        contentType='application/json'\n    )\n    response_body = json.loads(response['body'].read())\n    return response_body\n\ndef call_claude(system_prompt, input):\n    client = boto3.client(service_name='bedrock-runtime',region_name='us-west-2')\n    body=json.dumps(\n        {\n            \"anthropic_version\": \"bedrock-2023-05-31\",\n            \"max_tokens\": 2048,\n            \"messages\": [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": system_prompt + '\\n' + input,\n                    }\n                    ]\n                }\n                ]\n        }  \n    )  \n\n    \n    response = client.invoke_model(body=body, \n                                   modelId='anthropic.claude-3-5-sonnet-20241022-v2:0',\n                                   contentType='application/json',\n                                   accept='application/json')\n    response_body = json.loads(response.get('body').read())\n   \n    return response_body"
  },
  {
    "objectID": "notebook/sales_prediction.html#dataset",
    "href": "notebook/sales_prediction.html#dataset",
    "title": "Predict Zero-inflated and Long/fat-tailed Outcomes",
    "section": "Dataset",
    "text": "Dataset\nWe use the UCI Online Retail dataset, which are transactions from a UK-based, non-store online retail from 2010-12-01 and 2011-12-09. We perform the following data processing:\n\nRemove transactions without CustomerID; from 541,909 to 406,829 transactions\nFilter out transactions where either UnitPrice or Quantity is less than zero; from 406,829 to 397,884 transactions\nFill in missing product Description with value UNKNOWN.\n\n\nonline_retail = fetch_ucirepo(id=352) \ntransaction_df = online_retail['data']['original']\ntransaction_df.shape\n\n(541909, 8)\n\n\n\n#create yearmon for train-valid split\ntransaction_df['yearmon'] = transaction_df.InvoiceDate.map(string_to_yearmon)\n\n#get rid of transactions without cid\ntransaction_df = transaction_df[~transaction_df.CustomerID.isna()].reset_index(drop=True)\n\n#fill in unknown descriptions\ntransaction_df.Description = transaction_df.Description.fillna('UNKNOWN')\n\n#convert customer id to string\ntransaction_df['CustomerID'] = transaction_df['CustomerID'].map(lambda x: str(int(x)))\n\ntransaction_df.shape\n\n(406829, 9)\n\n\n\n#check if still na\ntransaction_df.isna().mean()\n\nInvoiceNo      0.0\nStockCode      0.0\nDescription    0.0\nQuantity       0.0\nInvoiceDate    0.0\nUnitPrice      0.0\nCustomerID     0.0\nCountry        0.0\nyearmon        0.0\ndtype: float64\n\n\n\n#simplify by filtering unit price and quantity to be non-zero (get rid of discounts, cancellations, etc)\ntransaction_df = transaction_df[(transaction_df.UnitPrice&gt;0)&\\\n                                (transaction_df.Quantity&gt;0)].reset_index(drop=True)\n#add sales\ntransaction_df['Sales'] = transaction_df.UnitPrice * transaction_df.Quantity\ntransaction_df.shape\n\n(397884, 10)"
  },
  {
    "objectID": "notebook/sales_prediction.html#problem-formulation-and-outcome",
    "href": "notebook/sales_prediction.html#problem-formulation-and-outcome",
    "title": "Predict Zero-inflated and Long/fat-tailed Outcomes",
    "section": "Problem Formulation and Outcome",
    "text": "Problem Formulation and Outcome\nWe formulate the problem as predicting the sales (TargetSales) during Q4 2011 for each customers who bought at least one item during Q1-Q3 2011. Note that we are interested in predicting the actual sales number per customer as accurately as possible; this is common for marketing use cases such as determining what spend threshold to give each customer in a promotion, targeting customers for upselling, or detecting early signs of churns.\nWe transform the transaction dataset into a customer-level dataset where we calculate features using transactions between 2011-01 to 2011-09 and outcome using transactions between 2011-10 to 2011-12, summing Quantity times UnitPrice. We left-join the customers in feature set to outcome set. This will result in the zero-inflated nature of the outcome as not all customers will come back in Q4. The distribution of non-zero sales is naturally long/fat-tailed with a few customers having extraordinarily high amount of sales in Q4. This resulted in a customer-level dataset with 3,438 customers.\n\nfeature_period = {'start': '2011-01', 'end': '2011-09'}\noutcome_period = {'start': '2011-10', 'end': '2011-12'}\n\nfeature_transaction = transaction_df[(transaction_df.yearmon&gt;=feature_period['start'])&\\\n                                      (transaction_df.yearmon&lt;=feature_period['end'])]\noutcome_transaction = transaction_df[(transaction_df.yearmon&gt;=outcome_period['start'])&\\\n                                      (transaction_df.yearmon&lt;=outcome_period['end'])]\nfeature_transaction.shape, outcome_transaction.shape\n\n((240338, 10), (131389, 10))\n\n\n\n#aggregate sales during outcome period\noutcome_sales = outcome_transaction.groupby('CustomerID').Sales.sum().reset_index()\noutcome_sales\n\n\n\n\n\n\n\n\nCustomerID\nSales\n\n\n\n\n0\n12347\n1519.14\n\n\n1\n12349\n1757.55\n\n\n2\n12352\n311.73\n\n\n3\n12356\n58.35\n\n\n4\n12357\n6207.67\n\n\n...\n...\n...\n\n\n2555\n18276\n335.86\n\n\n2556\n18277\n110.38\n\n\n2557\n18282\n77.84\n\n\n2558\n18283\n974.21\n\n\n2559\n18287\n1072.00\n\n\n\n\n2560 rows √ó 2 columns\n\n\n\n\n#aggregate sales during feature period\nfeature_sales = feature_transaction.groupby('CustomerID').Sales.sum().reset_index()\nfeature_sales\n\n\n\n\n\n\n\n\nCustomerID\nSales\n\n\n\n\n0\n12346\n77183.60\n\n\n1\n12347\n2079.07\n\n\n2\n12348\n904.44\n\n\n3\n12350\n334.40\n\n\n4\n12352\n2194.31\n\n\n...\n...\n...\n\n\n3433\n18280\n180.60\n\n\n3434\n18281\n80.82\n\n\n3435\n18282\n100.21\n\n\n3436\n18283\n1120.67\n\n\n3437\n18287\n765.28\n\n\n\n\n3438 rows √ó 2 columns\n\n\n\n\n#merge to get TargetSales including those who spent during feature period but not during outcome (zeroes)\noutcome_df = feature_sales[['CustomerID']].merge(outcome_sales, on='CustomerID', how='left')\noutcome_df['Sales'] = outcome_df['Sales'].fillna(0)\noutcome_df.columns = ['CustomerID', 'TargetSales']\noutcome_df\n\n\n\n\n\n\n\n\nCustomerID\nTargetSales\n\n\n\n\n0\n12346\n0.00\n\n\n1\n12347\n1519.14\n\n\n2\n12348\n0.00\n\n\n3\n12350\n0.00\n\n\n4\n12352\n311.73\n\n\n...\n...\n...\n\n\n3433\n18280\n0.00\n\n\n3434\n18281\n0.00\n\n\n3435\n18282\n77.84\n\n\n3436\n18283\n974.21\n\n\n3437\n18287\n1072.00\n\n\n\n\n3438 rows √ó 2 columns\n\n\n\n\n#confirm zero-inflated, long/fat-tailed\noutcome_df.TargetSales.describe(percentiles=[i/10 for i in range(10)])\n\ncount      3438.000000\nmean        666.245829\nstd        4016.843037\nmin           0.000000\n0%            0.000000\n10%           0.000000\n20%           0.000000\n30%           0.000000\n40%           0.000000\n50%         102.005000\n60%         263.006000\n70%         425.790000\n80%         705.878000\n90%        1273.611000\nmax      168469.600000\nName: TargetSales, dtype: float64\n\n\n\n#confirm zero-inflated, long/fat-tailed\noutcome_df[outcome_df.TargetSales&lt;=10_000].TargetSales.hist(bins=100)"
  },
  {
    "objectID": "notebook/sales_prediction.html#feature",
    "href": "notebook/sales_prediction.html#feature",
    "title": "Predict Zero-inflated and Long/fat-tailed Outcomes",
    "section": "Feature",
    "text": "Feature\nWe represent a customer using traditional RFM features namely recency of purchase, purchase days, total sales, number of distinct products purchased, number of distinct category purchased, customer tenure within 2011, average purchase frequency, average purchase value, and percentage of purchase across all 9 categories. This is based on data from Q1-Q3 2011.\nSince the UCI Online Retail dataset does not have a category but only contains descriptions over 3,000 items, we use LLaMA 3.2 90B to infer categories based on randomly selected 1,000 descriptions. This is to make the category preference representation for each customer, which is more tractable than including features about all 3,000+ items. After that, we use the same model to label a category for each description. The categories are:\n\nHome Decor\nKitchen and Dining\nFashion Accessories\nStationary and Gifts\nToys and Games\nSeasonal and Holiday\nPersonal Care and Wellness\nOutdoor and Garden\nOthers\n\n\nClassify Description into Category\n\nfeature_transaction.Description.nunique()\n\n3548\n\n\n\nGet Category\n\ndescriptions = feature_transaction.Description.unique().tolist()\nprint(descriptions[:5])\n\n#randomize descriptions with seed 112 to get which categories we should use\nnp.random.seed(112)\nrandom_descriptions = np.random.choice(descriptions, 1000, replace=False)\nprint(random_descriptions[:5])\n\n['JUMBO BAG PINK POLKADOT', 'BLUE POLKADOT WRAP', 'RED RETROSPOT WRAP ', 'RECYCLING BAG RETROSPOT ', 'RED RETROSPOT SHOPPER BAG']\n['MODERN FLORAL STATIONERY SET' 'PURPLE BERTIE GLASS BEAD BAG CHARM'\n 'PARTY INVITES SPACEMAN' 'MONTANA DIAMOND CLUSTER EARRINGS'\n 'SKULLS  DESIGN  COTTON TOTE BAG']\n\n\n\n# res = call_llama(\n#     'You are a product categorization assistant at a retail website.',\n#     'Given the following product descriptions, come up with a few product categories they should be classified into.'+'\\n'.join(random_descriptions)\n#     )\n\n# print(res['generation'])\n\n\n# res\n\n\n# res = call_claude(\n#     'You are a product categorization assistant at a retail website.',\n#     'Given the following product descriptions, come up with a few product categories they should be classified into.'+'\\n'.join(random_descriptions)\n#     )\n\n# print(res['content'][0]['text'])\n\n\n# res\n\nLLaMA 3.2 90B Output:\n&lt;&lt;SYS&gt;&gt;Based on the product descriptions, I would categorize them into the following categories:\n\n1. Home Decor:\n    * Wall art\n    * Decorative items (e.g. vases, figurines, etc.)\n    * Lighting (e.g. candles, lanterns, etc.)\n    * Textiles (e.g. throw pillows, blankets, etc.)\n2. Kitchen and Dining:\n    * Cookware and utensils\n    * Tableware (e.g. plates, cups, etc.)\n    * Kitchen decor (e.g. signs, magnets, etc.)\n    * Food and drink items (e.g. tea, coffee, etc.)\n3. Fashion and Accessories:\n    * Jewelry (e.g. necklaces, earrings, etc.)\n    * Handbags and wallets\n    * Clothing and accessories (e.g. scarves, hats, etc.)\n4. Stationery and Gifts:\n    * Cards and gift wrap\n    * Stationery (e.g. notebooks, pens, etc.)\n    * Gift items (e.g. mugs, keychains, etc.)\n5. Toys and Games:\n    * Toys (e.g. stuffed animals, puzzles, etc.)\n    * Games and puzzles\n6. Seasonal and Holiday:\n    * Christmas decorations and gifts\n    * Easter decorations and gifts\n    * Other seasonal items (e.g. Halloween, etc.)\n7. Personal Care and Wellness:\n    * Beauty and personal care items (e.g. skincare, haircare, etc.)\n    * Wellness and self-care items (e.g. essential oils, etc.)\n8. Outdoor and Garden:\n    * Garden decor and accessories\n    * Outdoor furniture and decor\n    * Gardening tools and supplies\n\nNote that some products may fit into multiple categories, but I have assigned them to the one that seems most relevant.\nClaude 3.5 v2 Output\nBased on these product descriptions, I would suggest the following main product categories:\n\n1. Home Decor\n- Candle holders\n- Picture frames\n- Wall art & signs\n- Clocks\n- Cushions & covers\n- Storage items\n- Decorative objects\n\n2. Jewelry & Accessories\n- Necklaces\n- Bracelets\n- Earrings\n- Hair accessories\n- Bag charms\n- Key rings\n\n3. Garden & Outdoor\n- Plant pots\n- Garden tools\n- Outdoor decorations\n- Bird houses\n- Garden markers\n\n4. Kitchen & Dining\n- Tea sets\n- Mugs\n- Kitchen storage\n- Cutlery\n- Baking accessories\n- Tea towels\n\n5. Stationery & Paper Goods\n- Notebooks\n- Gift wrap\n- Cards\n- Paper decorations\n- Writing sets\n\n6. Party & Celebrations\n- Party supplies\n- Gift bags\n- Christmas decorations\n- Easter items\n- Birthday items\n\n7. Children's Items\n- Toys\n- Children's tableware\n- School supplies\n- Kids' accessories\n\n8. Fashion Accessories\n- Bags\n- Purses\n- Scarves\n- Travel accessories\n\n9. Bath & Beauty\n- Bathroom accessories\n- Toiletry bags\n- Beauty items\n\n10. Lighting\n- Lamps\n- String lights\n- Tea lights\n- Lanterns\n\nThese categories cover the main types of products in the list while providing logical groupings for customers to browse.\n\ncategories = [\n    'Home Decor',\n    'Kitchen and Dining',\n    'Fashion Accessories',\n    'Stationary and Gifts',\n    'Toys and Games',\n    'Seasonal and Holiday',\n    'Personal Care and Wellness',\n    'Outdoor and Garden',   \n]\n\nlen(categories)\n\n8\n\n\n\n\nAnnotate Category to Description\n\n# #loop through descriptions in batches of batch_size\n# res_texts = []\n# batch_size = 100\n# for i in tqdm(range(0, len(descriptions), batch_size)):\n#     batch = descriptions[i:i+batch_size]\n#     d = \"\\n\".join(batch)\n#     inp = f'''Categorize the following product descriptions into {\", \".join(categories)} or Others, if they do not fall into any. \n# Only answer in the following format:\n\n# \"product description of product #1\"|\"product category classified into\"\n# \"product description of product #2\"|\"product category classified into\"\n# ...\n# \"product description of product #n\"|\"product category classified into\"\n\n# Here are the product descriptions:\n# {d}\n# '''\n#     while True:\n#         res = call_claude('You are a product categorizer at a retail website', inp)\n#         # if res['generation_token_count'] &gt; 1: #for llama\n#         if res['usage']['output_tokens'] &gt; 1:\n#             break\n#         else:\n#             print('Retrying...')\n#             time.sleep(2)\n#     res_text = res['content'][0]['text'].strip().split('\\n')\n#         #for llama\n#         # .replace('[SYS]','').replace('&lt;&lt;SYS&gt;&gt;','')\\\n#         # .replace('[/SYS]','').replace('&lt;&lt;/SYS&gt;&gt;','')\\\n#     if res_text!='':\n#         res_texts.extend(res_text)\n\n\n# with open('../data/sales_prediction/product_description_category.csv','w') as f:\n#     f.write('\"product_description\"|\"category\"\\n')\n#     for i in res_texts:\n#         f.write(f'{i}\\n')\n\n\nproduct_description_category = pd.read_csv('../data/sales_prediction/product_description_category.csv',\n                                           sep='|')\n\n#clean product_description\nproduct_description_category['Description'] = descriptions\nproduct_description_category.category.value_counts(normalize=True)\n\ncategory\nHome Decor                    0.328636\nKitchen and Dining            0.195885\nFashion Accessories           0.138670\nStationary and Gifts          0.116122\nSeasonal and Holiday          0.087373\nPersonal Care and Wellness    0.047351\nToys and Games                0.045096\nOutdoor and Garden            0.032976\nOthers                        0.007892\nName: proportion, dtype: float64\n\n\n\nfeature_transaction_cat = feature_transaction.merge(product_description_category,\n                                                    how='inner',\n                                                    on = 'Description',)\nfeature_transaction.shape, feature_transaction_cat.shape\n\n((240338, 10), (240338, 12))\n\n\n\n\n\nRFM\n\n#convert invoice date to datetime\nfeature_transaction_cat['InvoiceDate'] = pd.to_datetime(feature_transaction_cat['InvoiceDate'])\n\n# last date in feature set\ncurrent_date = feature_transaction_cat['InvoiceDate'].max()\n\n#rfm\ncustomer_features = feature_transaction_cat.groupby('CustomerID').agg({\n    'InvoiceDate': [\n        ('recency', lambda x: (current_date - x.max()).days),\n        ('first_purchase_date', 'min'),\n        ('purchase_day', 'nunique'),\n    ],\n    'InvoiceNo': [('nb_invoice', 'nunique')],\n    'Sales': [\n        ('total_sales', 'sum')\n    ],\n    'StockCode': [('nb_product', 'nunique')],\n    'category': [('nb_category', 'nunique')]\n}).reset_index()\n\n# Flatten column names\ncustomer_features.columns = [\n    'CustomerID',\n    'recency',\n    'first_purchase_date',\n    'purchase_day',\n    'nb_invoice',\n    'total_sales',\n    'nb_product',\n    'nb_category'\n]\n\n\n#almost always one purchase a day\n(customer_features.purchase_day==customer_features.nb_invoice).mean()\n\n0.977021524141943\n\n\n\ncustomer_features['customer_lifetime'] = (current_date - customer_features['first_purchase_date']).dt.days\ncustomer_features['avg_purchase_frequency'] = customer_features['customer_lifetime'] / customer_features['purchase_day']\ncustomer_features['avg_purchase_value'] = customer_features['total_sales'] / customer_features['purchase_day']\n\n\n\nCategory Preference\n\n#category preference\ncategory_sales = feature_transaction_cat.pivot_table(\n    values='Sales', \n    index='CustomerID', \n    columns='category', \n    aggfunc='sum', \n    fill_value=0\n)\ncategory_sales.columns = [i.lower().replace(' ','_') for i in category_sales.columns]\ncustomer_features = customer_features.merge(category_sales, on='CustomerID', how='left')\n\ntotal_sales = customer_features['total_sales']\nfor col in category_sales.columns:\n    percentage_col = f'per_{col}'\n    customer_features[percentage_col] = customer_features[col] / total_sales\n\n\n#make sure the categories are not too sparse\n(customer_features.iloc[:,-9:]==0).mean()\n\nper_fashion_accessories           0.409831\nper_home_decor                    0.081734\nper_kitchen_and_dining            0.122455\nper_others                        0.765561\nper_outdoor_and_garden            0.507853\nper_personal_care_and_wellness    0.448226\nper_seasonal_and_holiday          0.369401\nper_stationary_and_gifts          0.305410\nper_toys_and_games                0.487202\ndtype: float64\n\n\n\n\nPutting Them All Together\n\nselected_features = [\n 'recency',\n 'purchase_day',\n 'total_sales',\n 'nb_product',\n 'nb_category',\n 'customer_lifetime',\n 'avg_purchase_frequency',\n 'avg_purchase_value',\n 'per_fashion_accessories',\n 'per_home_decor',\n 'per_kitchen_and_dining',\n 'per_others',\n 'per_outdoor_and_garden',\n 'per_personal_care_and_wellness',\n 'per_seasonal_and_holiday',\n 'per_stationary_and_gifts',\n 'per_toys_and_games']\n\noutcome_variable = 'TargetSales'\n\n\ncustomer_features = customer_features[[ 'CustomerID']+selected_features]\ncustomer_features.head()\n\n\n\n\n\n\n\n\nCustomerID\nrecency\npurchase_day\ntotal_sales\nnb_product\nnb_category\ncustomer_lifetime\navg_purchase_frequency\navg_purchase_value\nper_fashion_accessories\nper_home_decor\nper_kitchen_and_dining\nper_others\nper_outdoor_and_garden\nper_personal_care_and_wellness\nper_seasonal_and_holiday\nper_stationary_and_gifts\nper_toys_and_games\n\n\n\n\n0\n12346\n255\n1\n77183.60\n1\n1\n255\n255.000000\n77183.600000\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n1\n12347\n59\n4\n2079.07\n65\n7\n247\n61.750000\n519.767500\n0.145834\n0.204168\n0.294021\n0.000000\n0.005628\n0.147614\n0.000000\n0.073013\n0.129721\n\n\n2\n12348\n5\n3\n904.44\n10\n4\n248\n82.666667\n301.480000\n0.000000\n0.000000\n0.000000\n0.132679\n0.000000\n0.825970\n0.018796\n0.022555\n0.000000\n\n\n3\n12350\n239\n1\n334.40\n17\n7\n239\n239.000000\n334.400000\n0.240431\n0.202751\n0.116926\n0.172548\n0.000000\n0.118421\n0.000000\n0.059211\n0.089713\n\n\n4\n12352\n2\n7\n2194.31\n47\n8\n226\n32.285714\n313.472857\n0.000000\n0.196531\n0.246187\n0.474090\n0.013535\n0.016680\n0.008066\n0.024404\n0.020508"
  },
  {
    "objectID": "notebook/sales_prediction.html#merge-features-and-outcome",
    "href": "notebook/sales_prediction.html#merge-features-and-outcome",
    "title": "Predict Zero-inflated and Long/fat-tailed Outcomes",
    "section": "Merge Features and Outcome",
    "text": "Merge Features and Outcome\n\ncustomer_features.shape, outcome_df.shape\n\n((3438, 18), (3438, 2))\n\n\n\ndf = outcome_df.merge(customer_features, on='CustomerID').drop('CustomerID', axis=1)\ndf.shape\n\n(3438, 18)\n\n\n\n#correlations\ndf.iloc[:,1:].corr()\n\n\n\n\n\n\n\n\nrecency\npurchase_day\ntotal_sales\nnb_product\nnb_category\ncustomer_lifetime\navg_purchase_frequency\navg_purchase_value\nper_fashion_accessories\nper_home_decor\nper_kitchen_and_dining\nper_others\nper_outdoor_and_garden\nper_personal_care_and_wellness\nper_seasonal_and_holiday\nper_stationary_and_gifts\nper_toys_and_games\n\n\n\n\nrecency\n1.000000\n-0.299308\n-0.132344\n-0.287415\n-0.326772\n0.298853\n0.893973\n0.008823\n-0.020861\n0.022013\n0.057244\n-0.016069\n0.071268\n-0.082792\n-0.085681\n-0.017813\n-0.009686\n\n\npurchase_day\n-0.299308\n1.000000\n0.540253\n0.690345\n0.304621\n0.332109\n-0.331543\n0.027488\n0.030683\n0.018684\n0.025269\n0.004299\n-0.019992\n-0.035665\n-0.020392\n-0.045384\n-0.028187\n\n\ntotal_sales\n-0.132344\n0.540253\n1.000000\n0.400467\n0.137064\n0.156018\n-0.148762\n0.361138\n0.016511\n-0.013819\n0.047834\n0.006398\n-0.029353\n-0.011937\n-0.016724\n-0.029181\n-0.013139\n\n\nnb_product\n-0.287415\n0.690345\n0.400467\n1.000000\n0.555551\n0.265594\n-0.294923\n0.061039\n-0.003137\n-0.017516\n0.035615\n-0.006842\n-0.026371\n-0.005309\n-0.016586\n0.026716\n-0.010069\n\n\nnb_category\n-0.326772\n0.304621\n0.137064\n0.555551\n1.000000\n0.224232\n-0.321596\n0.019955\n0.004863\n-0.138372\n-0.039363\n0.055555\n0.041405\n0.075882\n0.015498\n0.152869\n0.111150\n\n\ncustomer_lifetime\n0.298853\n0.332109\n0.156018\n0.265594\n0.224232\n1.000000\n0.358431\n0.014933\n0.011220\n0.066111\n0.069175\n-0.019971\n0.029726\n-0.127865\n-0.120399\n-0.050320\n-0.036484\n\n\navg_purchase_frequency\n0.893973\n-0.331543\n-0.148762\n-0.294923\n-0.321596\n0.358431\n1.000000\n0.009157\n-0.016093\n0.027208\n0.037053\n-0.027413\n0.060369\n-0.070352\n-0.074799\n-0.000546\n-0.010612\n\n\navg_purchase_value\n0.008823\n0.027488\n0.361138\n0.061039\n0.019955\n0.014933\n0.009157\n1.000000\n-0.003187\n-0.056690\n0.076862\n0.015427\n-0.028884\n0.004225\n-0.000200\n-0.012729\n-0.002396\n\n\nper_fashion_accessories\n-0.020861\n0.030683\n0.016511\n-0.003137\n0.004863\n0.011220\n-0.016093\n-0.003187\n1.000000\n-0.254015\n-0.177775\n-0.010436\n-0.082834\n-0.038493\n-0.124719\n-0.068166\n-0.051486\n\n\nper_home_decor\n0.022013\n0.018684\n-0.013819\n-0.017516\n-0.138372\n0.066111\n0.027208\n-0.056690\n-0.254015\n1.000000\n-0.481983\n-0.155784\n-0.080637\n-0.158837\n-0.165964\n-0.262313\n-0.245759\n\n\nper_kitchen_and_dining\n0.057244\n0.025269\n0.047834\n0.035615\n-0.039363\n0.069175\n0.037053\n0.076862\n-0.177775\n-0.481983\n1.000000\n-0.013075\n-0.144698\n-0.117031\n-0.204235\n-0.173386\n-0.143931\n\n\nper_others\n-0.016069\n0.004299\n0.006398\n-0.006842\n0.055555\n-0.019971\n-0.027413\n0.015427\n-0.010436\n-0.155784\n-0.013075\n1.000000\n-0.062652\n0.014794\n-0.047940\n-0.033975\n-0.040421\n\n\nper_outdoor_and_garden\n0.071268\n-0.019992\n-0.029353\n-0.026371\n0.041405\n0.029726\n0.060369\n-0.028884\n-0.082834\n-0.080637\n-0.144698\n-0.062652\n1.000000\n-0.045639\n-0.077947\n-0.057297\n-0.001034\n\n\nper_personal_care_and_wellness\n-0.082792\n-0.035665\n-0.011937\n-0.005309\n0.075882\n-0.127865\n-0.070352\n0.004225\n-0.038493\n-0.158837\n-0.117031\n0.014794\n-0.045639\n1.000000\n-0.057926\n-0.025871\n-0.017022\n\n\nper_seasonal_and_holiday\n-0.085681\n-0.020392\n-0.016724\n-0.016586\n0.015498\n-0.120399\n-0.074799\n-0.000200\n-0.124719\n-0.165964\n-0.204235\n-0.047940\n-0.077947\n-0.057926\n1.000000\n-0.019418\n-0.042970\n\n\nper_stationary_and_gifts\n-0.017813\n-0.045384\n-0.029181\n0.026716\n0.152869\n-0.050320\n-0.000546\n-0.012729\n-0.068166\n-0.262313\n-0.173386\n-0.033975\n-0.057297\n-0.025871\n-0.019418\n1.000000\n0.172039\n\n\nper_toys_and_games\n-0.009686\n-0.028187\n-0.013139\n-0.010069\n0.111150\n-0.036484\n-0.010612\n-0.002396\n-0.051486\n-0.245759\n-0.143931\n-0.040421\n-0.001034\n-0.017022\n-0.042970\n0.172039\n1.000000\n\n\n\n\n\n\n\n\n#target and most predictive variable\ndf[df.TargetSales&lt;=25_000].plot.scatter(x='TargetSales',y='total_sales')"
  },
  {
    "objectID": "notebook/sales_prediction.html#train-test-splits",
    "href": "notebook/sales_prediction.html#train-test-splits",
    "title": "Predict Zero-inflated and Long/fat-tailed Outcomes",
    "section": "Train-Test Splits",
    "text": "Train-Test Splits\nWe randomly split the dataset into train and test sets at 80/20 ratio. We also confirm the distribution of TargetSales is similar across percentiles and only different at the upper end.\n\n#split into train-valid sets\ntrain_df, test_df = train_test_split(df,\n                                      test_size=0.2, \n                                      random_state=112)\n\n\npd.concat([train_df.TargetSales.describe(percentiles=[i/10 for i in range(10)]).reset_index(),\ntest_df.TargetSales.describe(percentiles=[i/10 for i in range(10)]).reset_index(),], axis=1)\n\n\n\n\n\n\n\n\nindex\nTargetSales\nindex\nTargetSales\n\n\n\n\n0\ncount\n2750.000000\ncount\n688.000000\n\n\n1\nmean\n642.650436\nmean\n760.558808\n\n\n2\nstd\n4015.305436\nstd\n4024.524400\n\n\n3\nmin\n0.000000\nmin\n0.000000\n\n\n4\n0%\n0.000000\n0%\n0.000000\n\n\n5\n10%\n0.000000\n10%\n0.000000\n\n\n6\n20%\n0.000000\n20%\n0.000000\n\n\n7\n30%\n0.000000\n30%\n0.000000\n\n\n8\n40%\n0.000000\n40%\n0.000000\n\n\n9\n50%\n91.350000\n50%\n113.575000\n\n\n10\n60%\n260.308000\n60%\n277.836000\n\n\n11\n70%\n426.878000\n70%\n418.187000\n\n\n12\n80%\n694.164000\n80%\n759.582000\n\n\n13\n90%\n1272.997000\n90%\n1255.670000\n\n\n14\nmax\n168469.600000\nmax\n77099.380000"
  },
  {
    "objectID": "notebook/sales_prediction.html#baseline-regression",
    "href": "notebook/sales_prediction.html#baseline-regression",
    "title": "Predict Zero-inflated and Long/fat-tailed Outcomes",
    "section": "Baseline Regression",
    "text": "Baseline Regression\nThe most naive solution is to predict TargetSales based on the features. We use a stacked ensemble of LightGBM, CatBoost, XGBoost, Random Forest and Extra Trees via AutoGluon. We train with good_quality preset, stated to be ‚ÄúStronger than any other AutoML Framework‚Äù, for speedy training and inference but feel free to try more performant option. We exclude the neural-network models as they require further preprocessing of the features.\nWe use an industry-grade, non-parametric model to be as close to a real use case as possible and make a point that our methodology works not only in a toy-dataset setup.\n\npreset = 'good_quality'\n\n\npredictor = TabularPredictor(label='TargetSales').fit(train_df[selected_features + ['TargetSales']], \n                                                      presets=preset,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20241126_101003\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.1.1\nPython Version:     3.9.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP Wed Oct 23 01:22:11 UTC 2024\nCPU Count:          64\nMemory Avail:       470.86 GB / 480.23 GB (98.0%)\nDisk Space Avail:   1541.48 GB / 1968.52 GB (78.3%)\n===================================================\nPresets specified: ['good_quality']\nSetting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\nStack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\nNote: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n    You can avoid this risk by setting `save_bag_folds=True`.\nDyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n    This is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n    Running DyStack for up to 900s of the 3600s of remaining time (25%).\n2024-11-26 10:10:03,806 INFO util.py:154 -- Outdated packages:\n  ipywidgets==7.6.5 found, needs ipywidgets&gt;=8\nRun `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n    Running DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n2024-11-26 10:10:06,936 INFO worker.py:1743 -- Started a local Ray instance. View the dashboard at 127.0.0.1:8265 \n        Context path: \"AutogluonModels/ag-20241126_101003/ds_sub_fit/sub_fit_ho\"\n(_dystack pid=83268) Running DyStack sub-fit ...\n(_dystack pid=83268) Beginning AutoGluon training ... Time limit = 896s\n(_dystack pid=83268) AutoGluon will save models to \"AutogluonModels/ag-20241126_101003/ds_sub_fit/sub_fit_ho\"\n(_dystack pid=83268) Train Data Rows:    2444\n(_dystack pid=83268) Train Data Columns: 17\n(_dystack pid=83268) Label Column:       TargetSales\n(_dystack pid=83268) Problem Type:       regression\n(_dystack pid=83268) Preprocessing data ...\n(_dystack pid=83268) Using Feature Generators to preprocess the data ...\n(_dystack pid=83268) Fitting AutoMLPipelineFeatureGenerator...\n(_dystack pid=83268)    Available Memory:                    481311.53 MB\n(_dystack pid=83268)    Train Data (Original)  Memory Usage: 0.32 MB (0.0% of available memory)\n(_dystack pid=83268)    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n(_dystack pid=83268)    Stage 1 Generators:\n(_dystack pid=83268)        Fitting AsTypeFeatureGenerator...\n(_dystack pid=83268)    Stage 2 Generators:\n(_dystack pid=83268)        Fitting FillNaFeatureGenerator...\n(_dystack pid=83268)    Stage 3 Generators:\n(_dystack pid=83268)        Fitting IdentityFeatureGenerator...\n(_dystack pid=83268)    Stage 4 Generators:\n(_dystack pid=83268)        Fitting DropUniqueFeatureGenerator...\n(_dystack pid=83268)    Stage 5 Generators:\n(_dystack pid=83268)        Fitting DropDuplicatesFeatureGenerator...\n(_dystack pid=83268)    Types of features in original data (raw dtype, special dtypes):\n(_dystack pid=83268)        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n(_dystack pid=83268)        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n(_dystack pid=83268)    Types of features in processed data (raw dtype, special dtypes):\n(_dystack pid=83268)        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n(_dystack pid=83268)        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n(_dystack pid=83268)    0.0s = Fit runtime\n(_dystack pid=83268)    17 features in original data used to generate 17 features in processed data.\n(_dystack pid=83268)    Train Data (Processed) Memory Usage: 0.32 MB (0.0% of available memory)\n(_dystack pid=83268) Data preprocessing and feature engineering runtime = 0.05s ...\n(_dystack pid=83268) AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n(_dystack pid=83268)    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n(_dystack pid=83268)    To change this, specify the eval_metric parameter of Predictor()\n(_dystack pid=83268) User-specified model hyperparameters to be fit:\n(_dystack pid=83268) {\n(_dystack pid=83268)    'NN_TORCH': {},\n(_dystack pid=83268)    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n(_dystack pid=83268)    'CAT': {},\n(_dystack pid=83268)    'XGB': {},\n(_dystack pid=83268)    'FASTAI': {},\n(_dystack pid=83268)    'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n(_dystack pid=83268)    'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n(_dystack pid=83268) }\n(_dystack pid=83268) AutoGluon will fit 2 stack levels (L1 to L2) ...\n(_dystack pid=83268) Excluded models: ['FASTAI', 'NN_TORCH'] (Specified by `excluded_model_types`)\n(_dystack pid=83268) Fitting 7 L1 models ...\n(_dystack pid=83268) Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 596.87s of the 895.53s of remaining time.\n(_dystack pid=83268)    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n(_dystack pid=83268)    -3990.4801   = Validation score   (-root_mean_squared_error)\n(_dystack pid=83268)    1.93s    = Training   runtime\n(_dystack pid=83268)    0.02s    = Validation runtime\n(_dystack pid=83268) Fitting model: LightGBM_BAG_L1 ... Training model for up to 592.65s of the 891.31s of remaining time.\n(_dystack pid=83268)    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n(_dystack pid=83268)    -3921.7042   = Validation score   (-root_mean_squared_error)\n(_dystack pid=83268)    2.22s    = Training   runtime\n(_dystack pid=83268)    0.02s    = Validation runtime\n(_dystack pid=83268) Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 588.21s of the 886.87s of remaining time.\n(_dystack pid=83268)    -4516.1791   = Validation score   (-root_mean_squared_error)\n(_dystack pid=83268)    0.96s    = Training   runtime\n(_dystack pid=83268)    0.18s    = Validation runtime\n(_dystack pid=83268) Fitting model: CatBoost_BAG_L1 ... Training model for up to 586.98s of the 885.63s of remaining time.\n(_dystack pid=83268)    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n(_dystack pid=83268)    -3857.3111   = Validation score   (-root_mean_squared_error)\n(_dystack pid=83268)    3.16s    = Training   runtime\n(_dystack pid=83268)    0.03s    = Validation runtime\n(_dystack pid=83268) Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 581.54s of the 880.2s of remaining time.\n(_dystack pid=83268)    -3900.3038   = Validation score   (-root_mean_squared_error)\n(_dystack pid=83268)    0.82s    = Training   runtime\n(_dystack pid=83268)    0.17s    = Validation runtime\n(_dystack pid=83268) Fitting model: XGBoost_BAG_L1 ... Training model for up to 580.49s of the 879.15s of remaining time.\n(_dystack pid=83268)    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n(_dystack pid=83268)    -3941.3599   = Validation score   (-root_mean_squared_error)\n(_dystack pid=83268)    1.94s    = Training   runtime\n(_dystack pid=83268)    0.03s    = Validation runtime\n(_dystack pid=83268) Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 576.34s of the 875.0s of remaining time.\n(_dystack pid=83268)    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n(_dystack pid=83268)    -3912.54     = Validation score   (-root_mean_squared_error)\n(_dystack pid=83268)    3.68s    = Training   runtime\n(_dystack pid=83268)    0.02s    = Validation runtime\n(_dystack pid=83268) Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 869.04s of remaining time.\n(_dystack pid=83268)    Ensemble Weights: {'CatBoost_BAG_L1': 0.6, 'ExtraTreesMSE_BAG_L1': 0.36, 'LightGBM_BAG_L1': 0.04}\n(_dystack pid=83268)    -3835.4224   = Validation score   (-root_mean_squared_error)\n(_dystack pid=83268)    0.02s    = Training   runtime\n(_dystack pid=83268)    0.0s     = Validation runtime\n(_dystack pid=83268) Excluded models: ['FASTAI', 'NN_TORCH'] (Specified by `excluded_model_types`)\n(_dystack pid=83268) Fitting 7 L2 models ...\n(_dystack pid=83268) Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 868.99s of the 868.99s of remaining time.\n(_dystack pid=83268)    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n(_dystack pid=83268)    -3941.7891   = Validation score   (-root_mean_squared_error)\n(_dystack pid=83268)    1.85s    = Training   runtime\n(_dystack pid=83268)    0.02s    = Validation runtime\n(_dystack pid=83268) Fitting model: LightGBM_BAG_L2 ... Training model for up to 864.91s of the 864.91s of remaining time.\n(_dystack pid=83268)    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n\n\n(_ray_fit pid=93902) [1000] valid_set's rmse: 4314.99\n\n\n(_dystack pid=83268)    -3894.7078   = Validation score   (-root_mean_squared_error)\n(_dystack pid=83268)    2.77s    = Training   runtime\n(_dystack pid=83268)    0.02s    = Validation runtime\n(_dystack pid=83268) Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 859.86s of the 859.85s of remaining time.\n(_dystack pid=83268)    -4525.2057   = Validation score   (-root_mean_squared_error)\n(_dystack pid=83268)    0.97s    = Training   runtime\n(_dystack pid=83268)    0.17s    = Validation runtime\n(_dystack pid=83268) Fitting model: CatBoost_BAG_L2 ... Training model for up to 858.63s of the 858.62s of remaining time.\n(_dystack pid=83268)    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n(_dystack pid=83268)    -3904.7749   = Validation score   (-root_mean_squared_error)\n(_dystack pid=83268)    2.19s    = Training   runtime\n(_dystack pid=83268)    0.03s    = Validation runtime\n(_dystack pid=83268) Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 854.22s of the 854.21s of remaining time.\n(_dystack pid=83268)    -3952.2022   = Validation score   (-root_mean_squared_error)\n(_dystack pid=83268)    0.82s    = Training   runtime\n(_dystack pid=83268)    0.17s    = Validation runtime\n(_dystack pid=83268) Fitting model: XGBoost_BAG_L2 ... Training model for up to 853.14s of the 853.13s of remaining time.\n(_dystack pid=83268)    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n(_dystack pid=83268)    -3929.2019   = Validation score   (-root_mean_squared_error)\n(_dystack pid=83268)    3.11s    = Training   runtime\n(_dystack pid=83268)    0.04s    = Validation runtime\n(_dystack pid=83268) Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 847.75s of the 847.74s of remaining time.\n(_dystack pid=83268)    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n(_dystack pid=83268)    -3912.6409   = Validation score   (-root_mean_squared_error)\n(_dystack pid=83268)    4.46s    = Training   runtime\n(_dystack pid=83268)    0.02s    = Validation runtime\n(_dystack pid=83268) Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 841.06s of remaining time.\n(_dystack pid=83268)    Ensemble Weights: {'CatBoost_BAG_L1': 0.36, 'ExtraTreesMSE_BAG_L1': 0.32, 'LightGBM_BAG_L2': 0.32}\n(_dystack pid=83268)    -3823.1639   = Validation score   (-root_mean_squared_error)\n(_dystack pid=83268)    0.03s    = Training   runtime\n(_dystack pid=83268)    0.0s     = Validation runtime\n(_dystack pid=83268) AutoGluon training complete, total runtime = 54.61s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1766.1 rows/s (306 batch size)\n(_dystack pid=83268) Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n(_dystack pid=83268) Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n(_dystack pid=83268)    Models trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n(_dystack pid=83268)    This process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n(_dystack pid=83268)    To learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n(_dystack pid=83268) Fitting 1 L1 models ...\n(_dystack pid=83268) Fitting model: LightGBMXT_BAG_L1_FULL ...\n(_dystack pid=83268)    1.17s    = Training   runtime\n(_dystack pid=83268) Fitting 1 L1 models ...\n(_dystack pid=83268) Fitting model: LightGBM_BAG_L1_FULL ...\n(_dystack pid=83268)    0.19s    = Training   runtime\n(_dystack pid=83268) Fitting model: RandomForestMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n(_dystack pid=83268)    0.96s    = Training   runtime\n(_dystack pid=83268)    0.18s    = Validation runtime\n(_dystack pid=83268) Fitting 1 L1 models ...\n(_dystack pid=83268) Fitting model: CatBoost_BAG_L1_FULL ...\n(_dystack pid=83268)    0.99s    = Training   runtime\n(_dystack pid=83268) Fitting model: ExtraTreesMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n(_dystack pid=83268)    0.82s    = Training   runtime\n(_dystack pid=83268)    0.17s    = Validation runtime\n(_dystack pid=83268) Fitting 1 L1 models ...\n(_dystack pid=83268) Fitting model: XGBoost_BAG_L1_FULL ...\n(_dystack pid=83268)    0.18s    = Training   runtime\n(_dystack pid=83268) Fitting 1 L1 models ...\n(_dystack pid=83268) Fitting model: LightGBMLarge_BAG_L1_FULL ...\n(_dystack pid=83268)    0.33s    = Training   runtime\n(_dystack pid=83268) Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n(_dystack pid=83268)    Ensemble Weights: {'CatBoost_BAG_L1': 0.6, 'ExtraTreesMSE_BAG_L1': 0.36, 'LightGBM_BAG_L1': 0.04}\n(_dystack pid=83268)    0.02s    = Training   runtime\n(_dystack pid=83268) Fitting 1 L2 models ...\n(_dystack pid=83268) Fitting model: LightGBMXT_BAG_L2_FULL ...\n(_dystack pid=83268)    0.21s    = Training   runtime\n(_dystack pid=83268) Fitting 1 L2 models ...\n(_dystack pid=83268) Fitting model: LightGBM_BAG_L2_FULL ...\n(_dystack pid=83268)    0.31s    = Training   runtime\n(_dystack pid=83268) Fitting model: RandomForestMSE_BAG_L2_FULL | Skipping fit via cloning parent ...\n(_dystack pid=83268)    0.97s    = Training   runtime\n(_dystack pid=83268)    0.17s    = Validation runtime\n(_dystack pid=83268) Fitting 1 L2 models ...\n(_dystack pid=83268) Fitting model: CatBoost_BAG_L2_FULL ...\n(_dystack pid=83268)    0.17s    = Training   runtime\n(_dystack pid=83268) Fitting model: ExtraTreesMSE_BAG_L2_FULL | Skipping fit via cloning parent ...\n(_dystack pid=83268)    0.82s    = Training   runtime\n(_dystack pid=83268)    0.17s    = Validation runtime\n(_dystack pid=83268) Fitting 1 L2 models ...\n(_dystack pid=83268) Fitting model: XGBoost_BAG_L2_FULL ...\n(_dystack pid=83268)    0.22s    = Training   runtime\n(_dystack pid=83268) Fitting 1 L2 models ...\n(_dystack pid=83268) Fitting model: LightGBMLarge_BAG_L2_FULL ...\n(_dystack pid=83268)    0.4s     = Training   runtime\n(_dystack pid=83268) Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n(_dystack pid=83268)    Ensemble Weights: {'CatBoost_BAG_L1': 0.36, 'ExtraTreesMSE_BAG_L1': 0.32, 'LightGBM_BAG_L2': 0.32}\n(_dystack pid=83268)    0.03s    = Training   runtime\n(_dystack pid=83268) Updated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\n(_dystack pid=83268) Refit complete, total runtime = 4.84s ... Best model: \"WeightedEnsemble_L3_FULL\"\n(_dystack pid=83268) TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241126_101003/ds_sub_fit/sub_fit_ho\")\n(_dystack pid=83268) Deleting DyStack predictor artifacts (clean_up_fits=True) ...\nLeaderboard on holdout data (DyStack):\n                          model  score_holdout    score_val              eval_metric  pred_time_test  pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0          CatBoost_BAG_L1_FULL    -803.899801 -3857.311112  root_mean_squared_error        0.004578            NaN  0.994905                 0.004578                     NaN           0.994905            1       True          4\n1      WeightedEnsemble_L2_FULL    -813.259482 -3835.422447  root_mean_squared_error        0.130354            NaN  2.022119                 0.002616                     NaN           0.018397            2       True          8\n2          CatBoost_BAG_L2_FULL    -838.233626 -3904.774934  root_mean_squared_error        0.272134            NaN  4.816257                 0.005706                     NaN           0.166114            2       True         12\n3   RandomForestMSE_BAG_L1_FULL    -847.825565 -4516.179095  root_mean_squared_error        0.121127       0.175681  0.964108                 0.121127                0.175681           0.964108            1       True          3\n4     ExtraTreesMSE_BAG_L2_FULL    -890.912998 -3952.202176  root_mean_squared_error        0.389130            NaN  5.466693                 0.122702                0.169496           0.816550            2       True         13\n5     ExtraTreesMSE_BAG_L1_FULL    -922.896541 -3900.303809  root_mean_squared_error        0.120866       0.170631  0.819227                 0.120866                0.170631           0.819227            1       True          5\n6      WeightedEnsemble_L3_FULL    -977.887954 -3823.163850  root_mean_squared_error        0.274239            NaN  4.992532                 0.003509                     NaN           0.028692            3       True         16\n7          LightGBM_BAG_L1_FULL   -1086.123687 -3921.704247  root_mean_squared_error        0.002294            NaN  0.189590                 0.002294                     NaN           0.189590            1       True          2\n8   RandomForestMSE_BAG_L2_FULL   -1090.066132 -4525.205744  root_mean_squared_error        0.383968            NaN  5.621844                 0.117540                0.169827           0.971701            2       True         11\n9        LightGBMXT_BAG_L1_FULL   -1230.340360 -3990.480139  root_mean_squared_error        0.003167            NaN  1.170255                 0.003167                     NaN           1.170255            1       True          1\n10       LightGBMXT_BAG_L2_FULL   -1234.815155 -3941.789134  root_mean_squared_error        0.269577            NaN  4.864584                 0.003149                     NaN           0.214441            2       True          9\n11    LightGBMLarge_BAG_L1_FULL   -1345.024278 -3912.540001  root_mean_squared_error        0.004652            NaN  0.334220                 0.004652                     NaN           0.334220            1       True          7\n12    LightGBMLarge_BAG_L2_FULL   -1640.347524 -3912.640942  root_mean_squared_error        0.273687            NaN  5.046485                 0.007259                     NaN           0.396342            2       True         15\n13         LightGBM_BAG_L2_FULL   -1743.255667 -3894.707823  root_mean_squared_error        0.270730            NaN  4.963841                 0.004302                     NaN           0.313698            2       True         10\n14          XGBoost_BAG_L1_FULL   -2245.433966 -3941.359884  root_mean_squared_error        0.009745            NaN  0.177837                 0.009745                     NaN           0.177837            1       True          6\n15          XGBoost_BAG_L2_FULL   -2454.083373 -3929.201875  root_mean_squared_error        0.278816            NaN  4.869000                 0.012388                     NaN           0.218857            2       True         14\n    0    = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n    67s  = DyStack   runtime |  3533s    = Remaining runtime\nStarting main fit with num_stack_levels=0.\n    For future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\nBeginning AutoGluon training ... Time limit = 3533s\nAutoGluon will save models to \"AutogluonModels/ag-20241126_101003\"\nTrain Data Rows:    2750\nTrain Data Columns: 17\nLabel Column:       TargetSales\nProblem Type:       regression\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    481140.60 MB\n    Train Data (Original)  Memory Usage: 0.36 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    0.0s = Fit runtime\n    17 features in original data used to generate 17 features in processed data.\n    Train Data (Processed) Memory Usage: 0.36 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.07s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n}\nExcluded models: ['FASTAI', 'NN_TORCH'] (Specified by `excluded_model_types`)\nFitting 7 L1 models ...\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 3532.88s of the 3532.87s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -3713.1197   = Validation score   (-root_mean_squared_error)\n    4.46s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ... Training model for up to 3526.09s of the 3526.09s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -3635.1505   = Validation score   (-root_mean_squared_error)\n    3.79s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: RandomForestMSE_BAG_L1 ... Training model for up to 3520.01s of the 3520.01s of remaining time.\n    -4135.0334   = Validation score   (-root_mean_squared_error)\n    0.86s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ... Training model for up to 3518.9s of the 3518.9s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -3669.0125   = Validation score   (-root_mean_squared_error)\n    12.43s   = Training   runtime\n    0.03s    = Validation runtime\nFitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 3504.11s of the 3504.11s of remaining time.\n    -3678.3921   = Validation score   (-root_mean_squared_error)\n    0.9s     = Training   runtime\n    0.18s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ... Training model for up to 3502.97s of the 3502.97s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -3785.5048   = Validation score   (-root_mean_squared_error)\n    2.1s     = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ... Training model for up to 3498.6s of the 3498.6s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -3704.5742   = Validation score   (-root_mean_squared_error)\n    3.22s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 3493.14s of remaining time.\n    Ensemble Weights: {'LightGBM_BAG_L1': 0.5, 'ExtraTreesMSE_BAG_L1': 0.35, 'CatBoost_BAG_L1': 0.15}\n    -3608.5561   = Validation score   (-root_mean_squared_error)\n    0.02s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 39.85s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 4595.8 rows/s (344 batch size)\nAutomatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\nRefitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n    Models trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n    This process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n    To learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\nFitting 1 L1 models ...\nFitting model: LightGBMXT_BAG_L1_FULL ...\n    1.45s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBM_BAG_L1_FULL ...\n    0.44s    = Training   runtime\nFitting model: RandomForestMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.86s    = Training   runtime\n    0.18s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: CatBoost_BAG_L1_FULL ...\n    0.89s    = Training   runtime\nFitting model: ExtraTreesMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.9s     = Training   runtime\n    0.18s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: XGBoost_BAG_L1_FULL ...\n    0.54s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBMLarge_BAG_L1_FULL ...\n    0.41s    = Training   runtime\nFitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n    Ensemble Weights: {'LightGBM_BAG_L1': 0.5, 'ExtraTreesMSE_BAG_L1': 0.35, 'CatBoost_BAG_L1': 0.15}\n    0.02s    = Training   runtime\nUpdated best model to \"WeightedEnsemble_L2_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L2_FULL\" for predict() and predict_proba().\nRefit complete, total runtime = 4.11s ... Best model: \"WeightedEnsemble_L2_FULL\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241126_101003\")\n\n\n\ntest_df['pred_baseline'] = predictor.predict(test_df[selected_features])\n\n\nmetric_baseline = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_baseline'])\nmetric_baseline['model'] = 'baseline'\nmetric_baseline\n\n{'root_mean_squared_error': 3162.478744240967,\n 'mean_squared_error': 10001271.807775924,\n 'mean_absolute_error': 715.6442657130541,\n 'r2': 0.3816166296854987,\n 'pearsonr': 0.6190719671013133,\n 'spearmanr': 0.47008461549340863,\n 'median_absolute_error': 232.98208312988282,\n 'earths_mover_distance': 287.77728784026124,\n 'model': 'baseline'}"
  },
  {
    "objectID": "notebook/sales_prediction.html#regression-on-winsorized-outcome",
    "href": "notebook/sales_prediction.html#regression-on-winsorized-outcome",
    "title": "Predict Zero-inflated and Long/fat-tailed Outcomes",
    "section": "Regression on Winsorized Outcome",
    "text": "Regression on Winsorized Outcome\nOne possible approach to deal with long/fat-tailed outcome is to train on a winsorized outcome. This may lead to better performance when tested on a winsorized outcome but not so much on original outcome.\n\noutlier_per = 0.99\noutlier_cap_train = train_df['TargetSales'].quantile(outlier_per)\noutlier_cap_train\n\n7180.805199999947\n\n\n\n#winsorize\ntrain_df['TargetSales_win'] = train_df['TargetSales'].map(lambda x: outlier_cap_train if x&gt; outlier_cap_train else x)\ntest_df['TargetSales_win'] = test_df['TargetSales'].map(lambda x: outlier_cap_train if x&gt; outlier_cap_train else x)\n\n\npredictor = TabularPredictor(label='TargetSales_win').fit(train_df[selected_features+['TargetSales_win']],\n                                                      presets=preset,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20241126_101154\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.1.1\nPython Version:     3.9.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP Wed Oct 23 01:22:11 UTC 2024\nCPU Count:          64\nMemory Avail:       469.75 GB / 480.23 GB (97.8%)\nDisk Space Avail:   1541.40 GB / 1968.52 GB (78.3%)\n===================================================\nPresets specified: ['good_quality']\nSetting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\nStack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\nNote: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n    You can avoid this risk by setting `save_bag_folds=True`.\nDyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n    This is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n    Running DyStack for up to 900s of the 3600s of remaining time (25%).\n        Context path: \"AutogluonModels/ag-20241126_101154/ds_sub_fit/sub_fit_ho\"\nLeaderboard on holdout data (DyStack):\n                          model  score_holdout   score_val              eval_metric  pred_time_test  pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0           XGBoost_BAG_L2_FULL    -673.227470 -706.566643  root_mean_squared_error        0.290126            NaN  5.287011                 0.017494                     NaN           0.256777            2       True         14\n1          CatBoost_BAG_L2_FULL    -685.276375 -679.668006  root_mean_squared_error        0.278921            NaN  5.648248                 0.006289                     NaN           0.618015            2       True         12\n2     ExtraTreesMSE_BAG_L2_FULL    -686.432329 -688.280166  root_mean_squared_error        0.390370            NaN  5.851508                 0.117738                0.175184           0.821275            2       True         13\n3      WeightedEnsemble_L2_FULL    -687.292057 -677.748155  root_mean_squared_error        0.147735            NaN  3.111673                 0.004223                     NaN           0.018202            2       True          8\n4          CatBoost_BAG_L1_FULL    -688.830702 -682.216238  root_mean_squared_error        0.010066            NaN  0.742727                 0.010066                     NaN           0.742727            1       True          4\n5   RandomForestMSE_BAG_L2_FULL    -690.155342 -702.819447  root_mean_squared_error        0.382087            NaN  6.004801                 0.109455                0.173669           0.974567            2       True         11\n6     LightGBMLarge_BAG_L2_FULL    -699.457560 -701.790157  root_mean_squared_error        0.282755            NaN  5.677221                 0.010123                     NaN           0.646987            2       True         15\n7      WeightedEnsemble_L3_FULL    -699.646914 -664.915201  root_mean_squared_error        0.300374            NaN  6.563785                 0.003960                     NaN           0.028939            3       True         16\n8   RandomForestMSE_BAG_L1_FULL    -700.107179 -708.557877  root_mean_squared_error        0.118200       0.179320  0.993047                 0.118200                0.179320           0.993047            1       True          3\n9     ExtraTreesMSE_BAG_L1_FULL    -701.853556 -688.997247  root_mean_squared_error        0.115165       0.170742  0.812660                 0.115165                0.170742           0.812660            1       True          5\n10          XGBoost_BAG_L1_FULL    -717.776000 -710.501170  root_mean_squared_error        0.014938            NaN  0.129418                 0.014938                     NaN           0.129418            1       True          6\n11       LightGBMXT_BAG_L2_FULL    -723.560168 -701.334719  root_mean_squared_error        0.277760            NaN  5.320158                 0.005128                     NaN           0.289924            2       True          9\n12         LightGBM_BAG_L1_FULL    -726.112842 -700.802863  root_mean_squared_error        0.002237            NaN  0.163236                 0.002237                     NaN           0.163236            1       True          2\n13         LightGBM_BAG_L2_FULL    -728.829307 -669.578190  root_mean_squared_error        0.296413            NaN  6.534846                 0.023781                     NaN           1.504612            2       True         10\n14       LightGBMXT_BAG_L1_FULL    -733.594747 -704.073534  root_mean_squared_error        0.003345            NaN  1.408667                 0.003345                     NaN           1.408667            1       True          1\n15    LightGBMLarge_BAG_L1_FULL    -766.964045 -715.782974  root_mean_squared_error        0.008682            NaN  0.780480                 0.008682                     NaN           0.780480            1       True          7\n    0    = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n    82s  = DyStack   runtime |  3518s    = Remaining runtime\nStarting main fit with num_stack_levels=0.\n    For future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\nBeginning AutoGluon training ... Time limit = 3518s\nAutoGluon will save models to \"AutogluonModels/ag-20241126_101154\"\nTrain Data Rows:    2750\nTrain Data Columns: 17\nLabel Column:       TargetSales_win\nProblem Type:       regression\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    480645.68 MB\n    Train Data (Original)  Memory Usage: 0.36 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    0.0s = Fit runtime\n    17 features in original data used to generate 17 features in processed data.\n    Train Data (Processed) Memory Usage: 0.36 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.06s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n}\nExcluded models: ['FASTAI', 'NN_TORCH'] (Specified by `excluded_model_types`)\nFitting 7 L1 models ...\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 3517.72s of the 3517.71s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -710.5609    = Validation score   (-root_mean_squared_error)\n    1.99s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ... Training model for up to 3513.48s of the 3513.48s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -696.6213    = Validation score   (-root_mean_squared_error)\n    2.06s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: RandomForestMSE_BAG_L1 ... Training model for up to 3509.13s of the 3509.13s of remaining time.\n    -706.2702    = Validation score   (-root_mean_squared_error)\n    0.84s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ... Training model for up to 3508.04s of the 3508.04s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -668.1395    = Validation score   (-root_mean_squared_error)\n    3.84s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 3501.92s of the 3501.92s of remaining time.\n    -688.8913    = Validation score   (-root_mean_squared_error)\n    0.63s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ... Training model for up to 3501.01s of the 3501.0s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -699.3326    = Validation score   (-root_mean_squared_error)\n    2.06s    = Training   runtime\n    0.04s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ... Training model for up to 3496.71s of the 3496.71s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -714.8496    = Validation score   (-root_mean_squared_error)\n    3.48s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 3490.98s of remaining time.\n    Ensemble Weights: {'CatBoost_BAG_L1': 0.833, 'XGBoost_BAG_L1': 0.125, 'ExtraTreesMSE_BAG_L1': 0.042}\n    -667.3394    = Validation score   (-root_mean_squared_error)\n    0.02s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 26.88s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 3852.1 rows/s (344 batch size)\nAutomatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\nRefitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n    Models trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n    This process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n    To learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\nFitting 1 L1 models ...\nFitting model: LightGBMXT_BAG_L1_FULL ...\n    0.46s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBM_BAG_L1_FULL ...\n    0.44s    = Training   runtime\nFitting model: RandomForestMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.84s    = Training   runtime\n    0.18s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: CatBoost_BAG_L1_FULL ...\n    0.4s     = Training   runtime\nFitting model: ExtraTreesMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.63s    = Training   runtime\n    0.18s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: XGBoost_BAG_L1_FULL ...\n    0.12s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBMLarge_BAG_L1_FULL ...\n    0.77s    = Training   runtime\nFitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n    Ensemble Weights: {'CatBoost_BAG_L1': 0.833, 'XGBoost_BAG_L1': 0.125, 'ExtraTreesMSE_BAG_L1': 0.042}\n    0.02s    = Training   runtime\nUpdated best model to \"WeightedEnsemble_L2_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L2_FULL\" for predict() and predict_proba().\nRefit complete, total runtime = 2.53s ... Best model: \"WeightedEnsemble_L2_FULL\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241126_101154\")\n\n\n\ntest_df['pred_winsorized'] = predictor.predict(test_df[selected_features])\n\n\nmetric_winsorized = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_winsorized'])\nmetric_winsorized['model'] = 'winsorized'\nmetric_winsorized\n\n{'root_mean_squared_error': 3623.576377551195,\n 'mean_squared_error': 13130305.76394704,\n 'mean_absolute_error': 627.7880071099414,\n 'r2': 0.18814697894155963,\n 'pearsonr': 0.5757989413256978,\n 'spearmanr': 0.504301956183441,\n 'median_absolute_error': 219.62248107910156,\n 'earths_mover_distance': 432.1288432991232,\n 'model': 'winsorized'}\n\n\n\ncalculate_regression_metrics(test_df['TargetSales_win'], test_df['pred_winsorized'])\n\n{'root_mean_squared_error': 673.4846433338375,\n 'mean_squared_error': 453581.5648065064,\n 'mean_absolute_error': 376.77603327273135,\n 'r2': 0.6171771763549553,\n 'pearsonr': 0.7865724180212539,\n 'spearmanr': 0.504299950810919,\n 'median_absolute_error': 218.8311004638672,\n 'earths_mover_distance': 181.1168694619127}"
  },
  {
    "objectID": "notebook/sales_prediction.html#log1p-regression",
    "href": "notebook/sales_prediction.html#log1p-regression",
    "title": "Predict Zero-inflated and Long/fat-tailed Outcomes",
    "section": "Log1p Regression",
    "text": "Log1p Regression\nLog transformation handles long/fat-tailed distribution and is especially useful for certain models since the transformed distribution is roughly normal. However, it cannot handle zero-valued outcome and oftentimes scientists end up adding 1 to the outcome (so often that numpy even has a function for it). This not only introduces bias to the prediction, but also does not solve the zero-inflation as it becomes one-inflation instead.\n\n#log\ntrain_df['TargetSales_log1p'] = train_df['TargetSales'].map(np.log1p)\ntest_df['TargetSales_log1p'] = test_df['TargetSales'].map(np.log1p)\n\n\n#from zero-inflated to one-inflated\ntrain_df['TargetSales_log1p'].hist()\n\n\n\n\n\n\n\n\n\npredictor = TabularPredictor(label='TargetSales_log1p').fit(train_df[selected_features+['TargetSales_log1p']],\n                                                      presets=preset,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20241126_101346\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.1.1\nPython Version:     3.9.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP Wed Oct 23 01:22:11 UTC 2024\nCPU Count:          64\nMemory Avail:       469.47 GB / 480.23 GB (97.8%)\nDisk Space Avail:   1541.30 GB / 1968.52 GB (78.3%)\n===================================================\nPresets specified: ['good_quality']\nSetting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\nStack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\nNote: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n    You can avoid this risk by setting `save_bag_folds=True`.\nDyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n    This is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n    Running DyStack for up to 900s of the 3600s of remaining time (25%).\n        Context path: \"AutogluonModels/ag-20241126_101346/ds_sub_fit/sub_fit_ho\"\nLeaderboard on holdout data (DyStack):\n                          model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0          CatBoost_BAG_L1_FULL      -2.885523  -2.780750  root_mean_squared_error        0.020686            NaN   0.650833                 0.020686                     NaN           0.650833            1       True          4\n1      WeightedEnsemble_L2_FULL      -2.894191  -2.775644  root_mean_squared_error        0.160296            NaN   2.819383                 0.003478                     NaN           0.018452            2       True          8\n2   RandomForestMSE_BAG_L2_FULL      -2.894937  -2.808770  root_mean_squared_error        0.435059            NaN   5.443468                 0.124940                0.183134           1.003239            2       True         11\n3     ExtraTreesMSE_BAG_L2_FULL      -2.896741  -2.777983  root_mean_squared_error        0.435080            NaN   5.274917                 0.124961                0.172770           0.834688            2       True         13\n4        LightGBMXT_BAG_L1_FULL      -2.908863  -2.782297  root_mean_squared_error        0.003337            NaN   1.298610                 0.003337                     NaN           1.298610            1       True          1\n5          CatBoost_BAG_L2_FULL      -2.922107  -2.759026  root_mean_squared_error        0.320235            NaN   7.354950                 0.010116                     NaN           2.914721            2       True         12\n6          LightGBM_BAG_L2_FULL      -2.931031  -2.759814  root_mean_squared_error        0.325009            NaN   5.217557                 0.014890                     NaN           0.777328            2       True         10\n7           XGBoost_BAG_L2_FULL      -2.938193  -2.790059  root_mean_squared_error        0.328456            NaN   4.597451                 0.018338                     NaN           0.157222            2       True         14\n8      WeightedEnsemble_L3_FULL      -2.942265  -2.685363  root_mean_squared_error        0.497056            NaN  14.318665                 0.005495                     NaN           0.029343            3       True         16\n9     ExtraTreesMSE_BAG_L1_FULL      -2.946022  -2.815757  root_mean_squared_error        0.132796       0.178658   0.851489                 0.132796                0.178658           0.851489            1       True          5\n10         LightGBM_BAG_L1_FULL      -2.953480  -2.813496  root_mean_squared_error        0.002430            NaN   0.165181                 0.002430                     NaN           0.165181            1       True          2\n11          XGBoost_BAG_L1_FULL      -2.972277  -2.836214  root_mean_squared_error        0.016762            NaN   0.133264                 0.016762                     NaN           0.133264            1       True          6\n12    LightGBMLarge_BAG_L2_FULL      -2.977587  -2.794323  root_mean_squared_error        0.356780            NaN   8.470437                 0.046662                     NaN           4.030208            2       True         15\n13  RandomForestMSE_BAG_L1_FULL      -2.985264  -2.831375  root_mean_squared_error        0.127529       0.180724   0.979892                 0.127529                0.180724           0.979892            1       True          3\n14       LightGBMXT_BAG_L2_FULL      -2.995407  -2.694352  root_mean_squared_error        0.411671            NaN   9.324565                 0.101552                     NaN           4.884336            2       True          9\n15    LightGBMLarge_BAG_L1_FULL      -3.050660  -2.862792  root_mean_squared_error        0.006579            NaN   0.360961                 0.006579                     NaN           0.360961            1       True          7\n    0    = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n    173s     = DyStack   runtime |  3427s    = Remaining runtime\nStarting main fit with num_stack_levels=0.\n    For future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\nBeginning AutoGluon training ... Time limit = 3427s\nAutoGluon will save models to \"AutogluonModels/ag-20241126_101346\"\nTrain Data Rows:    2750\nTrain Data Columns: 17\nLabel Column:       TargetSales_log1p\nProblem Type:       regression\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    480330.83 MB\n    Train Data (Original)  Memory Usage: 0.36 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    0.0s = Fit runtime\n    17 features in original data used to generate 17 features in processed data.\n    Train Data (Processed) Memory Usage: 0.36 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.06s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n}\nExcluded models: ['FASTAI', 'NN_TORCH'] (Specified by `excluded_model_types`)\nFitting 7 L1 models ...\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 3427.2s of the 3427.2s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -2.7861  = Validation score   (-root_mean_squared_error)\n    1.97s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ... Training model for up to 3423.0s of the 3422.99s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -2.8189  = Validation score   (-root_mean_squared_error)\n    1.81s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: RandomForestMSE_BAG_L1 ... Training model for up to 3418.91s of the 3418.91s of remaining time.\n    -2.8468  = Validation score   (-root_mean_squared_error)\n    0.82s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ... Training model for up to 3417.81s of the 3417.81s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -2.7963  = Validation score   (-root_mean_squared_error)\n    1.79s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 3413.76s of the 3413.76s of remaining time.\n    -2.8191  = Validation score   (-root_mean_squared_error)\n    0.65s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ... Training model for up to 3412.83s of the 3412.83s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -2.8365  = Validation score   (-root_mean_squared_error)\n    1.77s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ... Training model for up to 3408.81s of the 3408.81s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -2.8667  = Validation score   (-root_mean_squared_error)\n    3.51s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 3403.0s of remaining time.\n    Ensemble Weights: {'LightGBMXT_BAG_L1': 0.714, 'CatBoost_BAG_L1': 0.143, 'ExtraTreesMSE_BAG_L1': 0.143}\n    -2.7845  = Validation score   (-root_mean_squared_error)\n    0.02s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 24.3s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 5481.1 rows/s (344 batch size)\nAutomatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\nRefitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n    Models trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n    This process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n    To learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\nFitting 1 L1 models ...\nFitting model: LightGBMXT_BAG_L1_FULL ...\n    0.35s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBM_BAG_L1_FULL ...\n    0.34s    = Training   runtime\nFitting model: RandomForestMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.82s    = Training   runtime\n    0.18s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: CatBoost_BAG_L1_FULL ...\n    0.18s    = Training   runtime\nFitting model: ExtraTreesMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.65s    = Training   runtime\n    0.19s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: XGBoost_BAG_L1_FULL ...\n    0.1s     = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBMLarge_BAG_L1_FULL ...\n    0.56s    = Training   runtime\nFitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n    Ensemble Weights: {'LightGBMXT_BAG_L1': 0.714, 'CatBoost_BAG_L1': 0.143, 'ExtraTreesMSE_BAG_L1': 0.143}\n    0.02s    = Training   runtime\nUpdated best model to \"WeightedEnsemble_L2_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L2_FULL\" for predict() and predict_proba().\nRefit complete, total runtime = 1.94s ... Best model: \"WeightedEnsemble_L2_FULL\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241126_101346\")\n\n\n\ntest_df['pred_log1p'] = predictor.predict(test_df[selected_features])\ntest_df['pred_log1p_expm1'] = test_df['pred_log1p'].map(np.expm1)\n\n\nmetric_log1p = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_log1p_expm1'])\nmetric_log1p['model'] = 'log1p'\nmetric_log1p\n\n{'root_mean_squared_error': 3725.342295894091,\n 'mean_squared_error': 13878175.221577456,\n 'mean_absolute_error': 618.9768466651894,\n 'r2': 0.14190585634701047,\n 'pearsonr': 0.5817166874396966,\n 'spearmanr': 0.5338156315937898,\n 'median_absolute_error': 89.55495441784018,\n 'earths_mover_distance': 581.0494444960044,\n 'model': 'log1p'}\n\n\n\ncalculate_regression_metrics(test_df['TargetSales_log1p'], test_df['pred_log1p'])\n\n{'root_mean_squared_error': 2.720047847858299,\n 'mean_squared_error': 7.398660294638562,\n 'mean_absolute_error': 2.418601533469381,\n 'r2': 0.30252750020590236,\n 'pearsonr': 0.5507740732825224,\n 'spearmanr': 0.5338156315937898,\n 'median_absolute_error': 2.349368453025818,\n 'earths_mover_distance': 1.8552344547363062}"
  },
  {
    "objectID": "notebook/sales_prediction.html#hurdle-model",
    "href": "notebook/sales_prediction.html#hurdle-model",
    "title": "Predict Zero-inflated and Long/fat-tailed Outcomes",
    "section": "Hurdle Model",
    "text": "Hurdle Model\nHurdle model is a two-stage approach that handles zero inflation by first having a classification model to predict if the outcome is zero or not, then a regression model, trained only on examples with actual non-zero outcomes, to fit a log-transformed outcome. When retransforming the predictions from log to non-log numbers, we perform correction of underestimation using Duan‚Äôs method. During inference time, we multiply the predictions from the classification and regression model.\n\nBinary Classification\n\ntrain_df['has_purchase'] = train_df.TargetSales.map(lambda x: 1 if x&gt;0 else 0)\ntest_df['has_purchase'] = test_df.TargetSales.map(lambda x: 1 if x&gt;0 else 0)\n\n\ntrain_df['has_purchase'].mean(), test_df['has_purchase'].mean()\n\n(0.5141818181818182, 0.5305232558139535)\n\n\n\npredictor_cls = TabularPredictor(label='has_purchase').fit(train_df[selected_features+['has_purchase']],\n                                                      presets=preset,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20241126_101706\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.1.1\nPython Version:     3.9.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP Wed Oct 23 01:22:11 UTC 2024\nCPU Count:          64\nMemory Avail:       469.22 GB / 480.23 GB (97.7%)\nDisk Space Avail:   1541.17 GB / 1968.52 GB (78.3%)\n===================================================\nPresets specified: ['good_quality']\nSetting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\nStack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\nNote: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n    You can avoid this risk by setting `save_bag_folds=True`.\nDyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n    This is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n    Running DyStack for up to 900s of the 3600s of remaining time (25%).\n        Context path: \"AutogluonModels/ag-20241126_101706/ds_sub_fit/sub_fit_ho\"\nLeaderboard on holdout data (DyStack):\n                           model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0           LightGBM_BAG_L1_FULL       0.699346   0.686989    accuracy        0.002175            NaN  0.108046                 0.002175                     NaN           0.108046            1       True          2\n1           CatBoost_BAG_L1_FULL       0.699346   0.698036    accuracy        0.003567            NaN  0.608703                 0.003567                     NaN           0.608703            1       True          5\n2       WeightedEnsemble_L2_FULL       0.699346   0.698036    accuracy        0.005767            NaN  0.782679                 0.002200                     NaN           0.173977            2       True         10\n3     ExtraTreesEntr_BAG_L1_FULL       0.692810   0.675123    accuracy        0.122157       0.179918  1.000016                 0.122157                0.179918           1.000016            1       True          7\n4         LightGBMXT_BAG_L2_FULL       0.692810   0.701718    accuracy        0.547326            NaN  7.217974                 0.004018                     NaN           0.180956            2       True         11\n5           CatBoost_BAG_L2_FULL       0.692810   0.707447    accuracy        0.549285            NaN  7.561007                 0.005977                     NaN           0.523989            2       True         15\n6       WeightedEnsemble_L3_FULL       0.692810   0.707856    accuracy        0.551327            NaN  7.873676                 0.002042                     NaN           0.312670            3       True         20\n7     ExtraTreesGini_BAG_L1_FULL       0.689542   0.673077    accuracy        0.129358       0.181783  1.053820                 0.129358                0.181783           1.053820            1       True          6\n8   RandomForestEntr_BAG_L1_FULL       0.683007   0.654255    accuracy        0.117779       0.174219  1.047733                 0.117779                0.174219           1.047733            1       True          4\n9     ExtraTreesGini_BAG_L2_FULL       0.679739   0.684534    accuracy        0.663375            NaN  8.053752                 0.120067                0.179437           1.016734            2       True         16\n10  RandomForestGini_BAG_L2_FULL       0.679739   0.684943    accuracy        0.678078            NaN  8.181539                 0.134771                0.179112           1.144521            2       True         13\n11          LightGBM_BAG_L2_FULL       0.676471   0.706219    accuracy        0.547352            NaN  7.223828                 0.004044                     NaN           0.186810            2       True         12\n12    ExtraTreesEntr_BAG_L2_FULL       0.676471   0.679624    accuracy        0.672358            NaN  8.049679                 0.129050                0.180169           1.012661            2       True         17\n13           XGBoost_BAG_L1_FULL       0.673203   0.687398    accuracy        0.015491            NaN  0.139807                 0.015491                     NaN           0.139807            1       True          8\n14           XGBoost_BAG_L2_FULL       0.673203   0.702946    accuracy        0.555419            NaN  7.180053                 0.012111                     NaN           0.143035            2       True         18\n15  RandomForestEntr_BAG_L2_FULL       0.673203   0.684534    accuracy        0.659769            NaN  8.097150                 0.116461                0.173954           1.060132            2       True         14\n16        LightGBMXT_BAG_L1_FULL       0.666667   0.694354    accuracy        0.003664            NaN  1.090716                 0.003664                     NaN           1.090716            1       True          1\n17     LightGBMLarge_BAG_L1_FULL       0.663399   0.673895    accuracy        0.009468            NaN  0.803435                 0.009468                     NaN           0.803435            1       True          9\n18  RandomForestGini_BAG_L1_FULL       0.660131   0.666121    accuracy        0.139650       0.181133  1.184743                 0.139650                0.181133           1.184743            1       True          3\n19     LightGBMLarge_BAG_L2_FULL       0.656863   0.704992    accuracy        0.551465            NaN  7.941736                 0.008157                     NaN           0.904718            2       True         19\n    0    = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n    69s  = DyStack   runtime |  3531s    = Remaining runtime\nStarting main fit with num_stack_levels=0.\n    For future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\nBeginning AutoGluon training ... Time limit = 3531s\nAutoGluon will save models to \"AutogluonModels/ag-20241126_101706\"\nTrain Data Rows:    2750\nTrain Data Columns: 17\nLabel Column:       has_purchase\nProblem Type:       binary\nPreprocessing data ...\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    480097.88 MB\n    Train Data (Original)  Memory Usage: 0.36 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    0.0s = Fit runtime\n    17 features in original data used to generate 17 features in processed data.\n    Train Data (Processed) Memory Usage: 0.36 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.06s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n}\nExcluded models: ['FASTAI', 'NN_TORCH'] (Specified by `excluded_model_types`)\nFitting 9 L1 models ...\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 3530.48s of the 3530.47s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    0.6964   = Validation score   (accuracy)\n    1.94s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ... Training model for up to 3526.32s of the 3526.31s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    0.6884   = Validation score   (accuracy)\n    2.1s     = Training   runtime\n    0.02s    = Validation runtime\nFitting model: RandomForestGini_BAG_L1 ... Training model for up to 3521.91s of the 3521.91s of remaining time.\n    0.6615   = Validation score   (accuracy)\n    0.91s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: RandomForestEntr_BAG_L1 ... Training model for up to 3520.72s of the 3520.72s of remaining time.\n    0.6644   = Validation score   (accuracy)\n    0.95s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ... Training model for up to 3519.49s of the 3519.49s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    0.6935   = Validation score   (accuracy)\n    1.93s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 3515.32s of the 3515.32s of remaining time.\n    0.6738   = Validation score   (accuracy)\n    0.96s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 3514.06s of the 3514.05s of remaining time.\n    0.6716   = Validation score   (accuracy)\n    0.85s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ... Training model for up to 3512.95s of the 3512.94s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    0.6789   = Validation score   (accuracy)\n    1.85s    = Training   runtime\n    0.04s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ... Training model for up to 3508.82s of the 3508.82s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    0.6738   = Validation score   (accuracy)\n    5.08s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 3501.48s of remaining time.\n    Ensemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n    0.6964   = Validation score   (accuracy)\n    0.18s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 29.29s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 26150.7 rows/s (344 batch size)\nAutomatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\nRefitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n    Models trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n    This process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n    To learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\nFitting 1 L1 models ...\nFitting model: LightGBMXT_BAG_L1_FULL ...\n    0.33s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBM_BAG_L1_FULL ...\n    0.35s    = Training   runtime\nFitting model: RandomForestGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.91s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: RandomForestEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.95s    = Training   runtime\n    0.19s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: CatBoost_BAG_L1_FULL ...\n    0.1s     = Training   runtime\nFitting model: ExtraTreesGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.96s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: ExtraTreesEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.85s    = Training   runtime\n    0.19s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: XGBoost_BAG_L1_FULL ...\n    0.12s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBMLarge_BAG_L1_FULL ...\n    1.07s    = Training   runtime\nFitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n    Ensemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n    0.18s    = Training   runtime\nUpdated best model to \"LightGBMXT_BAG_L1_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"LightGBMXT_BAG_L1_FULL\" for predict() and predict_proba().\nRefit complete, total runtime = 2.44s ... Best model: \"LightGBMXT_BAG_L1_FULL\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241126_101706\")\n\n\n\ntest_df['pred_binary'] = predictor_cls.predict(test_df[selected_features])\n\n\ncaluclate_classification_metrics(test_df['has_purchase'], test_df['pred_binary'])\n\n{'accuracy': 0.6918604651162791,\n 'precision': 0.6941069004479309,\n 'recall': 0.6918604651162791,\n 'f1_score': 0.6921418829824787,\n 'confusion_matrix': array([[229,  94],\n        [118, 247]])}\n\n\n\n\nRegression on Non-Zero Outcome\n\ntrain_df_nonzero = train_df[train_df.has_purchase==1].reset_index(drop=True)\ntest_df_nonzero = test_df[test_df.has_purchase==1].reset_index(drop=True)\n\ntrain_df_nonzero.shape, test_df_nonzero.shape\n\n((1414, 21), (365, 26))\n\n\n\n#log\ntrain_df_nonzero['TargetSales_log'] = train_df_nonzero['TargetSales'].map(np.log)\ntest_df_nonzero['TargetSales_log'] = test_df_nonzero['TargetSales'].map(np.log)\n\n\ntrain_df_nonzero['TargetSales_log'].hist()\n\n\n\n\n\n\n\n\n\npredictor_reg = TabularPredictor(label='TargetSales_log').fit(train_df_nonzero[selected_features+['TargetSales_log']],\n                                                      presets=preset,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20241126_101847\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.1.1\nPython Version:     3.9.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP Wed Oct 23 01:22:11 UTC 2024\nCPU Count:          64\nMemory Avail:       468.57 GB / 480.23 GB (97.6%)\nDisk Space Avail:   1541.01 GB / 1968.52 GB (78.3%)\n===================================================\nPresets specified: ['good_quality']\nSetting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\nStack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\nNote: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n    You can avoid this risk by setting `save_bag_folds=True`.\nDyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n    This is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n    Running DyStack for up to 900s of the 3600s of remaining time (25%).\n        Context path: \"AutogluonModels/ag-20241126_101847/ds_sub_fit/sub_fit_ho\"\nLeaderboard on holdout data (DyStack):\n                          model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0     ExtraTreesMSE_BAG_L1_FULL      -0.839346  -0.781522  root_mean_squared_error        0.123006       0.141352  0.799363                 0.123006                0.141352           0.799363            1       True          5\n1     ExtraTreesMSE_BAG_L2_FULL      -0.853589  -0.787303  root_mean_squared_error        0.436013            NaN  8.134050                 0.129594                0.145573           0.830991            2       True         13\n2      WeightedEnsemble_L3_FULL      -0.857556  -0.768871  root_mean_squared_error        0.332279            NaN  8.055929                 0.005146                     NaN           0.027863            3       True         16\n3   RandomForestMSE_BAG_L1_FULL      -0.861175  -0.804629  root_mean_squared_error        0.119744       0.147896  0.923559                 0.119744                0.147896           0.923559            1       True          3\n4      WeightedEnsemble_L2_FULL      -0.862117  -0.769820  root_mean_squared_error        0.150238            NaN  2.038445                 0.004235                     NaN           0.016720            2       True          8\n5        LightGBMXT_BAG_L1_FULL      -0.864283  -0.782882  root_mean_squared_error        0.002448            NaN  1.338318                 0.002448                     NaN           1.338318            1       True          1\n6          CatBoost_BAG_L2_FULL      -0.866735  -0.782409  root_mean_squared_error        0.313627            NaN  7.518480                 0.007208                     NaN           0.215421            2       True         12\n7   RandomForestMSE_BAG_L2_FULL      -0.867588  -0.801697  root_mean_squared_error        0.425690            NaN  8.229896                 0.119271                0.143491           0.926836            2       True         11\n8        LightGBMXT_BAG_L2_FULL      -0.867632  -0.795484  root_mean_squared_error        0.310434            NaN  7.486937                 0.004015                     NaN           0.183877            2       True          9\n9           XGBoost_BAG_L2_FULL      -0.867990  -0.801243  root_mean_squared_error        0.319756            NaN  7.634992                 0.013337                     NaN           0.331932            2       True         14\n10         LightGBM_BAG_L2_FULL      -0.869884  -0.797155  root_mean_squared_error        0.313795            NaN  7.696134                 0.007376                     NaN           0.393075            2       True         10\n11         LightGBM_BAG_L1_FULL      -0.872511  -0.786431  root_mean_squared_error        0.001964            NaN  0.186059                 0.001964                     NaN           0.186059            1       True          2\n12         CatBoost_BAG_L1_FULL      -0.876613  -0.774745  root_mean_squared_error        0.006662            NaN  0.856986                 0.006662                     NaN           0.856986            1       True          4\n13          XGBoost_BAG_L1_FULL      -0.908105  -0.786510  root_mean_squared_error        0.014371            NaN  0.179318                 0.014371                     NaN           0.179318            1       True          6\n14    LightGBMLarge_BAG_L2_FULL      -0.929002  -0.811482  root_mean_squared_error        0.315722            NaN  7.913350                 0.009303                     NaN           0.610290            2       True         15\n15    LightGBMLarge_BAG_L1_FULL      -0.943116  -0.811385  root_mean_squared_error        0.038224            NaN  3.019457                 0.038224                     NaN           3.019457            1       True          7\n    1    = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n    89s  = DyStack   runtime |  3511s    = Remaining runtime\nStarting main fit with num_stack_levels=1.\n    For future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\nBeginning AutoGluon training ... Time limit = 3511s\nAutoGluon will save models to \"AutogluonModels/ag-20241126_101847\"\nTrain Data Rows:    1414\nTrain Data Columns: 17\nLabel Column:       TargetSales_log\nProblem Type:       regression\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    479435.26 MB\n    Train Data (Original)  Memory Usage: 0.18 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    0.0s = Fit runtime\n    17 features in original data used to generate 17 features in processed data.\n    Train Data (Processed) Memory Usage: 0.18 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.05s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n}\nAutoGluon will fit 2 stack levels (L1 to L2) ...\nExcluded models: ['FASTAI', 'NN_TORCH'] (Specified by `excluded_model_types`)\nFitting 7 L1 models ...\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 2339.93s of the 3510.77s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -0.7909  = Validation score   (-root_mean_squared_error)\n    1.87s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ... Training model for up to 2335.79s of the 3506.63s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -0.7919  = Validation score   (-root_mean_squared_error)\n    1.94s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: RandomForestMSE_BAG_L1 ... Training model for up to 2331.57s of the 3502.41s of remaining time.\n    -0.8074  = Validation score   (-root_mean_squared_error)\n    0.77s    = Training   runtime\n    0.15s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ... Training model for up to 2330.53s of the 3501.37s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -0.783   = Validation score   (-root_mean_squared_error)\n    1.93s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 2326.32s of the 3497.16s of remaining time.\n    -0.7902  = Validation score   (-root_mean_squared_error)\n    0.62s    = Training   runtime\n    0.15s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ... Training model for up to 2325.45s of the 3496.29s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -0.8026  = Validation score   (-root_mean_squared_error)\n    1.72s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2321.49s of the 3492.33s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -0.8166  = Validation score   (-root_mean_squared_error)\n    3.35s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 3486.65s of remaining time.\n    Ensemble Weights: {'CatBoost_BAG_L1': 0.5, 'ExtraTreesMSE_BAG_L1': 0.25, 'LightGBM_BAG_L1': 0.188, 'LightGBMXT_BAG_L1': 0.062}\n    -0.78    = Validation score   (-root_mean_squared_error)\n    0.02s    = Training   runtime\n    0.0s     = Validation runtime\nExcluded models: ['FASTAI', 'NN_TORCH'] (Specified by `excluded_model_types`)\nFitting 7 L2 models ...\nFitting model: LightGBMXT_BAG_L2 ... Training model for up to 3486.56s of the 3486.55s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -0.7869  = Validation score   (-root_mean_squared_error)\n    3.2s     = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBM_BAG_L2 ... Training model for up to 3481.11s of the 3481.1s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -0.7932  = Validation score   (-root_mean_squared_error)\n    2.25s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: RandomForestMSE_BAG_L2 ... Training model for up to 3476.56s of the 3476.55s of remaining time.\n    -0.8097  = Validation score   (-root_mean_squared_error)\n    0.71s    = Training   runtime\n    0.15s    = Validation runtime\nFitting model: CatBoost_BAG_L2 ... Training model for up to 3475.61s of the 3475.6s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -0.7847  = Validation score   (-root_mean_squared_error)\n    5.45s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 3467.92s of the 3467.91s of remaining time.\n    -0.795   = Validation score   (-root_mean_squared_error)\n    0.62s    = Training   runtime\n    0.15s    = Validation runtime\nFitting model: XGBoost_BAG_L2 ... Training model for up to 3467.07s of the 3467.06s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -0.8102  = Validation score   (-root_mean_squared_error)\n    2.17s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L2 ... Training model for up to 3462.61s of the 3462.6s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=8, gpus=0, memory=0.00%)\n    -0.8107  = Validation score   (-root_mean_squared_error)\n    11.18s   = Training   runtime\n    0.05s    = Validation runtime\nFitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 3449.17s of remaining time.\n    Ensemble Weights: {'CatBoost_BAG_L1': 0.333, 'LightGBMXT_BAG_L2': 0.25, 'LightGBM_BAG_L2': 0.167, 'LightGBM_BAG_L1': 0.083, 'ExtraTreesMSE_BAG_L1': 0.083, 'LightGBMLarge_BAG_L2': 0.083}\n    -0.7746  = Validation score   (-root_mean_squared_error)\n    0.03s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 61.72s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 754.0 rows/s (177 batch size)\nAutomatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\nRefitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n    Models trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n    This process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n    To learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\nFitting 1 L1 models ...\nFitting model: LightGBMXT_BAG_L1_FULL ...\n    0.35s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBM_BAG_L1_FULL ...\n    0.35s    = Training   runtime\nFitting model: RandomForestMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.77s    = Training   runtime\n    0.15s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: CatBoost_BAG_L1_FULL ...\n    0.29s    = Training   runtime\nFitting model: ExtraTreesMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.62s    = Training   runtime\n    0.15s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: XGBoost_BAG_L1_FULL ...\n    0.09s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBMLarge_BAG_L1_FULL ...\n    0.63s    = Training   runtime\nFitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n    Ensemble Weights: {'CatBoost_BAG_L1': 0.5, 'ExtraTreesMSE_BAG_L1': 0.25, 'LightGBM_BAG_L1': 0.188, 'LightGBMXT_BAG_L1': 0.062}\n    0.02s    = Training   runtime\nFitting 1 L2 models ...\nFitting model: LightGBMXT_BAG_L2_FULL ...\n    0.7s     = Training   runtime\nFitting 1 L2 models ...\nFitting model: LightGBM_BAG_L2_FULL ...\n    0.43s    = Training   runtime\nFitting model: RandomForestMSE_BAG_L2_FULL | Skipping fit via cloning parent ...\n    0.71s    = Training   runtime\n    0.15s    = Validation runtime\nFitting 1 L2 models ...\nFitting model: CatBoost_BAG_L2_FULL ...\n    0.39s    = Training   runtime\nFitting model: ExtraTreesMSE_BAG_L2_FULL | Skipping fit via cloning parent ...\n    0.62s    = Training   runtime\n    0.15s    = Validation runtime\nFitting 1 L2 models ...\nFitting model: XGBoost_BAG_L2_FULL ...\n    0.19s    = Training   runtime\nFitting 1 L2 models ...\nFitting model: LightGBMLarge_BAG_L2_FULL ...\n    2.56s    = Training   runtime\nFitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n    Ensemble Weights: {'CatBoost_BAG_L1': 0.333, 'LightGBMXT_BAG_L2': 0.25, 'LightGBM_BAG_L2': 0.167, 'LightGBM_BAG_L1': 0.083, 'ExtraTreesMSE_BAG_L1': 0.083, 'LightGBMLarge_BAG_L2': 0.083}\n    0.03s    = Training   runtime\nUpdated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\nRefit complete, total runtime = 6.78s ... Best model: \"WeightedEnsemble_L3_FULL\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241126_101847\")\n\n\n\ntest_df_nonzero['pred_log'] = predictor_reg.predict(test_df_nonzero[selected_features])\ntest_df_nonzero['pred_log_exp'] = test_df_nonzero['pred_log'].map(np.exp)\n\ntest_df['pred_log'] = predictor_reg.predict(test_df[selected_features])\ntest_df['pred_log_exp'] = test_df['pred_log'].map(np.exp)\n\n\ncalculate_regression_metrics(test_df_nonzero['TargetSales'], test_df_nonzero['pred_log_exp'])\n\n{'root_mean_squared_error': 4330.443144695726,\n 'mean_squared_error': 18752737.82944221,\n 'mean_absolute_error': 880.0418223064565,\n 'r2': 0.3647576298877435,\n 'pearsonr': 0.6756393928483335,\n 'spearmanr': 0.5762190201444638,\n 'median_absolute_error': 243.0658528752748,\n 'earths_mover_distance': 546.7166312173882}\n\n\n\ntest_df['pred_hurdle'] = test_df.pred_binary * test_df.pred_log_exp\n\n\nmetric_hurdle = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_hurdle'])\nmetric_hurdle['model'] = 'hurdle'\nmetric_hurdle\n\n{'root_mean_squared_error': 3171.760744960863,\n 'mean_squared_error': 10060066.22327469,\n 'mean_absolute_error': 584.9162934881963,\n 'r2': 0.3779813431428882,\n 'pearsonr': 0.6769697889999318,\n 'spearmanr': 0.5107083593715698,\n 'median_absolute_error': 199.1780137692856,\n 'earths_mover_distance': 286.381442541919,\n 'model': 'hurdle'}\n\n\n\n\nDuan‚Äôs Method\nWhen predicting a log-transformed outcome, we typically want to re-transform the predictions to non-log numbers by applying the exponential function. However, this ignores a small bias due to the error term in the process.\n\\[ln(y) = f(X) + \\epsilon\\]\nwhere * \\(y\\) is actual outcome. * \\(X\\) is the features. * \\(f(.)\\) is a trained model. * \\(\\epsilon\\) is the error term.\nwhen re-transforming \\[\n\\begin{align}\ny &= exp(ln(y)) \\\\\n&= exp(f(X) + \\epsilon ) \\\\\n&= exp(f(X)) \\cdot exp(\\epsilon) \\\\\nE[y] &= E[exp(f(X))] \\cdot E[exp(\\epsilon)]\n\\end{align}\n\\]\nDuan estimates the E[\\(exp(\\epsilon)\\)] as \\[\n\\begin{align}\n\\hat \\lambda &= E[exp(ln(y) - ln(\\hat y))]\n\\end{align}\n\\]\nwhere * \\(\\hat \\lambda\\) is the Duan‚Äôs smearing estimator of the bias from re-transformation \\(E[exp(\\epsilon)]\\) * \\(\\hat y\\) is the prediction aka \\(f(X)\\)\n\ntrain_df_nonzero['pred_log'] = predictor_reg.predict(train_df_nonzero[selected_features])\ntrain_df_nonzero['pred_log_exp'] = train_df_nonzero['pred_log'].map(np.exp)\n\nsmearing_estimator = np.mean(np.exp(train_df_nonzero['TargetSales_log'] - train_df_nonzero['pred_log']))\nsmearing_estimator\n\n1.2280991653046711\n\n\n\ntest_df['pred_log_exp_corrected'] = test_df['pred_log_exp'] * smearing_estimator\ntest_df['pred_hurdle_corrected'] = test_df.pred_binary * test_df.pred_log_exp_corrected\n\n\nmetric_hurdle_corrected = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_hurdle_corrected'])\nmetric_hurdle_corrected['model'] = 'hurdle_corrected'\nmetric_hurdle_corrected\n\n{'root_mean_squared_error': 3055.3207868281233,\n 'mean_squared_error': 9334985.110424023,\n 'mean_absolute_error': 613.3946643257099,\n 'r2': 0.42281345159207295,\n 'pearsonr': 0.6769697889999318,\n 'spearmanr': 0.5107083593715698,\n 'median_absolute_error': 232.55557358084502,\n 'earths_mover_distance': 241.61839859133218,\n 'model': 'hurdle_corrected'}"
  },
  {
    "objectID": "notebook/sales_prediction.html#evaluation",
    "href": "notebook/sales_prediction.html#evaluation",
    "title": "Predict Zero-inflated and Long/fat-tailed Outcomes",
    "section": "Evaluation",
    "text": "Evaluation\nWe can see that the hurdle_corrected method performs best across all metrics except for 1) mean absolute error where it performs about 5% worse than hurdle method without the correction and 2) median absolute error where it only performs better than baseline regression and 3) Spearman‚Äôs rank correlation where it underperforms log1p by 4%; correlations are tied between the two Hurdle methods by definition since we multiply Duan‚Äôs smearing estimator to hurdle predictions to get hurdle_corrected.\n\nmetric_df = pd.DataFrame([metric_baseline,\n                       metric_winsorized,\n                       metric_log1p,\n                       metric_hurdle,\n                       metric_hurdle_corrected,])\n\n\nrank_df = metric_df.copy()\nfor col in metric_df.columns.tolist()[:-1]:\n    if col in ['r2', 'pearsonr', 'spearmanr']:\n        rank_df[f'{col}_rank'] = rank_df[col].rank(ascending=False)\n    else:\n        rank_df[f'{col}_rank'] = rank_df[col].rank(ascending=True)\nrank_df = rank_df.drop(metric_df.columns.tolist()[:-1], axis=1)\nrank_df['avg_rank'] = rank_df.iloc[:,1:].mean(axis=1)\nrank_df.transpose()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nmodel\nbaseline\nwinsorized\nlog1p\nhurdle\nhurdle_corrected\n\n\nroot_mean_squared_error_rank\n2.0\n4.0\n5.0\n3.0\n1.0\n\n\nmean_squared_error_rank\n2.0\n4.0\n5.0\n3.0\n1.0\n\n\nmean_absolute_error_rank\n5.0\n4.0\n3.0\n1.0\n2.0\n\n\nr2_rank\n2.0\n4.0\n5.0\n3.0\n1.0\n\n\npearsonr_rank\n3.0\n5.0\n4.0\n1.5\n1.5\n\n\nspearmanr_rank\n5.0\n4.0\n1.0\n2.5\n2.5\n\n\nmedian_absolute_error_rank\n5.0\n3.0\n1.0\n2.0\n4.0\n\n\nearths_mover_distance_rank\n3.0\n4.0\n5.0\n2.0\n1.0\n\n\navg_rank\n3.375\n4.0\n3.625\n2.25\n1.75\n\n\n\n\n\n\n\n\nmetric_df.transpose()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nroot_mean_squared_error\n3162.478744\n3623.576378\n3725.342296\n3171.760745\n3055.320787\n\n\nmean_squared_error\n10001271.807776\n13130305.763947\n13878175.221577\n10060066.223275\n9334985.110424\n\n\nmean_absolute_error\n715.644266\n627.788007\n618.976847\n584.916293\n613.394664\n\n\nr2\n0.381617\n0.188147\n0.141906\n0.377981\n0.422813\n\n\npearsonr\n0.619072\n0.575799\n0.581717\n0.67697\n0.67697\n\n\nspearmanr\n0.470085\n0.504302\n0.533816\n0.510708\n0.510708\n\n\nmedian_absolute_error\n232.982083\n219.622481\n89.554954\n199.178014\n232.555574\n\n\nearths_mover_distance\n287.777288\n432.128843\n581.049444\n286.381443\n241.618399\n\n\nmodel\nbaseline\nwinsorized\nlog1p\nhurdle\nhurdle_corrected\n\n\n\n\n\n\n\n\nWhy hurdle Outperforms hurdle_corrected in MAE?\nDuan‚Äôs method adjusts for underestimation from retransformation of log outcome. This could lead to smaller extreme errors but more less extreme ones. We verify this hypothesis by comparing mean absolute error before and after transformation for errors originally under and over 99th percentile. We confirm that is the case for this problem.\n\nerr_hurdle = (test_df['TargetSales'] - test_df['pred_hurdle']).abs()\nerr_hurdle_corrected = (test_df['TargetSales'] - test_df['pred_hurdle_corrected']).abs()\n\n\nerr_hurdle.describe(percentiles=[.25, .5, .75, .9, .95, .99]) \n\ncount      688.000000\nmean       584.916293\nstd       3119.628924\nmin          0.000000\n25%          0.000000\n50%        199.178014\n75%        475.603446\n90%        862.530026\n95%       1237.540954\n99%       6763.777844\nmax      55731.205996\ndtype: float64\n\n\n\nerr_hurdle[err_hurdle&lt;6763.777844].mean(),\\\nerr_hurdle[err_hurdle&gt;6763.777844].mean(),\n\n(355.4918014848842, 22904.641872667555)\n\n\n\nerr_hurdle_corrected[err_hurdle&lt;6763.777844].mean(),\\\nerr_hurdle_corrected[err_hurdle&gt;6763.777844].mean(),\n\n(392.7718802742851, 22076.839798471465)\n\n\n\n\nWhy log1p Performs So Much Better than Others in MedAE?\nIt is for similar reasons that hurdle outperforms hurdle_corrected in MedAE; however, log1p performs twice better than other approaches (it also slightly outperforms hurdle models in Spearman‚Äôs rank correlation), especially the Hurdle models which should be modeling the non-zero outcomes in the same manner. This is because Hurdle models depend not only on the regression but the classification model. We can see that if the classification model were perfect (instead of the current f1 = 0.69), other metrics also improved but not nearly as drastic as MedAE and Spearman‚Äôs rank correlation.\n\ntest_df['pred_hurdle_corrected_perfect_cls'] = test_df.has_purchase * test_df.pred_log_exp_corrected\nmetric_hurdle_corrected_perfect_cls = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_hurdle_corrected_perfect_cls'])\nmetric_hurdle_corrected_perfect_cls['model'] = 'hurdle_corrected_perfect_cls'\n\nmetric_df2 = pd.DataFrame([metric_baseline,\n                       metric_winsorized,\n                       metric_log1p,\n                       metric_hurdle,\n                       metric_hurdle_corrected,\n                       metric_hurdle_corrected_perfect_cls,])\nmetric_df2.transpose()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\nroot_mean_squared_error\n3162.478744\n3623.576378\n3725.342296\n3171.760745\n3055.320787\n3030.854831\n\n\nmean_squared_error\n10001271.807776\n13130305.763947\n13878175.221577\n10060066.223275\n9334985.110424\n9186081.006625\n\n\nmean_absolute_error\n715.644266\n627.788007\n618.976847\n584.916293\n613.394664\n479.558294\n\n\nr2\n0.381617\n0.188147\n0.141906\n0.377981\n0.422813\n0.43202\n\n\npearsonr\n0.619072\n0.575799\n0.581717\n0.67697\n0.67697\n0.687639\n\n\nspearmanr\n0.470085\n0.504302\n0.533816\n0.510708\n0.510708\n0.929419\n\n\nmedian_absolute_error\n232.982083\n219.622481\n89.554954\n199.178014\n232.555574\n34.991964\n\n\nearths_mover_distance\n287.777288\n432.128843\n581.049444\n286.381443\n241.618399\n234.587018\n\n\nmodel\nbaseline\nwinsorized\nlog1p\nhurdle\nhurdle_corrected\nhurdle_corrected_perfect_cls\n\n\n\n\n\n\n\n\n\nWhy Baseline Regression Performs Best at Aggregate Level\nIf we look at aggregated mean or sum of actual sales vs predicted sales, baseline regression performs best by far. This is due to the fact that without any constraints a regressor only minimizes the MSE loss and usually ends up predicting values around the mean to balance between under- and over-predictions. However, this level of prediction is often not very useful as a single point and more often done by in a time series setup.\n\ntest_df[['TargetSales','pred_baseline','pred_winsorized','pred_log1p_expm1','pred_hurdle','pred_hurdle_corrected']].mean()\n\nTargetSales              760.558808\npred_baseline            791.043945\npred_winsorized          508.281555\npred_log1p_expm1         186.200281\npred_hurdle              527.286811\npred_hurdle_corrected    647.560493\ndtype: float64\n\n\n\ntest_df[['TargetSales','pred_baseline','pred_winsorized','pred_log1p_expm1','pred_hurdle','pred_hurdle_corrected']].sum()\n\nTargetSales              523264.460000\npred_baseline            544238.250000\npred_winsorized          349697.718750\npred_log1p_expm1         128105.793618\npred_hurdle              362773.326124\npred_hurdle_corrected    445521.619008\ndtype: float64"
  },
  {
    "objectID": "posts/sales_prediction/index.html",
    "href": "posts/sales_prediction/index.html",
    "title": "Predict How Much A Customer Will Spend",
    "section": "",
    "text": "I have spent nearly a decade as a data scientist in the retail sector, but I have been approaching customer spend predictions the wrong way until I attended Gregory M. Duncan‚Äôs lecture. Accurately predicting how much an individual customer will spend in the next X days enables key retail use cases such as personalized promotion (determine X in Buy-X-Get-Y), customer targeting for upselling (which customers have higher purchasing power), and early churn detection (customers do not spend as much as they should). What makes this problem particularly difficult is because the distribution of customer spending is both zero-inflated and long/fat-tailed. Intuitively, most customers who visit your store are not going to make a purchase and among those who do, there will be some super customers who purchase an outrageous amount more than the average customer. Some parametric models allow for zero-inflated outcomes such as Poisson, negative binomial, Conway-Maxwell-Poisson; however, they do not handle the long/fat-tailed explicitly. Even for non-parametric models such as decision tree ensembles, more resources (trees and splits) will be dedicated to separating zeros and handling outliers; this could lead to deterioration in performance. Using the real-world dataset UCI Online Retail, we will compare the performance of common approaches namely naive baseline regression, regression on winsorized outcome, regression on log-plus-one-transformed outcome to what Duncan suggested: hurdle model with Duan‚Äôs method. We will demonstrate why this approach outperforms the others in most evaluation metrics and why it might not in some.\nCode\nimport pandas as pd\nimport numpy as np\nimport random\nfrom ucimlrepo import fetch_ucirepo \nimport boto3\nimport json\nfrom tqdm.auto import tqdm\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom autogluon.tabular import TabularDataset, TabularPredictor\n\nfrom sklearn.metrics import (\n    mean_squared_error, mean_absolute_error, r2_score, median_absolute_error,\n    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n)\nfrom scipy.stats import pearsonr, wasserstein_distance\n\ndef calculate_regression_metrics(y_true, y_pred):\n    return {\n        'root_mean_squared_error': np.sqrt(mean_squared_error(y_true, y_pred)),\n        'mean_squared_error': mean_squared_error(y_true, y_pred),\n        'mean_absolute_error': mean_absolute_error(y_true, y_pred),\n        'r2': r2_score(y_true, y_pred),\n        'pearsonr': pearsonr(y_true, y_pred)[0],\n        'median_absolute_error': median_absolute_error(y_true, y_pred),\n        'earths_mover_distance': wasserstein_distance(y_true, y_pred)\n    }\n\ndef caluclate_classification_metrics(y_true, y_pred):\n    return {\n        'accuracy': accuracy_score(y_true, y_pred),\n        'precision': precision_score(y_true, y_pred, average='weighted'),\n        'recall': recall_score(y_true, y_pred, average='weighted'),\n        'f1_score': f1_score(y_true, y_pred, average='weighted'),\n        'confusion_matrix': confusion_matrix(y_true, y_pred)\n    }\n\ndef string_to_yearmon(date):\n    date = date.split()\n    date = date[0].split('/') + date[1].split(':')\n    date = date[2] + '-' + date[0].zfill(2) #+ '-' + date[1].zfill(2) + ' ' + date[3].zfill(2) + ':' + date[4].zfill(2)\n    return date\n\ndef call_llama(system_prompt, input):\n    template = f\"\"\"&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;{system_prompt}&lt;&lt;/SYS&gt;&gt;{input}[/INST]\"\"\"\n    client = boto3.client(service_name='bedrock-runtime',region_name='us-west-2')\n    body = json.dumps({\n        \"prompt\": template,\n        \"temperature\": 0.,\n        \"top_p\": 0.9,\n        \"max_gen_len\": 2048,\n    })\n    response = client.invoke_model(\n        body=body,\n        modelId='us.meta.llama3-2-90b-instruct-v1:0',\n        accept='application/json',\n        contentType='application/json'\n    )\n    response_body = json.loads(response['body'].read())\n    return response_body\n\ndef call_claude(system_prompt, input):\n    client = boto3.client(service_name='bedrock-runtime',region_name='us-west-2')\n    body=json.dumps(\n        {\n            \"anthropic_version\": \"bedrock-2023-05-31\",\n            \"max_tokens\": 2048,\n            \"messages\": [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": system_prompt + '\\n' + input,\n                    }\n                    ]\n                }\n                ]\n        }  \n    )  \n\n    \n    response = client.invoke_model(body=body, \n                                   modelId='anthropic.claude-3-5-sonnet-20241022-v2:0',\n                                   contentType='application/json',\n                                   accept='application/json')\n    response_body = json.loads(response.get('body').read())\n   \n    return response_body"
  },
  {
    "objectID": "posts/sales_prediction/index.html#this-is-not-a-drill-real-world-datasets-meticulous-feature-engineering-state-of-the-art-automl",
    "href": "posts/sales_prediction/index.html#this-is-not-a-drill-real-world-datasets-meticulous-feature-engineering-state-of-the-art-automl",
    "title": "Predict How Much A Customer Will Spend",
    "section": "This Is Not a Drill: Real-world Datasets, Meticulous Feature Engineering, State-of-the-art AutoML",
    "text": "This Is Not a Drill: Real-world Datasets, Meticulous Feature Engineering, State-of-the-art AutoML\nTo make this exercise as realistic as possible, we will use a real-world dataset (as opposed to a simulated one), perform as much feature engineering as we would in a real-world setting, and employ the best AutoML solution the market has to offer in AutoGluon.\n\n\nCode\nonline_retail = fetch_ucirepo(id=352) \ntransaction_df = online_retail['data']['original']\noriginal_nb = transaction_df.shape[0]\n\n#create yearmon for train-valid split\ntransaction_df['yearmon'] = transaction_df.InvoiceDate.map(string_to_yearmon)\n\n#get rid of transactions without cid\ntransaction_df = transaction_df[~transaction_df.CustomerID.isna()].reset_index(drop=True)\nhas_cid_nb = transaction_df.shape[0]\n\n#fill in unknown descriptions\ntransaction_df.Description = transaction_df.Description.fillna('UNKNOWN')\n\n#convert customer id to string\ntransaction_df['CustomerID'] = transaction_df['CustomerID'].map(lambda x: str(int(x)))\n\n#simplify by filtering unit price and quantity to be non-zero (get rid of discounts, cancellations, etc)\ntransaction_df = transaction_df[(transaction_df.UnitPrice&gt;0)&\\\n                                (transaction_df.Quantity&gt;0)].reset_index(drop=True)\nhas_sales_nb = transaction_df.shape[0]\n\n#add sales\ntransaction_df['Sales'] = transaction_df.UnitPrice * transaction_df.Quantity\n\n\nWe use the UCI Online Retail dataset, which contain transactions from a UK-based, non-store online retail from 2010-12 and 2011-12. We perform the following data processing:\n\nRemove transactions without CustomerID; from 541,909 to 406,829 transactions\nFilter out transactions where either UnitPrice or Quantity is less than zero; from 406,829 to 397,884 transactions\nFill in missing product Description with value UNKNOWN.\n\n\n\nCode\nprint(transaction_df.shape)\ntransaction_df.sample(5)\n\n\n(397884, 10)\n\n\n\n\n\n\n\n\n\nInvoiceNo\nStockCode\nDescription\nQuantity\nInvoiceDate\nUnitPrice\nCustomerID\nCountry\nyearmon\nSales\n\n\n\n\n15471\n538308\n20727\nLUNCH BAG BLACK SKULL.\n10\n12/10/2010 13:29\n1.65\n14107\nUnited Kingdom\n2010-12\n16.50\n\n\n12309\n537840\n85099B\nJUMBO BAG RED RETROSPOT\n20\n12/8/2010 15:03\n1.95\n13531\nUnited Kingdom\n2010-12\n39.00\n\n\n316007\n573665\n84029E\nRED WOOLLY HOTTIE WHITE HEART.\n4\n10/31/2011 16:24\n4.25\n13982\nUnited Kingdom\n2011-10\n17.00\n\n\n138956\n554131\n84879\nASSORTED COLOUR BIRD ORNAMENT\n16\n5/23/2011 9:43\n1.69\n13988\nUnited Kingdom\n2011-05\n27.04\n\n\n367027\n578472\n82494L\nWOODEN FRAME ANTIQUE WHITE\n18\n11/24/2011 12:40\n2.95\n12476\nGermany\n2011-11\n53.10\n\n\n\n\n\n\n\nWe formulate the problem as predicting the sales (TargetSales) during Q4 2011 for each customers who bought at least one item during Q1-Q3 2011. Note that we are interested in predicting the spend per customer as accurately as possible; this is common for marketing use cases such as determining what spend threshold to give each customer in a promotion, targeting customers for upselling, or detecting early signs of churns. It is notably different from predicting total spend of all customers during a time period, which usually requires a different approach.\n\n\nCode\nfeature_period = {'start': '2011-01', 'end': '2011-09'}\noutcome_period = {'start': '2011-10', 'end': '2011-12'}\n\nfeature_transaction = transaction_df[(transaction_df.yearmon&gt;=feature_period['start'])&\\\n                                      (transaction_df.yearmon&lt;=feature_period['end'])]\noutcome_transaction = transaction_df[(transaction_df.yearmon&gt;=outcome_period['start'])&\\\n                                      (transaction_df.yearmon&lt;=outcome_period['end'])]\n\n#aggregate sales during outcome period\noutcome_sales = outcome_transaction.groupby('CustomerID').Sales.sum().reset_index()\n\n#aggregate sales during feature period\nfeature_sales = feature_transaction.groupby('CustomerID').Sales.sum().reset_index()\n\n#merge to get TargetSales including those who spent during feature period but not during outcome (zeroes)\noutcome_df = feature_sales[['CustomerID']].merge(outcome_sales, on='CustomerID', how='left')\noutcome_df['Sales'] = outcome_df['Sales'].fillna(0)\noutcome_df.columns = ['CustomerID', 'TargetSales']\n\n\nWe transform the transaction dataset into a customer-level dataset where we calculate features using transactions between 2011-01 to 2011-09 and outcome using transactions between 2011-10 to 2011-12, summing Quantity times UnitPrice. We left-join the customers in feature set to outcome set. This will result in the zero-inflated nature of the outcome as not all customers will come back in Q4. The distribution of non-zero sales is naturally long/fat-tailed with a few customers having extraordinarily high amount of sales in Q4. This resulted in a customer-level dataset with 3,438 customers.\n\n\nCode\n#confirm zero-inflated, long/fat-tailed\noutcome_df.TargetSales.describe(percentiles=[i/10 for i in range(10)])\n\n\ncount      3438.000000\nmean        666.245829\nstd        4016.843037\nmin           0.000000\n0%            0.000000\n10%           0.000000\n20%           0.000000\n30%           0.000000\n40%           0.000000\n50%         102.005000\n60%         263.006000\n70%         425.790000\n80%         705.878000\n90%        1273.611000\nmax      168469.600000\nName: TargetSales, dtype: float64\n\n\n\n\nCode\n#confirm zero-inflated, long/fat-tailed\noutcome_df[outcome_df.TargetSales&lt;=10_000].TargetSales.hist(bins=100)\n\n\n\n\n\n\n\n\n\nWe represent a customer using traditional RFM features namely recency of purchase, purchase days, total sales, number of distinct products purchased, number of distinct category purchased, customer tenure within 2011, average purchase frequency, average purchase value, and percentage of purchase across all 9 categories. This is based on data from Q1-Q3 2011.\nSince the UCI Online Retail dataset does not have a category but only contains descriptions over 3,000 items, we use LLaMA 3.2 90B to infer categories based on randomly selected 1,000 descriptions. This is to make the category preference representation for each customer, which is more tractable than including features about all 3,548 items. After that, we use Claude 3.5 v2 to label a category for each description as it performs structured output a little more reliably. The categories are:\n\nHome Decor\nKitchen and Dining\nFashion Accessories\nStationary and Gifts\nToys and Games\nSeasonal and Holiday\nPersonal Care and Wellness\nOutdoor and Garden\nOthers\n\n\n\nCode\ndescriptions = feature_transaction.Description.unique().tolist()\nprint(descriptions[:5])\n\n#randomize descriptions with seed 112 to get which categories we should use\nnp.random.seed(112)\nrandom_descriptions = np.random.choice(descriptions, 1000, replace=False)\n\nres = call_llama(\n    'You are a product categorization assistant at a retail website.',\n    'Given the following product descriptions, come up with a few product categories they should be classified into.'+'\\n'.join(random_descriptions)\n)\n\ncategories = [\n    'Home Decor',\n    'Kitchen and Dining',\n    'Fashion Accessories',\n    'Stationary and Gifts',\n    'Toys and Games',\n    'Seasonal and Holiday',\n    'Personal Care and Wellness',\n    'Outdoor and Garden',   \n]\n\nprint(res['generation'])\n\n\n['JUMBO BAG PINK POLKADOT', 'BLUE POLKADOT WRAP', 'RED RETROSPOT WRAP ', 'RECYCLING BAG RETROSPOT ', 'RED RETROSPOT SHOPPER BAG']\n &lt;&lt;SYS&gt;&gt;Based on the product descriptions, I would categorize them into the following categories:\n\n1. Home Decor:\n    * Wall art\n    * Decorative items (e.g. vases, figurines, etc.)\n    * Lighting (e.g. candles, lanterns, etc.)\n    * Textiles (e.g. throw pillows, blankets, etc.)\n2. Kitchen and Dining:\n    * Cookware and utensils\n    * Tableware (e.g. plates, cups, etc.)\n    * Kitchen decor (e.g. signs, magnets, etc.)\n    * Food and drink items (e.g. tea, coffee, etc.)\n3. Fashion and Accessories:\n    * Jewelry (e.g. necklaces, earrings, etc.)\n    * Handbags and wallets\n    * Clothing and accessories (e.g. scarves, hats, etc.)\n    * Beauty and personal care items (e.g. cosmetics, skincare, etc.)\n4. Stationery and Gifts:\n    * Greeting cards\n    * Gift wrap and bags\n    * Stationery (e.g. notebooks, pens, etc.)\n    * Gift items (e.g. mugs, keychains, etc.)\n5. Toys and Games:\n    * Toys (e.g. stuffed animals, puzzles, etc.)\n    * Games and puzzles\n    * Outdoor toys and games\n6. Seasonal and Holiday:\n    * Christmas decorations and gifts\n    * Easter decorations and gifts\n    * Other seasonal and holiday items (e.g. Halloween, Valentine's Day, etc.)\n7. Miscellaneous:\n    * Pet accessories\n    * Home office supplies\n    * Travel accessories\n    * Other miscellaneous items that don't fit into the above categories\n\nNote that some products may fit into multiple categories, but I've tried to categorize them based on their primary function or theme.\n\n\n\n\nCode\n#loop through descriptions in batches of batch_size\nres_texts = []\nbatch_size = 100\nfor i in tqdm(range(0, len(descriptions), batch_size)):\n    batch = descriptions[i:i+batch_size]\n    d = \"\\n\".join(batch)\n    inp = f'''Categorize the following product descriptions into {\", \".join(categories)} or Others, if they do not fall into any. \nOnly answer in the following format:\n\n\"product description of product #1\"|\"product category classified into\"\n\"product description of product #2\"|\"product category classified into\"\n...\n\"product description of product #n\"|\"product category classified into\"\n\nHere are the product descriptions:\n{d}\n'''\n    while True:\n        res = call_claude('You are a product categorizer at a retail website', inp)\n        # if res['generation_token_count'] &gt; 1: #for llama\n        if res['usage']['output_tokens'] &gt; 1:\n            break\n        else:\n            print('Retrying...')\n            time.sleep(2)\n    res_text = res['content'][0]['text'].strip().split('\\n')\n        #for llama\n        # .replace('[SYS]','').replace('&lt;&lt;SYS&gt;&gt;','')\\\n        # .replace('[/SYS]','').replace('&lt;&lt;/SYS&gt;&gt;','')\\\n    if res_text!='':\n        res_texts.extend(res_text)\n\nwith open('../../data/sales_prediction/product_description_category.csv','w') as f:\n    f.write('\"product_description\"|\"category\"\\n')\n    for i in res_texts:\n        f.write(f'{i}\\n')\n\n\nHere is the share of product descriptions in each annotated category:\n\n\nCode\nproduct_description_category = pd.read_csv('../../data/sales_prediction/product_description_category.csv',\n                                           sep='|')\n\n#clean product_description\nproduct_description_category['Description'] = descriptions\nproduct_description_category.category.value_counts(normalize=True)\n\n\ncategory\nHome Decor                    0.328636\nKitchen and Dining            0.195885\nFashion Accessories           0.138670\nStationary and Gifts          0.116122\nSeasonal and Holiday          0.087373\nPersonal Care and Wellness    0.047351\nToys and Games                0.045096\nOutdoor and Garden            0.032976\nOthers                        0.007892\nName: proportion, dtype: float64\n\n\nWe merge the RFM features with preference features, that is share of sales in each category for every customer, then the outcome TargetSales to create the universe set for the problem.\n\n\nCode\nfeature_transaction_cat = feature_transaction.merge(product_description_category,\n                                                    how='inner',\n                                                    on = 'Description',)\nfeature_transaction.shape, feature_transaction_cat.shape\n\n#convert invoice date to datetime\nfeature_transaction_cat['InvoiceDate'] = pd.to_datetime(feature_transaction_cat['InvoiceDate'])\n\n# last date in feature set\ncurrent_date = feature_transaction_cat['InvoiceDate'].max()\n\n#rfm\ncustomer_features = feature_transaction_cat.groupby('CustomerID').agg({\n    'InvoiceDate': [\n        ('recency', lambda x: (current_date - x.max()).days),\n        ('first_purchase_date', 'min'),\n        ('purchase_day', 'nunique'),\n    ],\n    'InvoiceNo': [('nb_invoice', 'nunique')],\n    'Sales': [\n        ('total_sales', 'sum')\n    ],\n    'StockCode': [('nb_product', 'nunique')],\n    'category': [('nb_category', 'nunique')]\n}).reset_index()\n\n# Flatten column names\ncustomer_features.columns = [\n    'CustomerID',\n    'recency',\n    'first_purchase_date',\n    'purchase_day',\n    'nb_invoice',\n    'total_sales',\n    'nb_product',\n    'nb_category'\n]\n\ncustomer_features['customer_lifetime'] = (current_date - customer_features['first_purchase_date']).dt.days\ncustomer_features['avg_purchase_frequency'] = customer_features['customer_lifetime'] / customer_features['purchase_day']\ncustomer_features['avg_purchase_value'] = customer_features['total_sales'] / customer_features['purchase_day']\n\n#category preference\ncategory_sales = feature_transaction_cat.pivot_table(\n    values='Sales', \n    index='CustomerID', \n    columns='category', \n    aggfunc='sum', \n    fill_value=0\n)\ncategory_sales.columns = [i.lower().replace(' ','_') for i in category_sales.columns]\ncustomer_features = customer_features.merge(category_sales, on='CustomerID', how='left')\n\ntotal_sales = customer_features['total_sales']\nfor col in category_sales.columns:\n    percentage_col = f'per_{col}'\n    customer_features[percentage_col] = customer_features[col] / total_sales\n\nselected_features = [\n 'recency',\n 'purchase_day',\n 'total_sales',\n 'nb_product',\n 'nb_category',\n 'customer_lifetime',\n 'avg_purchase_frequency',\n 'avg_purchase_value',\n 'per_fashion_accessories',\n 'per_home_decor',\n 'per_kitchen_and_dining',\n 'per_others',\n 'per_outdoor_and_garden',\n 'per_personal_care_and_wellness',\n 'per_seasonal_and_holiday',\n 'per_stationary_and_gifts',\n 'per_toys_and_games']\n\noutcome_variable = 'TargetSales'\n\ncustomer_features = customer_features[[ 'CustomerID']+selected_features]\ndf = outcome_df.merge(customer_features, on='CustomerID').drop('CustomerID', axis=1)\nprint(df.shape)\ndf.sample(5)\n\n\n(3438, 18)\n\n\n\n\n\n\n\n\n\nTargetSales\nrecency\npurchase_day\ntotal_sales\nnb_product\nnb_category\ncustomer_lifetime\navg_purchase_frequency\navg_purchase_value\nper_fashion_accessories\nper_home_decor\nper_kitchen_and_dining\nper_others\nper_outdoor_and_garden\nper_personal_care_and_wellness\nper_seasonal_and_holiday\nper_stationary_and_gifts\nper_toys_and_games\n\n\n\n\n2606\n0.00\n53\n2\n597.48\n138\n8\n184\n92.000000\n298.740\n0.079383\n0.433973\n0.343710\n0.003465\n0.000000\n0.041357\n0.016570\n0.056688\n0.024854\n\n\n196\n0.00\n78\n2\n2209.85\n37\n6\n226\n113.000000\n1104.925\n0.030771\n0.275245\n0.628549\n0.000000\n0.021178\n0.022535\n0.000000\n0.021721\n0.000000\n\n\n2900\n3893.79\n10\n6\n4099.11\n78\n9\n172\n28.666667\n683.185\n0.003879\n0.761507\n0.104540\n0.003879\n0.012442\n0.014015\n0.051597\n0.043312\n0.004830\n\n\n2187\n0.00\n227\n1\n122.40\n1\n1\n227\n227.000000\n122.400\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n322\n0.00\n68\n1\n147.12\n3\n2\n68\n68.000000\n147.120\n0.881729\n0.118271\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n\n\n\n\n\nUnivariate correlation expectedly pinpoints total_sales in during Q1-Q3 2011 as the most predictive feature; however, we can see that it is still not very predictive. This shows that the problem is not a trivial one.\n\n\nCode\nprint(df[['TargetSales','total_sales']].corr())\n\n#target and most predictive variable\ndf[df.TargetSales&lt;=25_000].plot.scatter(x='TargetSales',y='total_sales')\n\n\n             TargetSales  total_sales\nTargetSales     1.000000     0.558558\ntotal_sales     0.558558     1.000000\n\n\n\n\n\n\n\n\n\nWe randomly split the dataset into train and test sets at 80/20 ratio. We also confirm the distribution of TargetSales is similar across percentiles between train and test and only different at the upper end.\n\n\nCode\n#split into train-valid sets\ntrain_df, test_df = train_test_split(df,\n                                      test_size=0.2, \n                                      random_state=112)\npd.concat([train_df.TargetSales.describe(percentiles=[i/10 for i in range(10)]).reset_index(),\ntest_df.TargetSales.describe(percentiles=[i/10 for i in range(10)]).reset_index(),], axis=1)\n\n\n\n\n\n\n\n\n\nindex\nTargetSales\nindex\nTargetSales\n\n\n\n\n0\ncount\n2750.000000\ncount\n688.000000\n\n\n1\nmean\n642.650436\nmean\n760.558808\n\n\n2\nstd\n4015.305436\nstd\n4024.524400\n\n\n3\nmin\n0.000000\nmin\n0.000000\n\n\n4\n0%\n0.000000\n0%\n0.000000\n\n\n5\n10%\n0.000000\n10%\n0.000000\n\n\n6\n20%\n0.000000\n20%\n0.000000\n\n\n7\n30%\n0.000000\n30%\n0.000000\n\n\n8\n40%\n0.000000\n40%\n0.000000\n\n\n9\n50%\n91.350000\n50%\n113.575000\n\n\n10\n60%\n260.308000\n60%\n277.836000\n\n\n11\n70%\n426.878000\n70%\n418.187000\n\n\n12\n80%\n694.164000\n80%\n759.582000\n\n\n13\n90%\n1272.997000\n90%\n1255.670000\n\n\n14\nmax\n168469.600000\nmax\n77099.380000"
  },
  {
    "objectID": "posts/sales_prediction/index.html#naive-baseline-regression",
    "href": "posts/sales_prediction/index.html#naive-baseline-regression",
    "title": "Predict How Much A Customer Will Spend",
    "section": "Naive Baseline Regression",
    "text": "Naive Baseline Regression\nThe most naive solution is to simply predict TargetSales based on the features. We use a stacked ensemble of LightGBM, CatBoost, XGBoost, Random Forest and Extra Trees via AutoGluon. We train with good_quality preset, stated to be ‚ÄúStronger than any other AutoML Framework‚Äù, for speedy training and inference but feel free to try more performant options. We exclude the neural-network models as they require further preprocessing of the features. We use an industry-grade, non-parametric model to be as close to a real use case as possible and make a point that the methodology works not only in a toy-dataset setup.\n\n\nCode\npreset = 'good_quality'\n\npredictor = TabularPredictor(label='TargetSales').fit(train_df[selected_features + ['TargetSales']], \n                                                      presets=preset,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\ntest_df['pred_baseline'] = predictor.predict(test_df[selected_features])\n\n\n\n\nCode\nmetric_baseline = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_baseline'])\nmetric_baseline['model'] = 'baseline'\nmetric_baseline\n\n\n{'root_mean_squared_error': 3162.478744240967,\n 'mean_squared_error': 10001271.807775924,\n 'mean_absolute_error': 715.6442657130541,\n 'r2': 0.3816166296854987,\n 'pearsonr': 0.6190719671013133,\n 'median_absolute_error': 232.98208312988282,\n 'earths_mover_distance': 287.77728784026124,\n 'model': 'baseline'}"
  },
  {
    "objectID": "posts/sales_prediction/index.html#regression-on-winsorized-outcome",
    "href": "posts/sales_prediction/index.html#regression-on-winsorized-outcome",
    "title": "Predict How Much A Customer Will Spend",
    "section": "Regression on Winsorized Outcome",
    "text": "Regression on Winsorized Outcome\n\n\nCode\noutlier_per = 0.99\noutlier_cap_train = train_df['TargetSales'].quantile(outlier_per)\n\n\nAn alternative approach to deal with long/fat-tailed outcome is to train on a winsorized outcome. In our case, we cap the outlier at 99.0% or TargetSales equals 7,180.81. While this solves the long/fat-tailed issues, it does not deal with zero inflation and also introduce bias to the outcome. This leads to better performance when tested on the winsorized outcome, but not so much on the original outcome.\n\n\nCode\n#winsorize\ntrain_df['TargetSales_win'] = train_df['TargetSales'].map(lambda x: outlier_cap_train if x&gt; outlier_cap_train else x)\ntest_df['TargetSales_win'] = test_df['TargetSales'].map(lambda x: outlier_cap_train if x&gt; outlier_cap_train else x)\n\npredictor = TabularPredictor(label='TargetSales_win').fit(train_df[selected_features+['TargetSales_win']],\n                                                      presets=preset,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\n\ntest_df['pred_winsorized'] = predictor.predict(test_df[selected_features])\n\n\n\n\nCode\nmetric_winsorized = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_winsorized'])\nmetric_winsorized['model'] = 'winsorized'\nmetric_winsorized\n\n\n{'root_mean_squared_error': 3623.576377551195,\n 'mean_squared_error': 13130305.76394704,\n 'mean_absolute_error': 627.7880071099414,\n 'r2': 0.18814697894155963,\n 'pearsonr': 0.5757989413256978,\n 'median_absolute_error': 219.62248107910156,\n 'earths_mover_distance': 432.1288432991232,\n 'model': 'winsorized'}"
  },
  {
    "objectID": "posts/sales_prediction/index.html#regression-on-log-plus-one-transformed-outcome",
    "href": "posts/sales_prediction/index.html#regression-on-log-plus-one-transformed-outcome",
    "title": "Predict How Much A Customer Will Spend",
    "section": "Regression on Log-plus-one-transformed Outcome",
    "text": "Regression on Log-plus-one-transformed Outcome\nLog transformation handles long/fat-tailed distribution and is especially useful for certain models since the transformed distribution is closer normal. However, it cannot handle zero-valued outcome and oftentimes scientists end up adding 1 to the outcome (so often that numpy even has a function for it). This not only introduces bias to the prediction, but also does not solve the zero-inflation as it becomes one-inflation instead.\n\n\nCode\n#log\ntrain_df['TargetSales_log1p'] = train_df['TargetSales'].map(np.log1p)\ntest_df['TargetSales_log1p'] = test_df['TargetSales'].map(np.log1p)\n\n#from zero-inflated to one-inflated\ntrain_df['TargetSales_log1p'].hist()\n\n\n\n\n\n\n\n\n\nWe can see that this is the best performing approach so far, which is one of the reasons why so many scientists end up going for this not-entirely-correct approach.\n\n\nCode\npredictor = TabularPredictor(label='TargetSales_log1p').fit(train_df[selected_features+['TargetSales_log1p']],\n                                                      presets=preset,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\n\ntest_df['pred_log1p'] = predictor.predict(test_df[selected_features])\ntest_df['pred_log1p_expm1'] = test_df['pred_log1p'].map(np.expm1)\n\n\n\n\nCode\nmetric_log1p = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_log1p_expm1'])\nmetric_log1p['model'] = 'log1p'\nmetric_log1p\n\n\n{'root_mean_squared_error': 3725.342295894091,\n 'mean_squared_error': 13878175.221577456,\n 'mean_absolute_error': 618.9768466651894,\n 'r2': 0.14190585634701047,\n 'pearsonr': 0.5817166874396966,\n 'median_absolute_error': 89.55495441784018,\n 'earths_mover_distance': 581.0494444960044,\n 'model': 'log1p'}"
  },
  {
    "objectID": "posts/sales_prediction/index.html#hurdle-model",
    "href": "posts/sales_prediction/index.html#hurdle-model",
    "title": "Predict How Much A Customer Will Spend",
    "section": "Hurdle Model",
    "text": "Hurdle Model\nHurdle model is a two-stage approach that handles zero inflation by first having a classification model to predict if the outcome is zero or not, then a regression model, trained only on examples with actual non-zero outcomes, to fit a log-transformed outcome. When retransforming the predictions from log to non-log numbers, we perform correction of underestimation using Duan‚Äôs method. During inference time, we multiply the predictions from the classification and corrected regression model.\n\n\nCode\ntrain_df['has_purchase'] = train_df.TargetSales.map(lambda x: 1 if x&gt;0 else 0)\ntest_df['has_purchase'] = test_df.TargetSales.map(lambda x: 1 if x&gt;0 else 0)\n\npredictor_cls = TabularPredictor(label='has_purchase').fit(train_df[selected_features+['has_purchase']],\n                                                      presets=preset,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\ntest_df['pred_binary'] = predictor_cls.predict(test_df[selected_features])\n\n\nFor our splits, 51.42% of train and 53.05% of test include customers with non-zero purchase outcome. As with all two-stage approaches, we need to make sure the intermediate model performs reasonably in classifying zero/non-zero outcomes.\n\n\nCode\ncaluclate_classification_metrics(test_df['has_purchase'], test_df['pred_binary'])\n\n\n{'accuracy': 0.6918604651162791,\n 'precision': 0.6941069004479309,\n 'recall': 0.6918604651162791,\n 'f1_score': 0.6921418829824787,\n 'confusion_matrix': array([[229,  94],\n        [118, 247]])}\n\n\n\n\nCode\ntrain_df_nonzero = train_df[train_df.has_purchase==1].reset_index(drop=True)\ntest_df_nonzero = test_df[test_df.has_purchase==1].reset_index(drop=True)\n\n#log\ntrain_df_nonzero['TargetSales_log'] = train_df_nonzero['TargetSales'].map(np.log)\ntest_df_nonzero['TargetSales_log'] = test_df_nonzero['TargetSales'].map(np.log)\n\n\nAfter that, we perform log-transformed regression on the examples with non-zero outcome (1,414 examples in train). Without the need to worry about ln(0) outcome, the regression is much more straightforward albeit with fewer examples to train on.\n\n\nCode\ntrain_df_nonzero['TargetSales_log'].hist()\n\n\n\n\n\n\n\n\n\n\n\nCode\npredictor_reg = TabularPredictor(label='TargetSales_log').fit(train_df_nonzero[selected_features+['TargetSales_log']],\n                                                      presets=preset,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\ntest_df_nonzero['pred_log'] = predictor_reg.predict(test_df_nonzero[selected_features])\ntest_df_nonzero['pred_log_exp'] = test_df_nonzero['pred_log'].map(np.exp)\n\ntest_df['pred_log'] = predictor_reg.predict(test_df[selected_features])\ntest_df['pred_log_exp'] = test_df['pred_log'].map(np.exp)\n\ntest_df['pred_hurdle'] = test_df.pred_binary * test_df.pred_log_exp\n\n\nFor inference, we combine the binary prediction (purchase/no purchase) from the classification model with the re-transformed (exponentialized) numerical prediction from the regression model by simply multiplying them together. As you can see, this approach yields the best performance so far and this is where I used to think everything has been accounted for.\n\n\nCode\nmetric_hurdle = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_hurdle'])\nmetric_hurdle['model'] = 'hurdle'\nmetric_hurdle\n\n\n{'root_mean_squared_error': 3171.760744960863,\n 'mean_squared_error': 10060066.22327469,\n 'mean_absolute_error': 584.9162934881963,\n 'r2': 0.3779813431428882,\n 'pearsonr': 0.6769697889999318,\n 'median_absolute_error': 199.1780137692856,\n 'earths_mover_distance': 286.381442541919,\n 'model': 'hurdle'}"
  },
  {
    "objectID": "posts/sales_prediction/index.html#but-wait-there-is-more„Éºenter-naihua-duan",
    "href": "posts/sales_prediction/index.html#but-wait-there-is-more„Éºenter-naihua-duan",
    "title": "Predict How Much A Customer Will Spend",
    "section": "But Wait, There Is More„ÉºEnter Naihua Duan",
    "text": "But Wait, There Is More„ÉºEnter Naihua Duan\nIn the previous section, we have blissfully assumed that we can freely log-transform and re-transform the outcome during training and inference without any bias. This is not the case as there is a small bias generated in the process due to the error term.\n\\[ln(y) = f(X) + \\epsilon\\]\nwhere\n\n\\(y\\) is actual outcome.\n\\(X\\) is the features.\n\\(f(.)\\) is a trained model.\n\\(\\epsilon\\) is the error term.\n\nwhen re-transforming\n\\[\n\\begin{align}\ny &= exp(ln(y)) \\\\\n&= exp(f(X) + \\epsilon ) \\\\\n&= exp(f(X)) \\cdot exp(\\epsilon) \\\\\nE[y] &= E[exp(f(X))] \\cdot E[exp(\\epsilon)]\n\\end{align}\n\\]\nThe average treatment affect (ATE; \\(E[y]\\)) is underestimated by \\(E[exp(\\epsilon)]\\). Naihua Duan (ÊÆµ‰πÉËèØ), a Taiwanese biostatistician, suggested a consistent estimator of \\(E[exp(\\epsilon)]\\) in his 1983 work as\n\\[\n\\begin{align}\n\\hat \\lambda &= E[exp(ln(y) - ln(\\hat y))]\n\\end{align}\n\\]\nwhere\n\n\\(\\hat \\lambda\\) is the Duan‚Äôs smearing estimator of the bias from re-transformation \\(E[exp(\\epsilon)]\\)\n\\(\\hat y\\) is the prediction aka \\(f(X)\\)\n\nFun Fact: If you assume Duan were a western name, you would have been \npronouncing the method's name incorrectly since it should be [tw√†n]'s \nmethod, NOT /dw…ën/'s method.\nWe can easily derive Duan‚Äôs smearing estimator by taking mean of error between actual and predicted TargetSales in the training set.\n\n\nCode\ntrain_df_nonzero['pred_log'] = predictor_reg.predict(train_df_nonzero[selected_features])\ntrain_df_nonzero['pred_log_exp'] = train_df_nonzero['pred_log'].map(np.exp)\n\nsmearing_estimator = np.mean(np.exp(train_df_nonzero['TargetSales_log'] - train_df_nonzero['pred_log']))\nsmearing_estimator\n\n\n1.2280991653046711\n\n\nWe multiply this to the predictions of the Hurdel model to correct the underestimation due to re-transformation bias.\n\n\nCode\ntest_df['pred_log_exp_corrected'] = test_df['pred_log_exp'] * smearing_estimator\ntest_df['pred_hurdle_corrected'] = test_df.pred_binary * test_df.pred_log_exp_corrected\n\nmetric_hurdle_corrected = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_hurdle_corrected'])\nmetric_hurdle_corrected['model'] = 'hurdle_corrected'\nmetric_hurdle_corrected\n\n\n{'root_mean_squared_error': 3055.3207868281233,\n 'mean_squared_error': 9334985.110424023,\n 'mean_absolute_error': 613.3946643257099,\n 'r2': 0.42281345159207295,\n 'pearsonr': 0.6769697889999318,\n 'median_absolute_error': 232.55557358084502,\n 'earths_mover_distance': 241.61839859133218,\n 'model': 'hurdle_corrected'}"
  },
  {
    "objectID": "posts/sales_prediction/index.html#the-eval-bar",
    "href": "posts/sales_prediction/index.html#the-eval-bar",
    "title": "Predict How Much A Customer Will Spend",
    "section": "The Eval Bar",
    "text": "The Eval Bar\nWe can see that the hurdle model with Duan‚Äôs correction performs best across majority of the metrics. We will now deep dive on metrics where it did not to understand the caveats when taking this approach.\n\n\nCode\nmetric_df = pd.DataFrame([metric_baseline,\n                       metric_winsorized,\n                       metric_log1p,\n                       metric_hurdle,\n                       metric_hurdle_corrected,])\n\nrank_df = metric_df.copy()\nfor col in metric_df.columns.tolist()[:-1]:\n    if col in ['r2', 'pearsonr', 'spearmanr']:\n        rank_df[f'{col}_rank'] = rank_df[col].rank(ascending=False)\n    else:\n        rank_df[f'{col}_rank'] = rank_df[col].rank(ascending=True)\nrank_df = rank_df.drop(metric_df.columns.tolist()[:-1], axis=1)\nrank_df['avg_rank'] = rank_df.iloc[:,1:].mean(axis=1)\nrank_df.transpose()\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nmodel\nbaseline\nwinsorized\nlog1p\nhurdle\nhurdle_corrected\n\n\nroot_mean_squared_error_rank\n2.0\n4.0\n5.0\n3.0\n1.0\n\n\nmean_squared_error_rank\n2.0\n4.0\n5.0\n3.0\n1.0\n\n\nmean_absolute_error_rank\n5.0\n4.0\n3.0\n1.0\n2.0\n\n\nr2_rank\n2.0\n4.0\n5.0\n3.0\n1.0\n\n\npearsonr_rank\n3.0\n5.0\n4.0\n1.5\n1.5\n\n\nmedian_absolute_error_rank\n5.0\n3.0\n1.0\n2.0\n4.0\n\n\nearths_mover_distance_rank\n3.0\n4.0\n5.0\n2.0\n1.0\n\n\navg_rank\n3.142857\n4.0\n4.0\n2.214286\n1.642857\n\n\n\n\n\n\n\n\n\nCode\nmetric_df.transpose()\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nroot_mean_squared_error\n3162.478744\n3623.576378\n3725.342296\n3171.760745\n3055.320787\n\n\nmean_squared_error\n10001271.807776\n13130305.763947\n13878175.221577\n10060066.223275\n9334985.110424\n\n\nmean_absolute_error\n715.644266\n627.788007\n618.976847\n584.916293\n613.394664\n\n\nr2\n0.381617\n0.188147\n0.141906\n0.377981\n0.422813\n\n\npearsonr\n0.619072\n0.575799\n0.581717\n0.67697\n0.67697\n\n\nmedian_absolute_error\n232.982083\n219.622481\n89.554954\n199.178014\n232.555574\n\n\nearths_mover_distance\n287.777288\n432.128843\n581.049444\n286.381443\n241.618399\n\n\nmodel\nbaseline\nwinsorized\nlog1p\nhurdle\nhurdle_corrected\n\n\n\n\n\n\n\n\nWhy Duan‚Äôs Correction Results in Slightly Worse MAE?\nDuan‚Äôs method adjusts for underestimation from re-transformation of log outcome. This could lead to smaller extreme errors, but more frequent occurrences of less extreme ones. We verify this hypothesis by comparing mean absolute error before and after transformation for errors originally under and over 99th percentile. We confirm that is the case for our problem.\n\n\nCode\nerr_hurdle = (test_df['TargetSales'] - test_df['pred_hurdle']).abs()\nerr_hurdle_corrected = (test_df['TargetSales'] - test_df['pred_hurdle_corrected']).abs()\n\nprint('Distribution of errors for Hurdle model without correction')\nerr_hurdle.describe(percentiles=[.25, .5, .75, .9, .95, .99]) \n\n\nDistribution of errors for Hurdle model without correction\n\n\ncount      688.000000\nmean       584.916293\nstd       3119.628924\nmin          0.000000\n25%          0.000000\n50%        199.178014\n75%        475.603446\n90%        862.530026\n95%       1237.540954\n99%       6763.777844\nmax      55731.205996\ndtype: float64\n\n\n\n\nCode\nprint('Hurdle Model without correction')\nprint(f'Mean absolute error under 99th percentile: {err_hurdle[err_hurdle&lt;6763.777844].mean()}')\nprint(f'Mean absolute error over 99th percentile: {err_hurdle[err_hurdle&gt;6763.777844].mean()}')\n\nprint('Hurdle Model with correction')\nprint(f'Mean absolute error under 99th percentile: {err_hurdle_corrected[err_hurdle&lt;6763.777844].mean()}')\nprint(f'Mean absolute error over 99th percentile: {err_hurdle_corrected[err_hurdle&gt;6763.777844].mean()}')\n\n\nHurdle Model without correction\nMean absolute error under 99th percentile: 355.4918014848842\nMean absolute error over 99th percentile: 22904.641872667555\nHurdle Model with correction\nMean absolute error under 99th percentile: 392.7718802742851\nMean absolute error over 99th percentile: 22076.839798471465\n\n\n\n\nImportance of Classification Model\nThe overperformance of log-transform regression over both hurdle model approarches in Spearman‚Äôs rank correlation and median absolute error demonstrates the importance of a classification model. At first glance, it is perplexing since we have just spent a large portion of this article to justify that hurdle models handle zero inflation better and re-transformation without Duan‚Äôs method is biased. However, it becomes clear once you compare performance of the hurdle model with a classification model (f1 = 0.69) and a hypothetical, perfect classification model. Other metrics also improved but not nearly as drastic as MedAE and MAE.\n\n\nCode\ntest_df['pred_hurdle_corrected_perfect_cls'] = test_df.has_purchase * test_df.pred_log_exp_corrected\nmetric_hurdle_corrected_perfect_cls = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_hurdle_corrected_perfect_cls'])\nmetric_hurdle_corrected_perfect_cls['model'] = 'hurdle_corrected_perfect_cls'\n\nmetric_df2 = pd.DataFrame([metric_baseline,\n                       metric_winsorized,\n                       metric_log1p,\n                       metric_hurdle,\n                       metric_hurdle_corrected,\n                       metric_hurdle_corrected_perfect_cls,])\nmetric_df2.transpose()\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\nroot_mean_squared_error\n3162.478744\n3623.576378\n3725.342296\n3171.760745\n3055.320787\n3030.854831\n\n\nmean_squared_error\n10001271.807776\n13130305.763947\n13878175.221577\n10060066.223275\n9334985.110424\n9186081.006625\n\n\nmean_absolute_error\n715.644266\n627.788007\n618.976847\n584.916293\n613.394664\n479.558294\n\n\nr2\n0.381617\n0.188147\n0.141906\n0.377981\n0.422813\n0.43202\n\n\npearsonr\n0.619072\n0.575799\n0.581717\n0.67697\n0.67697\n0.687639\n\n\nmedian_absolute_error\n232.982083\n219.622481\n89.554954\n199.178014\n232.555574\n34.991964\n\n\nearths_mover_distance\n287.777288\n432.128843\n581.049444\n286.381443\n241.618399\n234.587018\n\n\nmodel\nbaseline\nwinsorized\nlog1p\nhurdle\nhurdle_corrected\nhurdle_corrected_perfect_cls\n\n\n\n\n\n\n\n\n\nRemember What Problem We Are Solving\nOne last thing to remember is that we are trying to predict sales of each individual customer, not total sales of all customers. If we look at aggregated mean or sum of actual sales vs predicted sales, baseline regression performs best by far. This is due to the fact that without any constraints a regressor only minimizes the MSE loss and usually ends up predicting values around the mean to balance between under- and over-predictions. However, this level of prediction is often not very useful as a single point. Imagine you want to give promotions with higher or lower spend thresholds to customers according to their purchasing power; you will not be able to do so with a model that is accurate on aggregate but not so much on individual customers.\n\n\nCode\ntest_df[['TargetSales','pred_baseline','pred_winsorized','pred_log1p_expm1','pred_hurdle','pred_hurdle_corrected']].mean()\n\n\nTargetSales              760.558808\npred_baseline            791.043945\npred_winsorized          508.281555\npred_log1p_expm1         186.200281\npred_hurdle              527.286811\npred_hurdle_corrected    647.560493\ndtype: float64"
  },
  {
    "objectID": "posts/sales_prediction/index.html#closing-remarks",
    "href": "posts/sales_prediction/index.html#closing-remarks",
    "title": "Predict How Much A Customer Will Spend",
    "section": "Closing Remarks",
    "text": "Closing Remarks\nAnd this is how you predict how much a customer will spend in the least wrong way. My hope is that you will not need to spend ten years in data science to find out how to do it like I did."
  }
]