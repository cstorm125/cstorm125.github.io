[
  {
    "objectID": "notebook/sql_almost_exercise.html#sanity-checks",
    "href": "notebook/sql_almost_exercise.html#sanity-checks",
    "title": "(Almost) All SQL You Need Exercise: Personalized Recommendation",
    "section": "Sanity Checks",
    "text": "Sanity Checks\nAfter downloading and creating transaction_tbl in our database, let us first do some sanity checks.\n\n# @title Query to randomly see 100 rows\n\nanswer_key = f\"\"\"\nselect * from transaction_tbl limit 100;\n\"\"\"\nhint = 'select * and limit'\n\n#input\ntext_area = widgets.Textarea(\n    value=\"Write your query here.\",\n    rows=10,  # Initial number of visible rows\n    description=\"Query:\",\n    layout={'width': '730px'} # Adjust width as needed\n)\n\n#output\nquery_result = query_result = widgets.Output(\n    layout=widgets.Layout(\n        border='1px solid lightgray', # Add a border to make the scrollable area visible\n        height='300px',              # Fixed height for the output area\n        overflow_y='scroll'          # Enable vertical scrolling\n    )\n)\n\n#button\nexecute_button = widgets.Button(\n    description='Execute query',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef process_query(b):\n    query = text_area.value\n    with query_result:\n        query_result.clear_output()\n        display(execute_query(query))\nexecute_button.on_click(process_query)\n\nanswer_button = widgets.Button(\n    description='Reveal answer key',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef show_answer_key(b):\n    query = text_area.value\n    with query_result:\n        query_result.clear_output()\n        print(f'Answer key is: \\n{answer_key}')\nanswer_button.on_click(show_answer_key)\n\nhint_button = widgets.Button(\n    description='Reveal hint',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef show_hint(b):\n    with query_result:\n        query_result.clear_output()\n        print(f'Hint: \\n{hint}')\nhint_button.on_click(show_hint)\n\ndisplay(text_area, execute_button, hint_button, answer_button, query_result)\n\n\n# @title Count how many events, how many unique customers, how many unique items, how many unique event types.\n\nanswer_key = f\"\"\"\nselect\n count(*) as nb_event\n ,count(distinct user_id) as nb_user\n ,count(distinct item_id) as nb_item\n ,count(distinct behavior_type) as nb_event_type\nfrom transaction_tbl;\n\"\"\"\nhint = 'count and count distinct'\n\n#input\ntext_area = widgets.Textarea(\n    value=\"Write your query here.\",\n    rows=10,  # Initial number of visible rows\n    description=\"Query:\",\n    layout={'width': '730px'} # Adjust width as needed\n)\n\n#output\nquery_result = query_result = widgets.Output(\n    layout=widgets.Layout(\n        border='1px solid lightgray', # Add a border to make the scrollable area visible\n        height='300px',              # Fixed height for the output area\n        overflow_y='scroll'          # Enable vertical scrolling\n    )\n)\n\n#button\nexecute_button = widgets.Button(\n    description='Execute query',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef process_query(b):\n    query = text_area.value\n    with query_result:\n        query_result.clear_output()\n        display(execute_query(query))\nexecute_button.on_click(process_query)\n\nanswer_button = widgets.Button(\n    description='Reveal answer key',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef show_answer_key(b):\n    query = text_area.value\n    with query_result:\n        query_result.clear_output()\n        print(f'Answer key is: \\n{answer_key}')\nanswer_button.on_click(show_answer_key)\n\nhint_button = widgets.Button(\n    description='Reveal hint',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef show_hint(b):\n    with query_result:\n        query_result.clear_output()\n        print(f'Hint: \\n{hint}')\nhint_button.on_click(show_hint)\n\ndisplay(text_area, execute_button, hint_button, answer_button, query_result)\n\n\n# @title How many events and unique users per event type?\n\nanswer_key = f\"\"\"\nselect\n behavior_type\n ,count(*) as nb_event\n ,count(distinct user_id) as nb_user\nfrom transaction_tbl\ngroup by 1;\n\"\"\"\nhint = 'group by'\n\n#input\ntext_area = widgets.Textarea(\n    value=\"Write your query here.\",\n    rows=10,  # Initial number of visible rows\n    description=\"Query:\",\n    layout={'width': '730px'} # Adjust width as needed\n)\n\n#output\nquery_result = query_result = widgets.Output(\n    layout=widgets.Layout(\n        border='1px solid lightgray', # Add a border to make the scrollable area visible\n        height='300px',              # Fixed height for the output area\n        overflow_y='scroll'          # Enable vertical scrolling\n    )\n)\n\n#button\nexecute_button = widgets.Button(\n    description='Execute query',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef process_query(b):\n    query = text_area.value\n    with query_result:\n        query_result.clear_output()\n        display(execute_query(query))\nexecute_button.on_click(process_query)\n\nanswer_button = widgets.Button(\n    description='Reveal answer key',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef show_answer_key(b):\n    query = text_area.value\n    with query_result:\n        query_result.clear_output()\n        print(f'Answer key is: \\n{answer_key}')\nanswer_button.on_click(show_answer_key)\n\nhint_button = widgets.Button(\n    description='Reveal hint',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef show_hint(b):\n    with query_result:\n        query_result.clear_output()\n        print(f'Hint: \\n{hint}')\nhint_button.on_click(show_hint)\n\ndisplay(text_area, execute_button, hint_button, answer_button, query_result)\n\n\n# @title Convert integer timestamp into human-readable timestamp `event_timestamp` and date `event_date`, then save as `transaction_tbl_x` view.\n\nanswer_key = f\"\"\"\ncreate or replace view transaction_tbl_x as (\nselect\n a.*\n ,to_timestamp(a.timestamp) as event_timestamp\n ,strftime(to_timestamp(a.timestamp), '%Y-%m-%d') as event_date\n ,substring(cast(to_timestamp(a.timestamp)as varchar),1,7) as year_month --optional\nfrom transaction_tbl a\n);\n\nselect * from transaction_tbl_x limit 10;\n\"\"\"\nhint = 'to_timestamp and date_trunc'\n\n#input\ntext_area = widgets.Textarea(\n    value=\"Write your query here.\",\n    rows=10,  # Initial number of visible rows\n    description=\"Query:\",\n    layout={'width': '730px'} # Adjust width as needed\n)\n\n#output\nquery_result = query_result = widgets.Output(\n    layout=widgets.Layout(\n        border='1px solid lightgray', # Add a border to make the scrollable area visible\n        height='300px',              # Fixed height for the output area\n        overflow_y='scroll'          # Enable vertical scrolling\n    )\n)\n\n#button\nexecute_button = widgets.Button(\n    description='Execute query',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef process_query(b):\n    query = text_area.value\n    with query_result:\n        query_result.clear_output()\n        display(execute_query(query))\nexecute_button.on_click(process_query)\n\nanswer_button = widgets.Button(\n    description='Reveal answer key',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef show_answer_key(b):\n    query = text_area.value\n    with query_result:\n        query_result.clear_output()\n        print(f'Answer key is: \\n{answer_key}')\nanswer_button.on_click(show_answer_key)\n\nhint_button = widgets.Button(\n    description='Reveal hint',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef show_hint(b):\n    with query_result:\n        query_result.clear_output()\n        print(f'Hint: \\n{hint}')\nhint_button.on_click(show_hint)\n\ndisplay(text_area, execute_button, hint_button, answer_button, query_result)\n\n\n# @title Check distribution of `event_date`. Is there anything weird?\n\nanswer_key = f\"\"\"\nselect\n event_date\n ,count(*) as nb_event\nfrom transaction_tbl_x\ngroup by 1\norder by 1;\n\"\"\"\nhint = 'Do not forget we are using `transaction_tbl_x` now. Order by `event_date` to see if there is anything odd.'\n\n#input\ntext_area = widgets.Textarea(\n    value=\"Write your query here.\",\n    rows=10,  # Initial number of visible rows\n    description=\"Query:\",\n    layout={'width': '730px'} # Adjust width as needed\n)\n\n#output\nquery_result = widgets.Output(\n    layout=widgets.Layout(\n        border='1px solid lightgray', # Add a border to make the scrollable area visible\n        height='300px',              # Fixed height for the output area\n        overflow_y='scroll'          # Enable vertical scrolling\n    )\n)\n\n#button\nexecute_button = widgets.Button(\n    description='Execute query',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef process_query(b):\n    query = text_area.value\n    with query_result:\n        query_result.clear_output()\n        display(execute_query(query))\nexecute_button.on_click(process_query)\n\nanswer_button = widgets.Button(\n    description='Reveal answer key',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef show_answer_key(b):\n    query = text_area.value\n    with query_result:\n        query_result.clear_output()\n        print(f'Answer key is: \\n{answer_key}')\nanswer_button.on_click(show_answer_key)\n\nhint_button = widgets.Button(\n    description='Reveal hint',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef show_hint(b):\n    with query_result:\n        query_result.clear_output()\n        print(f'Hint: \\n{hint}')\nhint_button.on_click(show_hint)\n\ndisplay(text_area, execute_button, hint_button, answer_button, query_result)\n\nIt is quite clear from exploration that most of the events come from 2017-11-25 to 2017-12-03, so let us use 2017-11-25 to 2017-11-30 as training set and 2017-12-01 to 2017-12-03 as test set. If this were your production database, having events from the 1920s mixed in there should raise a flag but we will ignore them in this educational setting.\n\n# @title Create `train_tbl` with events from `2017-11-25` to `2017-11-30` and `test_tbl` with events from `2017-12-01` to `2017-12-03`\n\nanswer_key = f\"\"\"\ncreate or replace view train_tbl as (\nselect\n *\nfrom transaction_tbl_x\nwhere event_date between '2017-11-25' and '2017-11-30'\n);\n\ncreate or replace view test_tbl as (\nselect\n *\nfrom transaction_tbl_x\nwhere event_date between '2017-12-01' and '2017-12-03'\n);\n\nselect 'train' split, count(*) nb_event, count(distinct user_id) nb_user from train_tbl\nunion all\nselect 'test' split, count(*) nb_event, count(distinct user_id) nb_user from test_tbl;\n\"\"\"\nhint = 'where-clause and `between`'\n\n#input\ntext_area = widgets.Textarea(\n    value=\"Write your query here.\",\n    rows=10,  # Initial number of visible rows\n    description=\"Query:\",\n    layout={'width': '730px'} # Adjust width as needed\n)\n\n#output\nquery_result = query_result = widgets.Output(\n    layout=widgets.Layout(\n        border='1px solid lightgray', # Add a border to make the scrollable area visible\n        height='300px',              # Fixed height for the output area\n        overflow_y='scroll'          # Enable vertical scrolling\n    )\n)\n\n#button\nexecute_button = widgets.Button(\n    description='Execute query',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef process_query(b):\n    query = text_area.value\n    with query_result:\n        query_result.clear_output()\n        display(execute_query(query))\nexecute_button.on_click(process_query)\n\nanswer_button = widgets.Button(\n    description='Reveal answer key',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef show_answer_key(b):\n    query = text_area.value\n    with query_result:\n        query_result.clear_output()\n        print(f'Answer key is: \\n{answer_key}')\nanswer_button.on_click(show_answer_key)\n\nhint_button = widgets.Button(\n    description='Reveal hint',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef show_hint(b):\n    with query_result:\n        query_result.clear_output()\n        print(f'Hint: \\n{hint}')\nhint_button.on_click(show_hint)\n\ndisplay(text_area, execute_button, hint_button, answer_button, query_result)"
  },
  {
    "objectID": "notebook/sql_almost_exercise.html#success-measurement-metric-and-baseline",
    "href": "notebook/sql_almost_exercise.html#success-measurement-metric-and-baseline",
    "title": "(Almost) All SQL You Need Exercise: Personalized Recommendation",
    "section": "Success Measurement: Metric and Baseline",
    "text": "Success Measurement: Metric and Baseline\nOur task is to recommend 10 items that each user will most likely purchase during the test set period. The only information we have is what they viewed (pv), added to cart (cart) or favorited (fav) during the training set period.\n\nMetric\nOur metric is top-10 hit rate based on purchases. This means that for each customer, we will recommend 10 items and if the user purchased any of those items during the test set period, the user is marked as 1 else as 0. We then take an average across all users to get our score.\nExample: top-3 hit rate calculation\n\n\n\nuser_id\npred\nis_purchased_in_test_tbl\n\n\n\n\nA\nX\n0\n\n\nA\nY\n0\n\n\nA\nZ\n1\n\n\nB\nW\n0\n\n\nB\nT\n0\n\n\nB\nR\n0\n\n\nC\nQ\n0\n\n\nC\nT\n1\n\n\nC\nX\n1\n\n\n\n\n\n\nuser_id\nhit\nhit_flag\n\n\n\n\nA\n1\n1\n\n\nB\n0\n0\n\n\nC\n2\n1\n\n\n\n\ntop-3 hit rate = 1+0+1 / 3 = 66%\n\n\n\nBaseline\nThe most simple baseline we can compare our recommendation system with is recommending top-10 best-selling items to all users.\nLet us try to measure the performance of this simple baseline.\n\n# @title Create view `eval_tbl` containing `user_id`, `item_id` only for items the users in `test_tbl` purchased (`buy`)\n\nanswer_key = f\"\"\"\ncreate or replace view eval_tbl as (\nselect\n user_id\n ,item_id\nfrom test_tbl\nwhere behavior_type = 'buy'\ngroup by 1,2\n);\n\nselect * from eval_tbl;\n\"\"\"\nhint = 'group by and where'\n\n#input\ntext_area = widgets.Textarea(\n    value=\"Write your query here.\",\n    rows=10,  # Initial number of visible rows\n    description=\"Query:\",\n    layout={'width': '730px'} # Adjust width as needed\n)\n\n#output\nquery_result = query_result = widgets.Output(\n    layout=widgets.Layout(\n        border='1px solid lightgray', # Add a border to make the scrollable area visible\n        height='300px',              # Fixed height for the output area\n        overflow_y='scroll'          # Enable vertical scrolling\n    )\n)\n\n#button\nexecute_button = widgets.Button(\n    description='Execute query',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef process_query(b):\n    query = text_area.value\n    with query_result:\n        query_result.clear_output()\n        display(execute_query(query))\nexecute_button.on_click(process_query)\n\nanswer_button = widgets.Button(\n    description='Reveal answer key',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef show_answer_key(b):\n    query = text_area.value\n    with query_result:\n        query_result.clear_output()\n        print(f'Answer key is: \\n{answer_key}')\nanswer_button.on_click(show_answer_key)\n\nhint_button = widgets.Button(\n    description='Reveal hint',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef show_hint(b):\n    with query_result:\n        query_result.clear_output()\n        print(f'Hint: \\n{hint}')\nhint_button.on_click(show_hint)\n\ndisplay(text_area, execute_button, hint_button, answer_button, query_result)\n\n\n# @title Get top 10 most-interacted items in training set\n\nanswer_key = f\"\"\"\nselect\n item_id\n ,count(*) as nb_event\nfrom train_tbl\ngroup by 1\norder by 2 desc\nlimit 10;\n\"\"\"\nhint = 'group by and limit'\n\n#input\ntext_area = widgets.Textarea(\n    value=\"Write your query here.\",\n    rows=10,  # Initial number of visible rows\n    description=\"Query:\",\n    layout={'width': '730px'} # Adjust width as needed\n)\n\n#output\nquery_result = query_result = widgets.Output(\n    layout=widgets.Layout(\n        border='1px solid lightgray', # Add a border to make the scrollable area visible\n        height='300px',              # Fixed height for the output area\n        overflow_y='scroll'          # Enable vertical scrolling\n    )\n)\n\n#button\nexecute_button = widgets.Button(\n    description='Execute query',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef process_query(b):\n    query = text_area.value\n    with query_result:\n        query_result.clear_output()\n        display(execute_query(query))\nexecute_button.on_click(process_query)\n\nanswer_button = widgets.Button(\n    description='Reveal answer key',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef show_answer_key(b):\n    query = text_area.value\n    with query_result:\n        query_result.clear_output()\n        print(f'Answer key is: \\n{answer_key}')\nanswer_button.on_click(show_answer_key)\n\nhint_button = widgets.Button(\n    description='Reveal hint',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef show_hint(b):\n    with query_result:\n        query_result.clear_output()\n        print(f'Hint: \\n{hint}')\nhint_button.on_click(show_hint)\n\ndisplay(text_area, execute_button, hint_button, answer_button, query_result)\n\n\n# @title Create view `pred_top10_tbl` containing all `user_id` from `test_tbl`. Each `user_id` should have 10 rows each row with the 10 recommendations, in this case top-10 most interacted items (same for all `user_id`).\nanswer_key = f\"\"\"\ncreate or replace view pred_top10_tbl as (\n\nwith test_user_tbl as (\nselect\n user_id\n ,max(1) as joining_column\nfrom test_tbl\ngroup by 1),\n\ntop10_tbl as (\nselect\n item_id\n ,count(*) as nb_buy\n ,max(1) as joining_column\nfrom train_tbl\ngroup by 1\norder by 2 desc\nlimit 10)\n\nselect\n a.user_id\n ,b.item_id\nfrom test_user_tbl a\ninner join top10_tbl b\non a.joining_column = b.joining_column\n);\n\nselect count(*) nb_pred, count(distinct user_id) nb_user from pred_top10_tbl;\n\"\"\"\nhint = '''\nYou need to create a new `joining_column` that is all the same values max(1) to `inner join` on.\nOr use `cross join`: https://www.geeksforgeeks.org/sql/sql-cross-join/\ncount(distinct user_id) and count(*) again to make sure that each user has 10 recommendations.\n'''\n\n#input\ntext_area = widgets.Textarea(\n    value=\"Write your query here. This is a tough one so you might want to use the hint.\",\n    rows=10,  # Initial number of visible rows\n    description=\"Query:\",\n    layout={'width': '730px'} # Adjust width as needed\n)\n\n#output\nquery_result = query_result = widgets.Output(\n    layout=widgets.Layout(\n        border='1px solid lightgray', # Add a border to make the scrollable area visible\n        height='300px',              # Fixed height for the output area\n        overflow_y='scroll'          # Enable vertical scrolling\n    )\n)\n\n#button\nexecute_button = widgets.Button(\n    description='Execute query',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef process_query(b):\n    query = text_area.value\n    with query_result:\n        query_result.clear_output()\n        display(execute_query(query))\nexecute_button.on_click(process_query)\n\nanswer_button = widgets.Button(\n    description='Reveal answer key',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef show_answer_key(b):\n    query = text_area.value\n    with query_result:\n        query_result.clear_output()\n        print(f'Answer key is: \\n{answer_key}')\nanswer_button.on_click(show_answer_key)\n\nhint_button = widgets.Button(\n    description='Reveal hint',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef show_hint(b):\n    with query_result:\n        query_result.clear_output()\n        print(f'Hint: \\n{hint}')\nhint_button.on_click(show_hint)\n\ndisplay(text_area, execute_button, hint_button, answer_button, query_result)\n\n\n# @title With `pred_top10_tbl` and `eval_tbl`, calculate top-10 hit rate\n\nanswer_key = f\"\"\"\nselect\n avg(case when nb_hit &gt; 0 then 1 else 0 end) as top_10_hit_rate\nfrom\n(select\n a.user_id\n ,sum(case when b.item_id is not null then 1 else 0 end) as nb_hit\nfrom eval_tbl a\nleft join pred_top10_tbl b\non a.user_id = b.user_id\n and a.item_id = b.item_id\ngroup by 1) c;\n\"\"\"\nhint = f\"\"\"\nRemember that we average 1/0s across users, not items.\n\"\"\"\n\n#input\ntext_area = widgets.Textarea(\n    value=\"Write your query here.\",\n    rows=10,  # Initial number of visible rows\n    description=\"Query:\",\n    layout={'width': '730px'} # Adjust width as needed\n)\n\n#output\nquery_result = query_result = widgets.Output(\n    layout=widgets.Layout(\n        border='1px solid lightgray', # Add a border to make the scrollable area visible\n        height='300px',              # Fixed height for the output area\n        overflow_y='scroll'          # Enable vertical scrolling\n    )\n)\n\n#button\nexecute_button = widgets.Button(\n    description='Execute query',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef process_query(b):\n    query = text_area.value\n    with query_result:\n        query_result.clear_output()\n        display(execute_query(query))\nexecute_button.on_click(process_query)\n\nanswer_button = widgets.Button(\n    description='Reveal answer key',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef show_answer_key(b):\n    query = text_area.value\n    with query_result:\n        query_result.clear_output()\n        print(f'Answer key is: \\n{answer_key}')\nanswer_button.on_click(show_answer_key)\n\nhint_button = widgets.Button(\n    description='Reveal hint',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef show_hint(b):\n    with query_result:\n        query_result.clear_output()\n        print(f'Hint: \\n{hint}')\nhint_button.on_click(show_hint)\n\ndisplay(text_area, execute_button, hint_button, answer_button, query_result)\n\nConsidering we have 1,253,465 items in total, it is not surprising that our top-10 hit rate is only 0.0923% of all users in the test set. Let us see if we can do better."
  },
  {
    "objectID": "notebook/sql_almost_exercise.html#recommend-what-they-have-interacted-with-but-have-yet-bought",
    "href": "notebook/sql_almost_exercise.html#recommend-what-they-have-interacted-with-but-have-yet-bought",
    "title": "(Almost) All SQL You Need Exercise: Personalized Recommendation",
    "section": "Recommend what they have interacted with but have yet bought",
    "text": "Recommend what they have interacted with but have yet bought\nOne of the most powerful recommendation algorithm is to recommend what users have interacted with the most. In fact, this is what you would see in world‚Äôs best online retail sites, especially those dealing with groceries, beauty, and health and personal care. We add a small twist by only ordering the top-10 recommendation for each customer by all interactions (pv, cart, fav) EXCEPT buy. This is because if a user has bought the item before, it might not be likely for them to buy again in such as short period of time between training set and test set.\n\n# @title Create view `pred_mfp_tbl` containing all `user_id` from `train_tbl`. Each user should have 10 recommendations based on the top-10 items, ordered by the number of times they have interacted (`pv`, `cart`, `fav`) with, but have yet bought (`buy`).\n\nanswer_key = f\"\"\"\ncreate or replace view pred_mfp_tbl as (\nselect * from\n(select\n user_id\n ,item_id\n ,row_number() over (partition by user_id order by nb_nonbuy_event desc) as rnk\nfrom\n(select\n user_id\n ,item_id\n ,count(*) as nb_nonbuy_event\nfrom train_tbl\nwhere behavior_type &lt;&gt; 'buy'\ngroup by 1,2) a\n) b\nwhere rnk &lt;=10);\n\nselect\n nb_pred\n ,count(*) as nb_user\nfrom\n(select\n user_id\n ,count(*) as nb_pred\nfrom pred_mfp_tbl\ngroup by 1) a\ngroup by 1 order by 2 desc;\n\"\"\"\nhint = f\"\"\"\nWe need window function to make sure we have at most 10 items per customer.\n\"\"\"\n\n#input\ntext_area = widgets.Textarea(\n    value=\"Write your query here.\",\n    rows=10,  # Initial number of visible rows\n    description=\"Query:\",\n    layout={'width': '730px'} # Adjust width as needed\n)\n\n#output\nquery_result = query_result = widgets.Output(\n    layout=widgets.Layout(\n        border='1px solid lightgray', # Add a border to make the scrollable area visible\n        height='300px',              # Fixed height for the output area\n        overflow_y='scroll'          # Enable vertical scrolling\n    )\n)\n\n#button\nexecute_button = widgets.Button(\n    description='Execute query',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef process_query(b):\n    query = text_area.value\n    with query_result:\n        query_result.clear_output()\n        display(execute_query(query))\nexecute_button.on_click(process_query)\n\nanswer_button = widgets.Button(\n    description='Reveal answer key',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef show_answer_key(b):\n    query = text_area.value\n    with query_result:\n        query_result.clear_output()\n        print(f'Answer key is: \\n{answer_key}')\nanswer_button.on_click(show_answer_key)\n\nhint_button = widgets.Button(\n    description='Reveal hint',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef show_hint(b):\n    with query_result:\n        query_result.clear_output()\n        print(f'Hint: \\n{hint}')\nhint_button.on_click(show_hint)\n\ndisplay(text_area, execute_button, hint_button, answer_button, query_result)\n\n\n# @title Check if we have recommendation in `pred_mfp_tbl` for every user in `test_tbl`?\n\nanswer_key = f\"\"\"\nselect\n avg(case when b.user_id is not null then 1 else 0 end) as has_pred\nfrom (select distinct user_id from test_tbl) a\nleft join (select distinct user_id from pred_mfp_tbl) b\non a.user_id = b.user_id;\n\"\"\"\nhint = f\"\"\"\nleft join then count the nulls\n\"\"\"\n\n#input\ntext_area = widgets.Textarea(\n    value=\"Write your query here.\",\n    rows=10,  # Initial number of visible rows\n    description=\"Query:\",\n    layout={'width': '730px'} # Adjust width as needed\n)\n\n#output\nquery_result = query_result = widgets.Output(\n    layout=widgets.Layout(\n        border='1px solid lightgray', # Add a border to make the scrollable area visible\n        height='300px',              # Fixed height for the output area\n        overflow_y='scroll'          # Enable vertical scrolling\n    )\n)\n\n#button\nexecute_button = widgets.Button(\n    description='Execute query',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef process_query(b):\n    query = text_area.value\n    with query_result:\n        query_result.clear_output()\n        display(execute_query(query))\nexecute_button.on_click(process_query)\n\nanswer_button = widgets.Button(\n    description='Reveal answer key',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef show_answer_key(b):\n    query = text_area.value\n    with query_result:\n        query_result.clear_output()\n        print(f'Answer key is: \\n{answer_key}')\nanswer_button.on_click(show_answer_key)\n\nhint_button = widgets.Button(\n    description='Reveal hint',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef show_hint(b):\n    with query_result:\n        query_result.clear_output()\n        print(f'Hint: \\n{hint}')\nhint_button.on_click(show_hint)\n\ndisplay(text_area, execute_button, hint_button, answer_button, query_result)\n\n\n# @title For users who we do not have recommendations for in `pred_mfp_tbl`, give them recommendations from `pred_top10_tbl`. Create a new view for this called `pred_final_tbl`.\n\nanswer_key = f\"\"\"\ncreate or replace view pred_top10_tbl_missing_only as (\nselect\n a.user_id\n ,a.item_id\nfrom pred_top10_tbl a\nleft join pred_mfp_tbl b\non a.user_id = b.user_id\nwhere b.user_id is null);\n\ncreate or replace view pred_final_tbl as (\nselect user_id,item_id from pred_mfp_tbl\nunion all\nselect user_id,item_id from pred_top10_tbl_missing_only);\n\nselect 'pred' split, count(*) nb_pred, count(distinct user_id) nb_user from pred_final_tbl\nunion all\nselect 'test' split, count(*) nb_pred, count(distinct user_id) nb_user from test_tbl;\n\"\"\"\nhint = f\"\"\"\nOne way you can do this is:\n\n1) Find out who did not get recommendation in `pred_mfp_tbl`\n2) Filter `pred_top10_tbl` to have only those users\n3) Concatenate `pred_top10_tbl` containing the missing users and `pred_mfp_tbl` to create view `pred_final_tbl`. Make sure columns are the same when you concatenate.\n\"\"\"\n\n#input\ntext_area = widgets.Textarea(\n    value=\"Write your query here.\",\n    rows=10,  # Initial number of visible rows\n    description=\"Query:\",\n    layout={'width': '730px'} # Adjust width as needed\n)\n\n#output\nquery_result = query_result = widgets.Output(\n    layout=widgets.Layout(\n        border='1px solid lightgray', # Add a border to make the scrollable area visible\n        height='300px',              # Fixed height for the output area\n        overflow_y='scroll'          # Enable vertical scrolling\n    )\n)\n\n#button\nexecute_button = widgets.Button(\n    description='Execute query',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef process_query(b):\n    query = text_area.value\n    with query_result:\n        query_result.clear_output()\n        display(execute_query(query))\nexecute_button.on_click(process_query)\n\nanswer_button = widgets.Button(\n    description='Reveal answer key',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef show_answer_key(b):\n    query = text_area.value\n    with query_result:\n        query_result.clear_output()\n        print(f'Answer key is: \\n{answer_key}')\nanswer_button.on_click(show_answer_key)\n\nhint_button = widgets.Button(\n    description='Reveal hint',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef show_hint(b):\n    with query_result:\n        query_result.clear_output()\n        print(f'Hint: \\n{hint}')\nhint_button.on_click(show_hint)\n\ndisplay(text_area, execute_button, hint_button, answer_button, query_result)\n\n\n# @title Calculate top-10 hit rate for `pred_final_tbl` and see if it is better than `pred_top10_tbl`\n\nanswer_key = f\"\"\"\nselect\n 'pred_top10_tbl' model\n ,avg(case when nb_hit &gt; 0 then 1 else 0 end) as top_10_hit_rate\nfrom\n(select\n a.user_id\n ,sum(case when b.item_id is not null then 1 else 0 end) as nb_hit\nfrom eval_tbl a\nleft join pred_top10_tbl b\non a.user_id = b.user_id\n and a.item_id = b.item_id\ngroup by 1) c\n\nunion all\n\nselect\n 'pred_final_tbl' model\n ,avg(case when nb_hit &gt; 0 then 1 else 0 end) as top_10_hit_rate\nfrom\n(select\n a.user_id\n ,sum(case when b.item_id is not null then 1 else 0 end) as nb_hit\nfrom eval_tbl a\nleft join pred_final_tbl b\non a.user_id = b.user_id\n and a.item_id = b.item_id\ngroup by 1) c;\n\"\"\"\nhint = f\"\"\"\nDo the same thing as what you did for `pred_top10_tbl` then concatenate the result together.\n\"\"\"\n\n#input\ntext_area = widgets.Textarea(\n    value=\"Write your query here.\",\n    rows=10,  # Initial number of visible rows\n    description=\"Query:\",\n    layout={'width': '730px'} # Adjust width as needed\n)\n\n#output\nquery_result = query_result = widgets.Output(\n    layout=widgets.Layout(\n        border='1px solid lightgray', # Add a border to make the scrollable area visible\n        height='300px',              # Fixed height for the output area\n        overflow_y='scroll'          # Enable vertical scrolling\n    )\n)\n\n#button\nexecute_button = widgets.Button(\n    description='Execute query',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef process_query(b):\n    query = text_area.value\n    with query_result:\n        query_result.clear_output()\n        display(execute_query(query))\nexecute_button.on_click(process_query)\n\nanswer_button = widgets.Button(\n    description='Reveal answer key',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef show_answer_key(b):\n    query = text_area.value\n    with query_result:\n        query_result.clear_output()\n        print(f'Answer key is: \\n{answer_key}')\nanswer_button.on_click(show_answer_key)\n\nhint_button = widgets.Button(\n    description='Reveal hint',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef show_hint(b):\n    with query_result:\n        query_result.clear_output()\n        print(f'Hint: \\n{hint}')\nhint_button.on_click(show_hint)\n\ndisplay(text_area, execute_button, hint_button, answer_button, query_result)\n\nüéä Congratulations! You have made 21x improvement on the baseline to get almost 2% top-10 hit rate. This marks the end of this exercise but if you want to further improve our algorithm, feel free to do so with the console below.\n\n# @title Run anything\n\nanswer_key = f\"\"\"\nThere is no correcrt answer in life.\n\"\"\"\nhint = f\"\"\"\nYou just need to mess around and find out.\n\"\"\"\n\n#input\ntext_area = widgets.Textarea(\n    value=\"Write your query here.\",\n    rows=10,  # Initial number of visible rows\n    description=\"Query:\",\n    layout={'width': '730px'} # Adjust width as needed\n)\n\n#output\nquery_result = query_result = widgets.Output(\n    layout=widgets.Layout(\n        border='1px solid lightgray', # Add a border to make the scrollable area visible\n        height='300px',              # Fixed height for the output area\n        overflow_y='scroll'          # Enable vertical scrolling\n    )\n)\n\n#button\nexecute_button = widgets.Button(\n    description='Execute query',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef process_query(b):\n    query = text_area.value\n    with query_result:\n        query_result.clear_output()\n        display(execute_query(query))\nexecute_button.on_click(process_query)\n\nanswer_button = widgets.Button(\n    description='Reveal answer key',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef show_answer_key(b):\n    query = text_area.value\n    with query_result:\n        query_result.clear_output()\n        print(f'Answer key is: \\n{answer_key}')\nanswer_button.on_click(show_answer_key)\n\nhint_button = widgets.Button(\n    description='Reveal hint',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    icon='check' # (FontAwesome icons available)\n)\ndef show_hint(b):\n    with query_result:\n        query_result.clear_output()\n        print(f'Hint: \\n{hint}')\nhint_button.on_click(show_hint)\n\ndisplay(text_area, execute_button, hint_button, answer_button, query_result)"
  },
  {
    "objectID": "posts/aib2025_survey/index.html",
    "href": "posts/aib2025_survey/index.html",
    "title": "AI Builders Survey 2025: What Five Years of Teaching Kids to Build Good AI Looks Like",
    "section": "",
    "text": "AI Builders is a 10-week, online program that teaches middle-to-high school students in Thailand to create an ML project they can be proud of. Every year, we select 40-60 students from 500+ applicants to participate in our intensive project development regimen. We start off with an orientation session and ice-breaking in the first week. Then, students are divided into 10-12 groups of 4-6 people with a mentor and teaching assistant responsbile. Each week, they learn from an online lesson, including videos and notebooks, and apply those freshly learned skills to work on their capstone project. At 7pm every Wednesday, students attend a one-to-two-hour long session with their mentors and teaching assistants to consult on the progress of their projects. Repeat this for 8 weeks and in the last week we score them according to problem statement (15), metrics and baselines (15), data collection and cleaning (15), exploratory data analysis (20), modeling and error analysis (20), and deployment (15). Students need to score at least 70 points overall and not a single zero-scored criterion to pass.\nAs we reach our half-decade milestone, we would like to shed some lights on our students based on both our student roster (their performance during the program) and a survey distributed in July, 2025 (their performance after the program).\nCode\nimport logging\nlogging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n\nsurvey_fname = '/Users/charipol/Work/admin/data/student_survey/aib2025_student_survey.csv'\nroster_fname = '/Users/charipol/Work/admin/data/student_roster/aib2025_student_roster_master.csv'\n\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom scipy import stats\nfrom statsmodels.stats.power import NormalIndPower\nfrom plotnine import *\nfrom mizani.formatters import *\nfrom itertools import product\n\ndef proportion_z_test(x1, n1, x2, n2, effect_size_alt=None, alpha=0.05, alternative='two-sided'):\n    \"\"\"\n    Performs a Z-test for the difference between two population proportions\n    and calculates the statistical power of the test if an effect size is provided.\n\n    Args:\n        x1 (int): Number of positives (successes) in sample 1.\n        n1 (int): Total number of samples (trials) in group 1.\n        x2 (int): Number of positives (successes) in sample 2.\n        n2 (int): Total number of samples (trials) in group 2.\n        effect_size_alt (float, optional): Directly specifies the effect size (Cohen's h)\n                                           for power calculation. If None, power is not calculated.\n        alpha (float, optional): Significance level (Type I error rate) for power calculation.\n                                 Defaults to 0.05.\n        alternative (str, optional): Defines the alternative hypothesis for both the Z-test\n                                     and power calculation. Can be 'two-sided', 'less', or 'greater'.\n                                     Defaults to 'two-sided'.\n\n    Returns:\n        tuple: A tuple containing:\n            - z_statistic (float): The calculated Z-statistic.\n            - p_value (float): The p-value corresponding to the chosen alternative.\n            - power (float or None): The calculated statistical power. None if effect_size_alt\n                                     is not provided, or if calculation isn't possible.\n    \"\"\"\n\n    # Validate alternative argument\n    if alternative not in ['two-sided', 'less', 'greater']:\n        raise ValueError(\"alternative must be 'two-sided', 'less', or 'greater'\")\n\n    # Calculate sample proportions\n    p1_hat = x1 / n1\n    p2_hat = x2 / n2\n\n    # Calculate the pooled proportion\n    p_pooled = (x1 + x2) / (n1 + n2)\n\n    # Calculate the standard error of the difference\n    try:\n        if p_pooled == 0 or p_pooled == 1:\n            se_difference = 0.0\n        else:\n            se_difference = np.sqrt(p_pooled * (1 - p_pooled) * (1/n1 + 1/n2))\n    except ZeroDivisionError:\n        return np.nan, np.nan, np.nan\n\n    if se_difference == 0:\n        return np.nan, np.nan, np.nan\n\n    # Calculate the Z-statistic\n    z_statistic = (p1_hat - p2_hat) / se_difference\n\n    # Calculate the p-value based on the alternative hypothesis\n    if alternative == 'two-sided':\n        p_value = 2 * (1 - stats.norm.cdf(abs(z_statistic)))\n    elif alternative == 'greater':\n        p_value = 1 - stats.norm.cdf(z_statistic)\n    elif alternative == 'less':\n        p_value = stats.norm.cdf(z_statistic)\n\n    # Calculate power if an effect size is provided\n    power = None\n    if effect_size_alt is not None:\n        try:\n            # Use NormalIndPower for power calculation of Z-tests\n            power_calculator = NormalIndPower()\n            power = power_calculator.solve_power(\n                effect_size=effect_size_alt, # Directly use the provided effect_size_alt\n                nobs1=n1,\n                alpha=alpha,\n                power=None, # We want to solve for power\n                ratio=n2/n1, # Ratio of sample sizes (n2 / n1)\n                alternative=alternative # Use the chosen alternative for power calc\n            )\n        except Exception as e:\n            print(f\"Warning: Could not calculate power. Error: {e}\")\n            power = np.nan\n\n    return z_statistic, p_value, power\n\nroster_df = pd.read_csv(roster_fname)\nsurvey_df = pd.read_csv(survey_fname)"
  },
  {
    "objectID": "posts/aib2025_survey/index.html#what-we-learned-from-our-student-roster",
    "href": "posts/aib2025_survey/index.html#what-we-learned-from-our-student-roster",
    "title": "AI Builders Survey 2025: What Five Years of Teaching Kids to Build Good AI Looks Like",
    "section": "What We Learned from Our Student Roster",
    "text": "What We Learned from Our Student Roster\nFrom 2021 to 2025, AI Builders had 214 students attended the program, of which 170 (79.4%) have passed our criteria and successfully graduated.\n\nStudy Level\nWhen we started the official AI Builders in 2021, we expected high school students to be our target audience but a not-so-small portion (about 10%) of students turn out to be middle schoolers and below (our very first Judge‚Äôs Award was to GemmyTheGeek, a sixth grader at the time).\n\n\nCode\n#study level \nd = roster_df.study_level.value_counts(normalize=True).reset_index()\nd['study_level'] = d['study_level'].astype(int)\n\ng = (ggplot(d, aes(x='study_level', y='proportion'))\n     + geom_col(fill='darkred')\n    + geom_text(\n        aes(label=after_stat('y * 100')), # Label the actual percentage value\n        nudge_y=0.0001, # Adjust this value to move labels slightly above the bars\n        va='bottom',  # Vertical alignment: 'bottom' places text just above the 'y' coordinate\n        format_string='{:.1f}%' # Format to one decimal place and add '%'\n    )\n     + xlab('Secondary School Grade') + ylab('Percentage of Students') \n     + ggtitle('Secondary Grades When Starting Program')\n     + scale_y_continuous(labels=percent_format())\n     + scale_x_continuous(breaks = [i for i in range(1,7)])\n     + theme_xkcd()\n    )\ng\n\n\n\n\n\n\n\n\n\n\n\nLocation\nSpring of 2021 was an unusual time where physically organizing a summer school was out-of-question. We took a conscious decision to make AI Builders 100% online in order to accommodate students outside of Bangkok Metropolitan Area (Bangkok, Nonthaburi, Nakhon Pathom, Pathum Thani, Samut Prakarn, and Samut Sakorn). As of the 2019 census, Bangkok Metropolian Area is over six times larger than the second-largest city Nakhon Ratchasima. This primate city issue represents uneven distribution of resources especially in education.\n\n\nCode\n#bangkok vs non-bangkok\ngreater_bangkok = ['‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£','‡∏ô‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ','‡∏ô‡∏Ñ‡∏£‡∏õ‡∏ê‡∏°','‡∏õ‡∏ó‡∏∏‡∏°‡∏ò‡∏≤‡∏ô‡∏µ','‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£','‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏≤‡∏Ñ‡∏£']\nroster_df['province_grouped'] = roster_df.province.map(lambda x: '‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏Ø‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏¥‡∏°‡∏ì‡∏ë‡∏•' if x in greater_bangkok else x)\nroster_df['is_greater_bangkok'] = roster_df.province_grouped.map(lambda x: 1 if x=='‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏Ø‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏¥‡∏°‡∏ì‡∏ë‡∏•' else 0)\nd = roster_df.province_grouped.value_counts(normalize=True).reset_index()\n\n\nThe online format has allowed 38.3% of students to join from non-Bangkok provinces (see full list in Appendix), a feat that would not have been possible with offline format and our limited resources.\nHowever, we found a statistically significant lower pass rate for non-Bangkok students. This points towards room for growth in terms of supporting students outside of Bangkok Metropolitan Area.\n\n\nCode\n#discrepancies in pass rate by location\nd = roster_df.groupby('is_greater_bangkok').agg({\n    'student_name': ['count'],\n    'passed': ['sum'],\n}).reset_index()\nd.columns = ['is_greater_bangkok','nb_student','nb_passed']#,'pct_passed']\nd_m = d.melt(id_vars='is_greater_bangkok')\nd_m.columns = ['is_greater_bangkok','Pass/Total','value']\nd_m['is_greater_bangkok'] = d_m['is_greater_bangkok'].map(lambda x: 'Bangkok Metropolitan' if x==1 else 'Other Provinces')\nd_m['Pass/Total'] = d_m['Pass/Total'].map(lambda x: 'Passed' if x=='nb_passed' else 'Total')\np = roster_df.groupby('is_greater_bangkok').passed.mean().reset_index()\np['is_greater_bangkok'] = p['is_greater_bangkok'].map(lambda x: 'Bangkok Metropolitan' if x==1 else 'Other Provinces')\np.columns = ['is_greater_bangkok','pass_rate']\nd_m_merged = pd.merge(d_m, p, on='is_greater_bangkok', how='left')\n\n#hypothesis test for pass rate among bangkok vs non-bangkok; lacking quite a bit of power\nz,p_value,power = proportion_z_test(x1=64,n1=89,x2=107,n2=125,\n                  effect_size_alt=0.1,\n                  alpha=0.05, \n                  alternative='two-sided')\nz,p_value,power\n\ng = (ggplot(d_m_merged, aes(x='is_greater_bangkok', y='value', fill='Pass/Total'))\n     + geom_col(position='dodge')\n    + geom_text(\n        d_m_merged[d_m_merged['Pass/Total'] == 'Passed'],\n        aes(x='is_greater_bangkok', y='value', label='pass_rate * 100'),\n        nudge_y=1,\n        va='bottom', \n        format_string='{:.1f}%',)\n     + xlab('Location') + ylab('Number of Students')\n     + ggtitle(f'Students from Bangkok Metro Have Higher Pass Rate \\n p-value = {p_value:.3f}; power = {100*power:.1f}%')\n     + scale_fill_manual(values={'Passed':'darkred', 'Total':'grey'})\n     + theme_xkcd()\n    )\ng\n\n\n\n\n\n\n\n\n\n\n\nBiological Sex\n\n\nCode\n#female %\nroster_df['is_biological_female'] = roster_df.biological_sex.map(lambda x: 1 if x=='f' else 0)\nd = roster_df.groupby('class_of').agg({\n    'student_name': ['count'],\n    'is_biological_female': ['sum','mean']\n}).reset_index()\nd.columns = ['class_of','nb_student','nb_f','pct_f']\n\n\nSimilar to overall trends in STEM, biologically female students are a minority in our program at 22.4%. Nonetheless, we see a clear upward trend in participation over the last 5 years.\n\n\nCode\ng = (ggplot(d, aes(x='class_of', y='pct_f'))\n     +geom_point()\n    + geom_text(\n        aes(label='pct_f'), # Map the 'percentage_label' column to the label aesthetic\n        format_string='{:.1%}',         # Format it as a percentage with one decimal place\n        nudge_y=0.02,                   # Nudge labels slightly above points (adjust as needed)\n        va='bottom',                    # Vertical alignment to place text above the point                 # Adjust font size\n        show_legend=False               # Often you don't need a legend for these labels\n    )\n     + geom_smooth(method='glm', se=True, color='darkred', linetype='dashed')\n     + scale_y_continuous(labels=percent_format())\n     + xlab('Class of') + ylab('% Biological Female') + ggtitle('Biological Female Students on The Rise')\n     + theme_xkcd()\n     )\ng\n\n\n\n\n\n\n\n\n\nMoreover, unlike the Bangkok-vs-non-Bangkok situation, there is no statistically significant difference in pass rates among biologically male and female students. Once we get them into the program, they perform equally. The challenge is how to get more biologically female students into it.\n\n\nCode\n#discrepancies in pass rate by location\nd = roster_df.groupby('is_biological_female').agg({\n    'student_name': ['count'],\n    'passed': ['sum'],\n}).reset_index()\nd.columns = ['is_biological_female','nb_student','nb_passed']#,'pct_passed']\nd_m = d.melt(id_vars='is_biological_female')\nd_m.columns = ['is_biological_female','Pass/Total','value']\nd_m['is_biological_female'] = d_m['is_biological_female'].map(lambda x: 'F' if x==1 else 'M')\nd_m['Pass/Total'] = d_m['Pass/Total'].map(lambda x: 'Passed' if x=='nb_passed' else 'Total')\np = roster_df.groupby('is_biological_female').passed.mean().reset_index()\np['is_biological_female'] = p['is_biological_female'].map(lambda x: 'F' if x==1 else 'M')\np.columns = ['is_biological_female','pass_rate']\nd_m_merged = pd.merge(d_m, p, on='is_biological_female', how='left')\n\n#hypothesis test for pass rate among sexes; lacking quite a bit of power\nz,p_value,power = proportion_z_test(x1=134,n1=166,x2=37,n2=48,\n                  effect_size_alt=0.1,\n                  alpha=0.05, \n                  alternative='two-sided')\n\ng = (ggplot(d_m_merged, aes(x='is_biological_female', y='value', fill='Pass/Total'))\n     + geom_col(position='dodge')\n    + geom_text(\n        d_m_merged[d_m_merged['Pass/Total'] == 'Passed'],\n        aes(x='is_biological_female', y='value', label='pass_rate * 100'),\n        nudge_y=1,\n        va='bottom', \n        format_string='{:.1f}%',)\n     + xlab('Biological Sex') + ylab('Number of Students')\n     + ggtitle(f'No Pass Rate Difference between Biological Sexes \\n p-value = {p_value:.3f}; power = {100*power:.1f}%')\n     + scale_fill_manual(values={'Passed':'darkred', 'Total':'grey'})\n     + theme_xkcd()\n    )\ng\n\n\n\n\n\n\n\n\n\n\n\nProject Themes and Data Types\nLet us look into what students like to build the most. Top three most popular combinations are medical imaging, literature mostly in Thai, and music. Other archetypes worth mentioning are hand sign projects (under acessibility/images), many types of secruity projects from fraud detection to hacking, education, and entertainment. These seem to be topics closest to the hearts and minds of middle-to-high schoolers.\n\n\nCode\nall_combination = list(product(roster_df.theme.unique(), roster_df.main_data_type.unique()))\ncombined_df = pd.DataFrame(all_combination, columns=['theme', 'main_data_type'])\ncombination_with_count = roster_df.groupby(['theme', 'main_data_type']).size().reset_index(name='count')\ncombined_df = combined_df[(~combined_df.theme.isna())&(~combined_df.main_data_type.isna())]\nheatmap_data = combined_df.merge(combination_with_count, on=['theme', 'main_data_type'], how='left')\nheatmap_data['count'] = heatmap_data['count'].fillna(0).astype(int)\n\ng = (\n    ggplot(heatmap_data, aes(x='theme', y='main_data_type', fill='count'))\n    + geom_tile(aes(width=0.9, height=0.9), color=\"white\") # Add white borders for separation# Thickness of the line\n    + geom_text(aes(label='count'), color='black', size=10) # Add count numbers\n    + scale_fill_gradient(low='lightyellow', high='darkred') # Choose a color scale\n    + labs(\n        title='Project Themes vs Main Data Types',\n        x='Themes',\n        y='Main Data Types',\n        fill='Count'\n    )\n    + theme_xkcd()\n    + theme(\n        axis_text_x=element_text(angle=45, hjust=1), # Rotate x-axis labels if they overlap\n        panel_background=element_text(fill='white'), # Set background to white\n        plot_background=element_text(fill='white')\n    )\n)\n\ng\n\n\n\n\n\n\n\n\n\nStrong interests in healthcare could stem from the cultural expectation that well-performing students would go on to medical school (more on this later). Considering the popularity of music projects despite our curriculumn not containing an audio track, we will create one starting in 2026.\n\n\nCode\nd = roster_df.theme.value_counts().reset_index()\\\n    .sort_values('count',ascending=False).reset_index()\nd['theme'] = pd.Categorical(d['theme'], categories=d['theme'], ordered=True)\n\ng = (ggplot(d, aes(x='theme', y='count',label='count'))\n     + geom_col(fill='darkred')\n    + geom_text(\n        nudge_y=0.0001, # Adjust this value to move labels slightly above the bars\n        va='bottom',  # Vertical alignment: 'bottom' places text just above the 'y' coordinate\n    )\n     + xlab('Project Themes') + ylab('Number of Projects') \n     + ggtitle('Project Theme Ranking')\n     + theme_xkcd()\n     + theme(axis_text_x=element_text(angle=45, hjust=1))\n     + scale_y_continuous(limits=(0,32))\n    )\ng\n\n\n\n\n\n\n\n\n\nImages being the most prevalent data type is understandable since they are the most beginner-friendly when it comes to deep learning; a well-versed student can spin up an image classification project end-to-end in a few hours while it would probably take a few weeks for them to do the same for a reinforcement learning simulation. Thai being a medium-resource language and the fact that many of our mentors work on PyThaiNLP, the largest Thai language processing library, might also have inspired many Thai NLP projects.\n\n\nCode\nd = roster_df.main_data_type.value_counts().reset_index()\\\n    .sort_values('count',ascending=False).reset_index()\nd['main_data_type'] = pd.Categorical(d['main_data_type'], categories=d['main_data_type'], ordered=True)\n\ng = (ggplot(d, aes(x='main_data_type', y='count',label='count'))\n     + geom_col(fill='darkred')\n    + geom_text(\n        nudge_y=0.0001, # Adjust this value to move labels slightly above the bars\n        va='bottom',  # Vertical alignment: 'bottom' places text just above the 'y' coordinate\n    )\n     + xlab('Project Data Types') + ylab('Number of Projects') \n     + ggtitle('Project Data Type Ranking')\n     + theme_xkcd()\n     + theme(axis_text_x=element_text(angle=45, hjust=1))\n     + scale_y_continuous(limits=(0,91))\n    )\ng"
  },
  {
    "objectID": "posts/aib2025_survey/index.html#what-students-do-after-the-program",
    "href": "posts/aib2025_survey/index.html#what-students-do-after-the-program",
    "title": "AI Builders Survey 2025: What Five Years of Teaching Kids to Build Good AI Looks Like",
    "section": "What Students Do After the Program",
    "text": "What Students Do After the Program\n\n\nCode\n#standardize columns\nsurvey_df['biological_sex'] = survey_df.gender.map(lambda x: 'f' if x=='‡∏´‡∏ç‡∏¥‡∏á' else 'm')\nsurvey_df['study_level'] = survey_df.secondary_grade\n\nsurvey_no_latest = survey_df[survey_df.class_of!=2025].reset_index(drop=True)\n\n\nThis July right after AI Builders 2025 ended, we sent out a survey to all our students from 2021 to 2025 (technically also to a few students we prototyped the project on in 2020). We heard back from 72 out of 214 students (33.6%). Here are what they have to say about their lives after AI Builders.\n\nDisclaimer: It goes without saying that there is self selection among those who responded to the survey. The result might seem ‚Äútoo good to be true‚Äù but we argue that even if we assume the remaining 66.4% responded in the worst way possible, our result would still look decent.\n\n\nWho Answered the Survey\nUnderstandably, response rate drops off for students from earlier class-ofs. From 70% in the current year to 30% for students 1-2 years prior and 15% for those earlier. This 15% is most likely students who have become teaching assistants in subsequent programs and kept contact in our Discord server. From 2022 onwards, almost all of our teaching assistants were program alumni.\n\n\nCode\nd = survey_df.class_of.value_counts(normalize=False).reset_index()\nd_roster = roster_df.class_of.value_counts(normalize=False).reset_index()\nd_combined = d.merge(d_roster, on='class_of',how='left').dropna()\nd_combined['response_rate'] = d_combined.count_x/d_combined.count_y\n\ng = (ggplot(d_combined, aes(x='class_of', y='response_rate'))\n     +geom_point(color='darkred')\n     +geom_line(color='darkred')\n    + geom_text(\n        aes(label='response_rate'), # Map the 'percentage_label' column to the label aesthetic\n        format_string='{:.1%}',         # Format it as a percentage with one decimal place\n        nudge_y=0.02,                   # Nudge labels slightly above points (adjust as needed)\n        va='bottom',                    # Vertical alignment to place text above the point                 # Adjust font size\n        show_legend=False               # Often you don't need a legend for these labels\n    )\n     + scale_y_continuous(labels=percent_format())\n     + xlab('Class of') + ylab('Response Rate') + ggtitle('Response Rate by Class-of')\n     + theme_xkcd()\n     )\ng\n\n\n\n\n\n\n\n\n\nWe found that respondents are similarly distributed in terms of biological sexes to the student roster. However, we have a bit of a skew towards secondary grade 6 in the survey.\n\n\nCode\nd_roster = roster_df.biological_sex.value_counts(normalize=True).reset_index()\nd_roster['source'] = 'roster'\nd_survey = survey_df.biological_sex.value_counts(normalize=True).reset_index()\nd_survey['source'] = 'survey'\nd = pd.concat([d_roster,d_survey],axis=0)\nd['biological_sex'] = pd.Categorical(d['biological_sex'], categories=['m','f'], ordered=True)\n\ng = (ggplot(d, aes(x='source', y='proportion', fill='biological_sex'))\n    + geom_col()\n    + geom_text(\n        d[d.biological_sex=='f'],\n        aes(x='source', y='proportion', label='proportion * 100'),\n        nudge_y=0.01,\n        va='bottom', \n        format_string='{:.1f}%',)\n     + xlab('Data Source') + ylab('Percentage of Students/Survey Respondents')\n     + ggtitle(f'Biological Sex Distribution of Student Roster vs Survey')\n     + scale_y_continuous(labels=percent_format())\n     + scale_fill_manual(values={'f':'darkred', 'm':'darkblue'})\n     + theme_xkcd()\n    )\ng\n\n\n\n\n\n\n\n\n\n\n\nCode\nd_roster = roster_df.study_level.value_counts(normalize=True).reset_index()\nd_roster['source'] = 'roster'\nd_survey = survey_df.study_level.value_counts(normalize=True).reset_index()\nd_survey['source'] = 'survey'\nd = pd.concat([d_roster,d_survey],axis=0)\nd['study_level'] = pd.Categorical(d['study_level'], categories=[1,2,3,4,5,6], ordered=True)\n\ng = (ggplot(d, aes(x='source', y='proportion', fill='study_level'))\n    + geom_col()\n     + xlab('Data Source') + ylab('Percentage of Students')\n     + ggtitle(f'Study Grades When Starting Program\\nStudent Roster vs Survey')\n     + scale_y_continuous(labels=percent_format())\n     + theme_xkcd()\n    )\ng\n\n\n\n\n\n\n\n\n\n\n\nUniversity\nUnsurprisingly, a large portion of our alumni is still pre-college. However, for those who are in university, a significant portion is in computer science and engineering and almost all of them are in STEM. Anecdotally, we have heard from students that participating in AI Builders changed their educational choice towards AI-related majors.\n\n\nCode\nd = survey_df.major.fillna('Still Not in University').value_counts().reset_index()\n\nd['major'] = pd.Categorical(d['major'], categories=d['major'], ordered=True)\n\ng = (ggplot(d, aes(x='major', y='count',label='count'))\n     + geom_col(fill='darkred')\n    + geom_text(\n        nudge_y=0.0001, # Adjust this value to move labels slightly above the bars\n        va='bottom',  # Vertical alignment: 'bottom' places text just above the 'y' coordinate\n    )\n     + xlab('University Major') + ylab('Number of Students') \n     + ggtitle('What Our Students Major in at University')\n     + theme_xkcd()\n     + theme(axis_text_x=element_text(angle=45, hjust=1))\n     + scale_y_continuous(limits=(0,30))\n    )\ng\n\n\n\n\n\n\n\n\n\nTop university choices for our students are domestic ones strong in STEM such as Chulalongkorn and KMUTT. Surprisingly, 10 out of 72 respondents pursued their higher education overseas.\n\n\nCode\nd = survey_df.university.fillna('Still Not in University').value_counts().reset_index()\n\nd['university'] = pd.Categorical(d['university'], categories=d['university'], ordered=True)\n\ng = (ggplot(d, aes(x='university', y='count',label='count'))\n     + geom_col(fill='darkred')\n    + geom_text(\n        nudge_y=0.0001, # Adjust this value to move labels slightly above the bars\n        va='bottom',  # Vertical alignment: 'bottom' places text just above the 'y' coordinate\n    )\n     + xlab('University') + ylab('Number of Students') \n     + ggtitle('Where Students Go to University')\n     + theme_xkcd()\n     + theme(axis_text_x=element_text(angle=45, hjust=1))\n     + scale_y_continuous(limits=(0,31))\n    )\ng\n\n\n\n\n\n\n\n\n\n\n\nDo Students Think We Upgraded Them\nWe asked students if they think participating in AI Builders have improved their AI skills and expanded their networks of likeminded people, on a scale of 1-5 (1 being the worst and 5 being the best). Luckily, we seems to be thought of as effective, especially for skill improvement where all the scores are 3 and above.\n\n\nCode\ncombi = pd.DataFrame({'skill_upgrade':[1,2,3,4,5]})\nd = combi.merge(survey_df.skill_upgrade.value_counts(normalize=True).reset_index(), on='skill_upgrade',how='left').fillna(0)\n\n# d['skill_upgrade'] = pd.Categorical(d['skill_upgrade'], categories=d['skill_upgrade'], ordered=True)\n\ng = (ggplot(d, aes(x='skill_upgrade', y='proportion',label='proportion'))\n     + geom_col(fill='darkred')\n    + geom_text(\n        nudge_y=0.0001, # Adjust this value to move labels slightly above the bars\n        va='bottom',  # Vertical alignment: 'bottom' places text just above the 'y' coordinate\n        format_string='{:.1%}',  \n    )\n     + xlab('1: Worse, 5: Best') + ylab('Percentage of Students') \n     + ggtitle('Have AI Builders Helped You Improve Your AI Skills?')\n     + theme_xkcd()\n     + theme(axis_text_x=element_text(angle=45, hjust=1))\n     + scale_y_continuous(limits=(0,.6),labels=percent_format())\n    )\ng\n\n\n\n\n\n\n\n\n\n\n\nCode\ncombi = pd.DataFrame({'network_upgrade':[1,2,3,4,5]})\nd = combi.merge(survey_df.network_upgrade.value_counts(normalize=True).reset_index(), on='network_upgrade',how='left').fillna(0)\n\ng = (ggplot(d, aes(x='network_upgrade', y='proportion',label='proportion'))\n     + geom_col(fill='darkred')\n    + geom_text(\n        nudge_y=0.0001, # Adjust this value to move labels slightly above the bars\n        va='bottom',  # Vertical alignment: 'bottom' places text just above the 'y' coordinate\n        format_string='{:.1%}',  \n    )\n     + xlab('1: Worse, 5: Best') + ylab('Percentage of Students') \n     + ggtitle('Have AI Builders Broadened Your Network?')\n     + theme_xkcd()\n     + theme(axis_text_x=element_text(angle=45, hjust=1))\n     + scale_y_continuous(limits=(0,.6),labels=percent_format())\n    )\ng\n\n\n\n\n\n\n\n\n\n\n\nActivities Student Engage in After the Program\nWe asked about activities they have taken part after obtaining the skills in AI Builders in the following domains: school activity, hackathon, project competition, personal project, research, internship and work. Fortunately, most of the students have used what they learn to at least work on a personal project, which has always been our intention. About half of them have used it for events available to the middle-to-high school age group. And some even leapfrogged a bit to internship and work.\n\n\nCode\nusage_dict = {\n    '‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡πÉ‡∏ô‡πÇ‡∏£‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô': 'School Activity',  \n    '‡∏á‡∏≤‡∏ô‡πÅ‡∏Ç‡πà‡∏á Hackathon': 'Hackathon',\n    '‡∏õ‡∏£‡∏∞‡∏Å‡∏ß‡∏î‡πÇ‡∏Ñ‡∏£‡∏á‡∏á‡∏≤‡∏ô': 'Project Competition', \n    '‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏ï‡πå‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß': 'Personal Project',\n    '‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢': 'Research',  \n    '‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏á‡∏≤‡∏ô': 'Internship',  \n    '‡∏ó‡∏≥‡∏á‡∏≤‡∏ô': 'Work',  \n}\n\nfor val in usage_dict.keys():\n    survey_df[usage_dict[val]] = survey_df.usage.fillna('unknown').map(lambda x: 1 if (val in x) else 0)\n\nusage_flags = [usage_dict[k] for k in usage_dict.keys()]\nd = survey_df[usage_flags].mean().reset_index().sort_values(0, ascending=False)\nd.columns = ['activity','proportion']\nd['activity'] = pd.Categorical(d['activity'], categories=d['activity'], ordered=True)\n\ng = (ggplot(d, aes(x='activity', y='proportion',label='proportion'))\n     + geom_col(fill='darkred')\n    + geom_text(\n        nudge_y=0.0001, # Adjust this value to move labels slightly above the bars\n        va='bottom',  # Vertical alignment: 'bottom' places text just above the 'y' coordinate\n        format_string='{:.1%}',  \n    )\n     + xlab('After-Program Activity') + ylab('Percentage of Students') \n     + ggtitle('How Have You Applied What You Learned from AI Builders?')\n     + theme_xkcd()\n     + theme(axis_text_x=element_text(angle=45, hjust=1))\n     + scale_y_continuous(limits=(0,.9),labels=percent_format())\n    )\ng\n\n\n\n\n\n\n\n\n\nLooking a bit deeper, we can see that the median AI Builder engaged in about 3 after-program activity categories whereas some small percentage of seemingly superhuman alumni engage in all 7 categories.\n\n\nCode\nsurvey_df['nb_activity'] = survey_df[usage_flags].sum(axis=1)\nd = survey_df['nb_activity'].value_counts(normalize=True).reset_index()\n\ng = (ggplot(d, aes(x='nb_activity', y='proportion',label='proportion'))\n     + geom_col(fill='darkred')\n    + geom_text(\n        nudge_y=0.0001, # Adjust this value to move labels slightly above the bars\n        va='bottom',  # Vertical alignment: 'bottom' places text just above the 'y' coordinate\n        format_string='{:.1%}',  \n    )\n     + xlab('Number of After-Program Activities') + ylab('Percentage of Students') \n     + ggtitle('How Many Activities Have You Applied What\\nYou Learned from AI Builders to?')\n     + theme_xkcd()\n     + scale_y_continuous(limits=(0,.27),labels=percent_format())\n     + scale_x_continuous(breaks=[i for i in range(8)],)\n    )\ng\n\n\n\n\n\n\n\n\n\n\n\nMore on After-Program Activities\nWe had a few free-text questions for students to tell us about the details of their after-program activities in events, research and knowledge sharing. Here is a sampler of their achievements at domestic and international levels, which we hope we were at least a tiny part of:\n\nAfter-Program Events\nInternational Achievements:\n‚Ä¢ Silver Medal at Genius Olympiad 2024\n‚Ä¢ Breakthrough Pioneer Award at International Student Science Fair\n‚Ä¢ Finalist at nwHack (University of British Columbia)\n‚Ä¢ Excellence Award at Shanghai AI Global Innovation Competition\nNational Competitions:\n‚Ä¢ Multiple wins at Siriraj Hackathon (2022-2024)\n‚Ä¢ Gold Medal in Super AI Engineer\n‚Ä¢ First Prize in Computer Software Projects Competition (Sillapa70)\n‚Ä¢ Multiple awards in National Software Contest (NSC)\n‚Ä¢ Success in APICTA and TICTA competitions\nHackathons:\n‚Ä¢ Champion at SAS x CBS Hackathon 2023\n‚Ä¢ 2nd Runner-up at AIhack Thailand 2023\n‚Ä¢ Finalist positions in multiple events:\n\nBDI Hackathon\nSEA-Qthon 2025\nRehack Search 2024\nSPU AI Prompt Mini Hackathon\n\nProfessional Development:\n‚Ä¢ Multiple internships at Agoda (2023-2025)\n‚Ä¢ Internships at Provincial Electricity Authority\n‚Ä¢ Internship at Ramathibodi Hospital\nTechnical Projects & Research:\n‚Ä¢ Healthcare: EEG seizure detection, PPG vital sign monitoring\n‚Ä¢ Agriculture: Durian tree health analysis system\n‚Ä¢ AI/ML: Various projects involving computer vision, NLP, and machine learning\n\n\nAfter-Program Research Works\nPublications & Presentations:\n‚Ä¢ Published and presented at JCSSE2023, receiving Best Presentation Award\n‚Ä¢ Selected for poster presentation at EMBC 2025 in Denmark\n‚Ä¢ Authored paper on graph embedding methods for drug discovery\nAcademic Institutions:\n‚Ä¢ Research Assistant positions at:\n\nKasetsart University (Document Analysis)\nVISTEC (Multiple EEG and PPG processing projects)\nBrown University\nUniversity of Wisconsin-Madison (MRI processing)\nBeijing Institute of Technology (Agent and Embodied Agent research)\n\nMedical Research:\n‚Ä¢ Siriraj Hospital:\n\nCardiac Center research collaboration\nVISTEC joint research project\nMr.Rehab medical consultant\n\nIndustry Collaboration:\n‚Ä¢ SCBX - AI Researcher Intern\n‚Ä¢ American Family Insurance - DS/ML Intern\n‚Ä¢ AI Singapore - Student Assistance (Quality Assurance)\nSpecialized Projects:\n‚Ä¢ Virtual Power Plant AI implementation with Walailak University\n‚Ä¢ Disaster Management System Development at Nakhon Ratchasima Rajabhat University\n‚Ä¢ Individual Study projects at Chulalongkorn University\nNotable Scholarships:\n‚Ä¢ JSTP Scholarship 2024 Researcher\n‚Ä¢ ‡∏û‡∏™‡∏ß‡∏ó. (DPST) Scholar\n\n\nAfter-Program Knowledge Sharing\nUniversity Level Involvement:\n‚Ä¢ Teaching Assistant positions at:\n\nBrown University (CSCI0410)\nKasetsart University (Machine Learning Systems)\nVarious Computer Programming courses\n\nSecondary Education Impact:\n‚Ä¢ Organized and taught at multiple high school programs:\n\nComcamp36 (Data Science Fundamentals)\nNSTDA‚Äôs RAC Program (AI & Data Science Camp)\nRobotics workshops\nComputer Olympics training\n\nCommunity Initiatives:\n‚Ä¢ Founded and led technical clubs:\n\nInformation System Development club at Chulalongkorn University\nKU Tech club activities\nCU Dev Club workshops\n\nDigital Content Creation:\n‚Ä¢ Created educational content:\n\nYouTube channel for AI programming using Kittenblock\nE-book: ‚ÄúAI & Machine Learning for Kids‚Äù\n\nEvent Organization:\n‚Ä¢ Hackathons and Workshops:\n\nSI Hackathon 2023\nI-Squared Hackathon (Data Science and ML)\nChAMP Data Career Track\nMachine Learning in Computer Vision workshop (40 participants)\n\nTechnical Projects:\n‚Ä¢ Developed backend systems:\n\nRegistration and event management APIs\nPet adoption website backend\nVarious student-focused technical solutions\n\nMentorship:\n‚Ä¢ Provided guidance for:\n\nTechnology club members\nProject competition participants\nCareer preparation in data science fields\nSchool technology initiatives"
  },
  {
    "objectID": "posts/aib2025_survey/index.html#closing-remarks",
    "href": "posts/aib2025_survey/index.html#closing-remarks",
    "title": "AI Builders Survey 2025: What Five Years of Teaching Kids to Build Good AI Looks Like",
    "section": "Closing Remarks",
    "text": "Closing Remarks\nBy cross-tabulating the roster and survey, we have reaffirmed the effectiveness of our project-based learning approach, the advantage of the all-online format, and the growing community based on no-nonsense, impact-driven buildership. Looking forward, we will tackle our gap in lession (audio track), support for non-Bangkok students, and pipeline intake for biologically female students.\nIf you are an organization or a person that share our vision for building, please feel free to reach out for any kind of sponsorship, code contribution, mentorship, and moral support.\nNo influencers. No scammers. Just builders."
  },
  {
    "objectID": "posts/aib2025_survey/index.html#appendiex",
    "href": "posts/aib2025_survey/index.html#appendiex",
    "title": "AI Builders Survey 2025: What Five Years of Teaching Kids to Build Good AI Looks Like",
    "section": "Appendiex",
    "text": "Appendiex\n\nStudent Location\n\n\nCode\n#bangkok vs non-bangkok\ngreater_bangkok = ['‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏°‡∏´‡∏≤‡∏ô‡∏Ñ‡∏£','‡∏ô‡∏ô‡∏ó‡∏ö‡∏∏‡∏£‡∏µ','‡∏ô‡∏Ñ‡∏£‡∏õ‡∏ê‡∏°','‡∏õ‡∏ó‡∏∏‡∏°‡∏ò‡∏≤‡∏ô‡∏µ','‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏≤‡∏£','‡∏™‡∏°‡∏∏‡∏ó‡∏£‡∏™‡∏≤‡∏Ñ‡∏£']\nroster_df['province_grouped'] = roster_df.province.map(lambda x: '‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏Ø‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏¥‡∏°‡∏ì‡∏ë‡∏•' if x in greater_bangkok else x)\nroster_df['is_greater_bangkok'] = roster_df.province_grouped.map(lambda x: 1 if x=='‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏Ø‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏¥‡∏°‡∏ì‡∏ë‡∏•' else 0)\nd = roster_df.province_grouped.value_counts(normalize=True).reset_index()\nd\n\n\n\n\n\n\n\n\n\nprovince_grouped\nproportion\n\n\n\n\n0\n‡∏Å‡∏£‡∏∏‡∏á‡πÄ‡∏ó‡∏û‡∏Ø‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏¥‡∏°‡∏ì‡∏ë‡∏•\n0.616822\n\n\n1\n‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà\n0.098131\n\n\n2\n‡∏™‡∏á‡∏Ç‡∏•‡∏≤\n0.051402\n\n\n3\n‡∏ô‡∏Ñ‡∏£‡∏®‡∏£‡∏µ‡∏ò‡∏£‡∏£‡∏°‡∏£‡∏≤‡∏ä\n0.023364\n\n\n4\n‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡∏£‡∏≤‡∏¢\n0.023364\n\n\n5\n‡∏™‡∏£‡∏∞‡∏ö‡∏∏‡∏£‡∏µ\n0.023364\n\n\n6\n‡∏ä‡∏•‡∏ö‡∏∏‡∏£‡∏µ\n0.023364\n\n\n7\n‡∏ö‡∏∏‡∏£‡∏µ‡∏£‡∏±‡∏°‡∏¢‡πå\n0.014019\n\n\n8\n‡∏ï‡∏£‡∏±‡∏á\n0.009346\n\n\n9\n‡∏û‡∏¥‡∏©‡∏ì‡∏∏‡πÇ‡∏•‡∏Å\n0.009346\n\n\n10\n‡∏™‡∏∏‡∏£‡∏≤‡∏©‡∏é‡∏£‡πå‡∏ò‡∏≤‡∏ô‡∏µ\n0.009346\n\n\n11\n‡∏Ç‡∏≠‡∏ô‡πÅ‡∏Å‡πà‡∏ô\n0.009346\n\n\n12\n‡∏†‡∏π‡πÄ‡∏Å‡πá‡∏ï\n0.009346\n\n\n13\n‡∏£‡∏∞‡∏¢‡∏≠‡∏á\n0.009346\n\n\n14\n‡∏™‡∏ï‡∏π‡∏•\n0.009346\n\n\n15\n‡∏â‡∏∞‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏ó‡∏£‡∏≤\n0.009346\n\n\n16\n‡∏°‡∏´‡∏≤‡∏™‡∏≤‡∏£‡∏Ñ‡∏≤‡∏°\n0.004673\n\n\n17\n‡πÄ‡∏û‡∏ä‡∏£‡∏ö‡∏π‡∏£‡∏ì‡πå\n0.004673\n\n\n18\n‡∏ô‡∏Ñ‡∏£‡∏£‡∏≤‡∏ä‡∏™‡∏µ‡∏°‡∏≤\n0.004673\n\n\n19\n‡∏Å‡∏≥‡πÅ‡∏û‡∏á‡πÄ‡∏û‡∏ä‡∏£\n0.004673\n\n\n20\n‡πÄ‡∏•‡∏¢\n0.004673\n\n\n21\n‡∏°‡∏∏‡∏Å‡∏î‡∏≤‡∏´‡∏≤‡∏£\n0.004673\n\n\n22\n‡∏ï‡∏≤‡∏Å\n0.004673\n\n\n23\n‡πÄ‡∏û‡∏ä‡∏£‡∏ö‡∏∏‡∏£‡∏µ\n0.004673\n\n\n24\n‡∏®‡∏£‡∏µ‡∏™‡∏∞‡πÄ‡∏Å‡∏©\n0.004673\n\n\n25\n‡∏•‡∏≥‡∏õ‡∏≤‡∏á\n0.004673\n\n\n26\n‡∏•‡∏û‡∏ö‡∏∏‡∏£‡∏µ\n0.004673"
  },
  {
    "objectID": "posts/sql_almost/index.html",
    "href": "posts/sql_almost/index.html",
    "title": "(Almost) All SQL You Need",
    "section": "",
    "text": "Skill mastery is an endeavor of diminishing returns. You can get from zero to good enough (think first-time chess player to 500 online rating) within 100 hours of deliberate practice; however, it is exponentially more difficult to get from good enough to mastered (think a 500-rated chess player trying to become a 2200-FIDE-rated Candidate Master), requiring over 100x more delibrate practice. SQL is the basic building blocks of data analysis and I posit that most people„Éºproduct managers, marketers, salespeople, and other makeshift data analysts„Éºsimply need to be good enough.\nThis tutorial aims to encapsulate almost all SQL techniques you need to glean actionable insights from your (non-transactional) datasets. We will use the Taobao User Behavior dataset and duckdb to simulate a SQL interface. There will be idiosyncracies according to which flavors of SQL you are using„ÉºPostgres, Presto, Redshift, BigQuery, and so on„Éºbut you should be able to adapt the principles outlined here with a little help of modern coding assistants.\nI recommend that you first quickly skim through this post to have a rough idea of what SQL is all about, then move on to complete the exercise for some hands-on shenanigans. While completing it, feel free to refer back to this post and read some sections more in details. The road to becoming a true SQL monkey starts with writing queries.\nCode\nimport duckdb\nimport pandas as pd\npd.set_option('display.max_rows', 6) \nimport numpy as np\nimport random\nfrom tqdm.auto import tqdm\nimport os\nimport timeit\n\n\ndf = pd.read_csv('../../data/sql_almost/UserBehavior.csv',\n      header=None)\ndf.columns = [\n  'user_id',\n  'item_id',\n  'category_id',\n  'behavior_type',\n  'timestamp']\n\n#sample 5M rows out of 100M to run in reasonable time\ndf = df.sample(n=5_000_000, random_state=112).reset_index(drop=True)\n\n# # save to parquet\n# con = duckdb.connect()\n# con.register('transaction_tbl', df)\n# output_dir = '../../data/sql_almost/transaction_tbl'\n# os.makedirs(output_dir, exist_ok=True)\n\n# con.execute(f\"\"\"\n# COPY transaction_tbl\n# TO '{output_dir}'\n# (FORMAT PARQUET, PARTITION_BY (category_id, behavior_type), OVERWRITE_OR_IGNORE);\n# \"\"\")"
  },
  {
    "objectID": "posts/sql_almost/index.html#intuition",
    "href": "posts/sql_almost/index.html#intuition",
    "title": "(Almost) All SQL You Need",
    "section": "0. Intuition",
    "text": "0. Intuition\nImagine you are at a library to look for a certain piece of information. The entire library is off limits to the general public. The only way you can access any information is by telling the librarian exactly what you want and let them fetch it for you. The entire library is your database. The librarian is your query engine and what you tell them is your query. In the library, there are a number of shelves (representing tables) that contain any number of books (representing rows in a table) to answer your query. Each book from the same shelf contains the same number of pages (representing columns or fields in a row), each page having a distinct piece of information. Following your instruction, the librarian may walk to different shelves, scour some books and pages, mix-and-match the information, then present the answer to you. Therefore, our task is to give an instruction such that the librarian can give us the most accurate answer using the shortest time and energy possible."
  },
  {
    "objectID": "posts/sql_almost/index.html#sanity-checks-keep-you-sane",
    "href": "posts/sql_almost/index.html#sanity-checks-keep-you-sane",
    "title": "(Almost) All SQL You Need",
    "section": "1. Sanity Checks Keep You Sane",
    "text": "1. Sanity Checks Keep You Sane\nWe are treating the Taobao User Behavior dataset as our table called transaction_tbl. It is a 5M-row subset based on parquet files partitioned by category_id and behavior_type.\n\n\nCode\ncon = duckdb.connect()\n\n#short-hand function to execute query with duckdb easily\ndef execute_query(query): return con.execute(query).fetchdf()\n\nquery = f\"\"\"\nCREATE OR REPLACE TABLE transaction_tbl AS\nSELECT *\nFROM read_parquet('../../data/sql_almost/transaction_tbl/*/*/*.parquet', hive_partitioning=true);\n\"\"\"\n\n#load saved, partitioned table as transaction_tbl\nexecute_query(query)\n\n\nThe table is a run-of-the-mill user behavior log consisting of\n\nuser_id: identifier of user\nitem_id: identifier of item\ntimestamp: unix timestamp of when action happened\nbehavior_type: what type of action it was\ncategory_id: identifier of category the item belongs to\n\n\nExample: select *\nThe first thing you do with a table you have not seen before is to select a few rows to look at. The texts in variable query is what you would type in your SQL interface.\n\n\nCode\nquery = f\"\"\"\nselect * from transaction_tbl limit 100;\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nuser_id\nitem_id\ntimestamp\nbehavior_type\ncategory_id\n\n\n\n\n0\n184439\n688483\n1512105722\nbuy\n1000858\n\n\n1\n860167\n3032030\n1511876592\npv\n1000858\n\n\n2\n29019\n1174848\n1511952716\npv\n1000858\n\n\n...\n...\n...\n...\n...\n...\n\n\n97\n109393\n336502\n1512302682\npv\n1003726\n\n\n98\n928768\n2826670\n1511780591\npv\n1003726\n\n\n99\n719622\n2297792\n1511703912\npv\n1003726\n\n\n\n\n100 rows √ó 5 columns\n\n\n\nselect * returns all columns from a table, and is almost always followed by limit [NUMBER OF ROWS YOU WANT] in order to not return the entire table but a random subset of it. This is less of a problem in the day and age where most query engines have a built-in safeguard to not display the entire table as a result, otherwise you can crash your system by asking for a terabyte-level output.\n\n\nExample: describe\nAnother useful command is describe. It will tell you what the columns are, their data types, do they have null (missing) values and so on. Common data types are int/bigint for integers, float for approximate decimals, varchar/string for texts, boolean for true/false and timestamp/date for, well, timestamps and dates. However, they are always ever so slightly different depending on which SQL you are using so better confirm with the documentation such as this one for Presto.\n\n\nCode\nquery = f\"\"\"\ndescribe transaction_tbl;\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\ncolumn_name\ncolumn_type\nnull\nkey\ndefault\nextra\n\n\n\n\n0\nuser_id\nBIGINT\nYES\nNone\nNone\nNone\n\n\n1\nitem_id\nBIGINT\nYES\nNone\nNone\nNone\n\n\n2\ntimestamp\nBIGINT\nYES\nNone\nNone\nNone\n\n\n3\nbehavior_type\nVARCHAR\nYES\nNone\nNone\nNone\n\n\n4\ncategory_id\nBIGINT\nYES\nNone\nNone\nNone\n\n\n\n\n\n\n\n\n\nExample: count and count(distinct [COLUMN])\nThe last check you always want to do is to count how many rows exist in the table:\n\n\nCode\nquery = f\"\"\"\nselect count(*) from transaction_tbl;\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\ncount_star()\n\n\n\n\n0\n5000000\n\n\n\n\n\n\n\nWe can also count how many unique values are in any column by adding distinct in front of the column name in count. You can also name your derived columns to know which ones mean what using as.\n\n\nCode\n# count how many unique users\nquery = f\"\"\"\nselect \n count(*) as nb_event\n ,count(distinct user_id) as nb_user\nfrom transaction_tbl;\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nnb_event\nnb_user\n\n\n\n\n0\n5000000\n888335"
  },
  {
    "objectID": "posts/sql_almost/index.html#assorted-platter-of-selects",
    "href": "posts/sql_almost/index.html#assorted-platter-of-selects",
    "title": "(Almost) All SQL You Need",
    "section": "2. Assorted Platter of Selects",
    "text": "2. Assorted Platter of Selects\nNow that you know how to sanity-check a table, let us move on to selecting what you want. This is often done by filtering the result with conditions using the where clause. Here are some examples you want to familiarize yourself with.\n\nExample: Who are the users that viewed item 2067266?\nLink multiple conditions with logical operators and / or.\n\n\nCode\nquery = f\"\"\"\nselect \n distinct user_id\nfrom transaction_tbl\nwhere behavior_type = 'pv'\n and item_id = 2067266;\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nuser_id\n\n\n\n\n0\n377356\n\n\n1\n690994\n\n\n2\n485340\n\n\n...\n...\n\n\n4\n469061\n\n\n5\n457064\n\n\n6\n329947\n\n\n\n\n7 rows √ó 1 columns\n\n\n\n\n\nExample: Who are the users that viewed item 2067266, ranked by who viewed last to who viewed first?\nWe use order by [COLUMN] followed by asc for ascending order (default) and desc for descending order.\n\n\nCode\nquery = f\"\"\"\nselect \n user_id\n ,timestamp\nfrom transaction_tbl\nwhere behavior_type = 'pv'\n and item_id = 2067266\norder by timestamp desc;\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nuser_id\ntimestamp\n\n\n\n\n0\n241453\n1512288234\n\n\n1\n485340\n1512160311\n\n\n2\n690994\n1512127740\n\n\n...\n...\n...\n\n\n4\n469061\n1511865189\n\n\n5\n329947\n1511850299\n\n\n6\n457064\n1511601891\n\n\n\n\n7 rows √ó 2 columns\n\n\n\n\n\nExample: How many users did not buy?\n&lt;&gt; means not equal.\n\n\nCode\nquery = f\"\"\"\nselect \n count(distinct user_id) \nfrom transaction_tbl\nwhere behavior_type &lt;&gt; 'buy';\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\ncount(DISTINCT user_id)\n\n\n\n\n0\n882077\n\n\n\n\n\n\n\n\n\nExample: How many events happened before November 26, 2017 (timestamp 1511622000)?\n&gt;, &lt;, &gt;=, &lt;= also works with numeric columns.\n\n\nCode\nquery = f\"\"\"\nselect \n count(*)\nfrom transaction_tbl\nwhere timestamp &lt; 1511622000;\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\ncount_star()\n\n\n\n\n0\n489758\n\n\n\n\n\n\n\n\n\nExample: How many events happened between November 25, 2017 (timestamp 1511535600) and November 26, 2017 (timestamp 1511622000)?\nYou can use between to replace &gt;= and &lt;=.\n\n\nCode\nquery = f\"\"\"\nselect \n count(*)\nfrom transaction_tbl\nwhere timestamp between 1511535600 and 1511622000;\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\ncount_star()\n\n\n\n\n0\n487676\n\n\n\n\n\n\n\n\n\nExample: How many events are either purchase (buy) or add-to-cart (cart)?\nWe can use in in place of multiple or\n\n\nCode\nquery = f\"\"\"\nselect \n count(*)\nfrom transaction_tbl\nwhere behavior_type in ('buy', 'cart');\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\ncount_star()\n\n\n\n\n0\n376999\n\n\n\n\n\n\n\nCheck to see if multiple or really gives the same result.\n\n\nCode\nquery = f\"\"\"\nselect \n count(*)\nfrom transaction_tbl\nwhere behavior_type = 'buy' or behavior_type = 'cart';\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\ncount_star()\n\n\n\n\n0\n376999\n\n\n\n\n\n\n\n\n\nExample: How many users interact (pv, cart, fav) with Taobao but never buy?\nWe can use not as a negation for in.\n\n\nCode\nquery = f\"\"\"\nselect \n count(distinct user_id)\nfrom transaction_tbl\nwhere behavior_type not in ('buy');\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\ncount(DISTINCT user_id)\n\n\n\n\n0\n882077\n\n\n\n\n\n\n\n\n\nExample: How many behavior_type ends with a v?\nWe use like to match texts that are similar what we want. % is the wild card to say anything can come before/after it. In this simple example, we already know the answer is 2: pv and fav.\n\n\nCode\nquery = f\"\"\"\nselect \n count(distinct behavior_type)\nfrom transaction_tbl\nwhere behavior_type like '%v';\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\ncount(DISTINCT behavior_type)\n\n\n\n\n0\n2"
  },
  {
    "objectID": "posts/sql_almost/index.html#partitions-partitions-partitions",
    "href": "posts/sql_almost/index.html#partitions-partitions-partitions",
    "title": "(Almost) All SQL You Need",
    "section": "3. Partitions, Partitions, Partitions",
    "text": "3. Partitions, Partitions, Partitions\nOnce you have mastered the basic select patterns, the next thing you must never forget when selecting from a table is to ALWAYS SPECIFY THE PARTITIONS YOU NEED IN THE WHERE CLAUSE. A modern SQL table is stored as multiple compressed files (most commonly parquet) in a nested subfolder structure as seen below. In this case, the partitions (subfolders) are columns category_id and behavior_type.\n\nFun Fact: Not only do you need to specify the partitions, you need to specify them with the correct data types. I once had a query than ran for 12 hours instead of 5 minutes, simply because I did not check if the partition was in datetime not string. Do not be me.\n\n\n\nCode\ndef print_table_structure(table_path, first_category_id_dir):\n\n    #print root folder\n    print(table_path.split(os.sep)[-1])  \n\n    #get category_id partitions\n    category_ids = sorted([d for d in os.listdir(table_path) if os.path.isdir(os.path.join(table_path, d)) and d.startswith('category_id=')])\n\n    #print only the first category_id partition\n    category_id_path = os.path.join(table_path, first_category_id_dir)\n    print(f\"  ‚îî‚îÄ‚îÄ {first_category_id_dir}\")\n\n    #get behavior_type partitions\n    behavior_types = sorted([d for d in os.listdir(category_id_path) if os.path.isdir(os.path.join(category_id_path, d)) and d.startswith('behavior_type=')])\n\n    #print all behavior_type partitions\n    for behavior_type_dir in behavior_types:\n      behavior_type_path = os.path.join(category_id_path, behavior_type_dir)\n      print(f\"    ‚îî‚îÄ‚îÄ {behavior_type_dir}\")\n\n      #get all parquet files\n      files = sorted([f for f in os.listdir(behavior_type_path) if f.endswith('.parquet')])\n\n      #print all files under category_id, behavior_type partitions\n      for i, file in enumerate(files):\n          if i == len(files) - 1:\n              print(f\"        ‚îî‚îÄ‚îÄ {file}\")\n          else:\n              print(f\"        ‚îú‚îÄ‚îÄ {file}\")\n\n    if len(category_ids) &gt; 1:\n        print(f\"  {len(category_ids) - 1} more category_id folders...\")\n\nprint_table_structure('../../data/sql_almost/transaction_tbl','category_id=1462446')\n\n\ntransaction_tbl\n  ‚îî‚îÄ‚îÄ category_id=1462446\n    ‚îî‚îÄ‚îÄ behavior_type=buy\n        ‚îú‚îÄ‚îÄ data_0.parquet\n        ‚îú‚îÄ‚îÄ data_1.parquet\n        ‚îú‚îÄ‚îÄ data_10.parquet\n        ‚îú‚îÄ‚îÄ data_2.parquet\n        ‚îú‚îÄ‚îÄ data_3.parquet\n        ‚îú‚îÄ‚îÄ data_4.parquet\n        ‚îú‚îÄ‚îÄ data_5.parquet\n        ‚îú‚îÄ‚îÄ data_6.parquet\n        ‚îú‚îÄ‚îÄ data_7.parquet\n        ‚îú‚îÄ‚îÄ data_8.parquet\n        ‚îî‚îÄ‚îÄ data_9.parquet\n    ‚îî‚îÄ‚îÄ behavior_type=cart\n        ‚îú‚îÄ‚îÄ data_0.parquet\n        ‚îú‚îÄ‚îÄ data_1.parquet\n        ‚îú‚îÄ‚îÄ data_10.parquet\n        ‚îú‚îÄ‚îÄ data_2.parquet\n        ‚îú‚îÄ‚îÄ data_3.parquet\n        ‚îú‚îÄ‚îÄ data_4.parquet\n        ‚îú‚îÄ‚îÄ data_5.parquet\n        ‚îú‚îÄ‚îÄ data_6.parquet\n        ‚îú‚îÄ‚îÄ data_7.parquet\n        ‚îú‚îÄ‚îÄ data_8.parquet\n        ‚îî‚îÄ‚îÄ data_9.parquet\n    ‚îî‚îÄ‚îÄ behavior_type=fav\n        ‚îú‚îÄ‚îÄ data_0.parquet\n        ‚îú‚îÄ‚îÄ data_1.parquet\n        ‚îú‚îÄ‚îÄ data_2.parquet\n        ‚îú‚îÄ‚îÄ data_3.parquet\n        ‚îú‚îÄ‚îÄ data_4.parquet\n        ‚îú‚îÄ‚îÄ data_5.parquet\n        ‚îú‚îÄ‚îÄ data_6.parquet\n        ‚îî‚îÄ‚îÄ data_7.parquet\n    ‚îî‚îÄ‚îÄ behavior_type=pv\n        ‚îú‚îÄ‚îÄ data_0.parquet\n        ‚îú‚îÄ‚îÄ data_1.parquet\n        ‚îú‚îÄ‚îÄ data_10.parquet\n        ‚îú‚îÄ‚îÄ data_2.parquet\n        ‚îú‚îÄ‚îÄ data_3.parquet\n        ‚îú‚îÄ‚îÄ data_4.parquet\n        ‚îú‚îÄ‚îÄ data_5.parquet\n        ‚îú‚îÄ‚îÄ data_6.parquet\n        ‚îú‚îÄ‚îÄ data_7.parquet\n        ‚îú‚îÄ‚îÄ data_8.parquet\n        ‚îî‚îÄ‚îÄ data_9.parquet\n  7796 more category_id folders...\n\n\nTo understand why specifying partitions is crucial, let us try our first query again and see how long it takes to run. We conduct 100 trials of 100 runs each to get mean and standard deviation of query time.\n\nExample: Who are the users that viewed item 2067266? Run WITHOUT partition in where clause.\n\n\nCode\nquery = \"\"\"\nselect distinct user_id from transaction_tbl\nwhere item_id = 2067266;\n\"\"\"\n\nts = timeit.repeat(\n  lambda: execute_query(query),\n  number=100,\n  repeat=100\n)\n\nprint(f'Query time WITHOUT partition: {np.mean(ts):.2f}¬±{np.std(ts):.2f} seconds')\n\n\nQuery time WITHOUT partition: 0.15¬±0.08 seconds\n\n\n\n\nExample: Who are the users that viewed item 2067266? Run WITH category_id partition in where clause.\n\n\nCode\nquery = \"\"\"\nselect distinct user_id from transaction_tbl\nwhere item_id = 2067266\n and category_id = 4339722;\n\"\"\"\n\nts = timeit.repeat(\n  lambda: execute_query(query),\n  number=100,\n  repeat=100\n)\n\nprint(f'Query time WITH partition: {np.mean(ts):.2f}¬±{np.std(ts):.2f} seconds')\n\n\nQuery time WITH partition: 0.06¬±0.06 seconds\n\n\nWhat sorcery is this? The query time is roughly halved! This is because when we include partitions in the where clause, we are telling the query engine to only look at the specific parts of the data not the entire table. In real life, not specifying which partitions you need as detailed as possible can spell the difference between waiting for two hours or a few seconds."
  },
  {
    "objectID": "posts/sql_almost/index.html#column-wise-manipulation",
    "href": "posts/sql_almost/index.html#column-wise-manipulation",
    "title": "(Almost) All SQL You Need",
    "section": "4. Column-wise Manipulation",
    "text": "4. Column-wise Manipulation\nBefore we move on, you might have noticed that our dataset is a little bland with only a timestamp and categorical columns (timestamp, user_id, item_id, category_id, behavior_type). In reality, such table as the one we are using often contains price of the item and quantity by which they were purchased. We can add that by manipulating existing columns.\n\nExample: Add price column where price is item_id modulus 1000 + 50\nHere we add the price column we randomly generated to the result. You can manipulate any numeric columns with arithmetic operators.\n\n\nCode\nquery = f\"\"\"\nselect \n *\n ,item_id % 1000 + 50 as price\nfrom transaction_tbl;\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nuser_id\nitem_id\ntimestamp\nbehavior_type\ncategory_id\nprice\n\n\n\n\n0\n184439\n688483\n1512105722\nbuy\n1000858\n533\n\n\n1\n860167\n3032030\n1511876592\npv\n1000858\n80\n\n\n2\n29019\n1174848\n1511952716\npv\n1000858\n898\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n4999997\n461462\n1642299\n1511677441\npv\n999980\n349\n\n\n4999998\n891831\n2064564\n1512255716\npv\n999980\n614\n\n\n4999999\n10430\n2006178\n1512012994\npv\n999980\n228\n\n\n\n\n5000000 rows √ó 6 columns\n\n\n\n\n\nExample: Add quantity column that is a random number between 1 and 10, only for buy events. For all other events, leave it as missing values (null).\nWhen you buy from a store, you need to specify a quantity of the items; however, for other events (when you view, favorite or add-to-cart), you do not. We must give the query engine an if-else logic depending on the values in each row. In SQL, we do this by using case when [CONDITION] then [VALUE] else [DEFAULT VALUE] end. We use setseed to keep the randomized numbers the same set.\n\n\nCode\n#random is randomizing number between 0 and 1 then floor rounds it down\nquery = f\"\"\"\nselect setseed(0.112);\nselect \n *\n ,item_id % 1000 + 50 as price\n ,case when behavior_type = 'buy' then FLOOR(RANDOM() * 10) + 1 else null end as quantity\nfrom transaction_tbl\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nuser_id\nitem_id\ntimestamp\nbehavior_type\ncategory_id\nprice\nquantity\n\n\n\n\n0\n184439\n688483\n1512105722\nbuy\n1000858\n533\n2.0\n\n\n1\n860167\n3032030\n1511876592\npv\n1000858\n80\nNaN\n\n\n2\n29019\n1174848\n1511952716\npv\n1000858\n898\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4999997\n461462\n1642299\n1511677441\npv\n999980\n349\nNaN\n\n\n4999998\n891831\n2064564\n1512255716\npv\n999980\n614\nNaN\n\n\n4999999\n10430\n2006178\n1512012994\npv\n999980\n228\nNaN\n\n\n\n\n5000000 rows √ó 7 columns\n\n\n\nWe can also have as many when as we want. If we want to explicitly state the conditions for all behavior_type, it will look something like:\n\n\nCode\nquery = f\"\"\"\nselect setseed(0.112);\n\nselect \n *\n ,item_id % 1000 + 50 as price\n ,case \n   when behavior_type = 'buy' then FLOOR(RANDOM() * 10) + 1 \n   when behavior_type = 'pv' then null\n   when behavior_type = 'cart' then null\n   when behavior_type = 'fav' then null\n   else null \n  end as quantity\nfrom transaction_tbl;\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nuser_id\nitem_id\ntimestamp\nbehavior_type\ncategory_id\nprice\nquantity\n\n\n\n\n0\n184439\n688483\n1512105722\nbuy\n1000858\n533\n4.0\n\n\n1\n860167\n3032030\n1511876592\npv\n1000858\n80\nNaN\n\n\n2\n29019\n1174848\n1511952716\npv\n1000858\n898\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4999997\n461462\n1642299\n1511677441\npv\n999980\n349\nNaN\n\n\n4999998\n891831\n2064564\n1512255716\npv\n999980\n614\nNaN\n\n\n4999999\n10430\n2006178\n1512012994\npv\n999980\n228\nNaN\n\n\n\n\n5000000 rows √ó 7 columns\n\n\n\n\n\nExample: Calculate sales by multiplying price and quantity\nWe can do operations among columns of the table. This example necessitates us to perform select twice: first to create price and quantity, then to create sales by multiplying them together. We do this by writing a subquery.\n\n\nCode\nquery = f\"\"\"\nselect setseed(0.112);\n\nselect \n whatever_subquery_alias_you_want.*\n ,price * quantity as sales\nfrom \n(select \n *\n ,item_id % 1000 + 50 as price\n ,case when behavior_type = 'buy' then FLOOR(RANDOM() * 10) + 1 else null end as quantity\nfrom transaction_tbl) whatever_subquery_alias_you_want;\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nuser_id\nitem_id\ntimestamp\nbehavior_type\ncategory_id\nprice\nquantity\nsales\n\n\n\n\n0\n184439\n688483\n1512105722\nbuy\n1000858\n533\n6.0\n3198.0\n\n\n1\n860167\n3032030\n1511876592\npv\n1000858\n80\nNaN\nNaN\n\n\n2\n29019\n1174848\n1511952716\npv\n1000858\n898\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4999997\n461462\n1642299\n1511677441\npv\n999980\n349\nNaN\nNaN\n\n\n4999998\n891831\n2064564\n1512255716\npv\n999980\n614\nNaN\nNaN\n\n\n4999999\n10430\n2006178\n1512012994\npv\n999980\n228\nNaN\nNaN\n\n\n\n\n5000000 rows √ó 8 columns\n\n\n\nYou would notice that the query becomes exponentially less readable with subqueries. This is especially the case when you have multiple nested subqueries and they become unreadable even by you in the next 3 months. An elegant solution is to separate these subqueries with with clauses. This is especially useful if you have a subquery you would like to reuse later in the same query. There is no performance difference between subqueries and with clauses, so pick what is easiest to read for you.\n\n\nCode\nquery = f\"\"\"\nselect setseed(0.112);\n\nwith t1_tbl as (\nselect \n *\n ,item_id % 1000 + 50 as price\n ,case when behavior_type = 'buy' then FLOOR(RANDOM() * 10) + 1 else null end as quantity\nfrom transaction_tbl\n)\n\nselect \n *\n ,price * quantity as sales\nfrom t1_tbl;\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nuser_id\nitem_id\ntimestamp\nbehavior_type\ncategory_id\nprice\nquantity\nsales\n\n\n\n\n0\n184439\n688483\n1512105722\nbuy\n1000858\n533\n6.0\n3198.0\n\n\n1\n860167\n3032030\n1511876592\npv\n1000858\n80\nNaN\nNaN\n\n\n2\n29019\n1174848\n1511952716\npv\n1000858\n898\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4999997\n461462\n1642299\n1511677441\npv\n999980\n349\nNaN\nNaN\n\n\n4999998\n891831\n2064564\n1512255716\npv\n999980\n614\nNaN\nNaN\n\n\n4999999\n10430\n2006178\n1512012994\npv\n999980\n228\nNaN\nNaN\n\n\n\n\n5000000 rows √ó 8 columns\n\n\n\n\n\nExample: Fill in missing values in sales with zero.\nWe use coalesce to fill in missing values. Notice that we can have multiple with clauses (consecutive ones puncutated by , and do not need with) depending on how you thin is most readable.\n\n\nCode\nquery = f\"\"\"\nselect setseed(0.112);\n\nwith t1_tbl as (\nselect \n *\n ,item_id % 1000 + 50 as price\n ,case when behavior_type = 'buy' then FLOOR(RANDOM() * 10) + 1 else null end as quantity\nfrom transaction_tbl\n),\n\nt2_tbl as (\nselect \n *\n ,price * quantity as sales\nfrom t1_tbl\n)\n\nselect\n user_id\n ,item_id\n ,timestamp\n ,behavior_type\n ,category_id\n ,price\n ,quantity\n ,coalesce(sales, 0) as sales\nfrom t2_tbl;\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nuser_id\nitem_id\ntimestamp\nbehavior_type\ncategory_id\nprice\nquantity\nsales\n\n\n\n\n0\n184439\n688483\n1512105722\nbuy\n1000858\n533\n2.0\n1066.0\n\n\n1\n860167\n3032030\n1511876592\npv\n1000858\n80\nNaN\n0.0\n\n\n2\n29019\n1174848\n1511952716\npv\n1000858\n898\nNaN\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4999997\n461462\n1642299\n1511677441\npv\n999980\n349\nNaN\n0.0\n\n\n4999998\n891831\n2064564\n1512255716\npv\n999980\n614\nNaN\n0.0\n\n\n4999999\n10430\n2006178\n1512012994\npv\n999980\n228\nNaN\n0.0\n\n\n\n\n5000000 rows √ó 8 columns\n\n\n\n\n\nExample: Convert timestamp from unix timestamp to yyyy-mm-dd format.\nDatetime conversion, as in any programming script, is a very confusing affair, so I highly recommend you refer to your specific SQL‚Äôs documentation such as this one for Spark. But the idea is simply applying some function over your columns. Here we use to_timestamp to convert from int to unix timestamp and then use strftime to convert to a string formatted as yyyy-mm-dd.\n\n\nCode\nquery = f\"\"\"\nselect setseed(0.112);\n\nwith t1_tbl as (\nselect \n *\n ,item_id % 1000 + 50 as price\n ,case when behavior_type = 'buy' then FLOOR(RANDOM() * 10) + 1 else null end as quantity\nfrom transaction_tbl\n),\n\nt2_tbl as (\nselect \n *\n ,price * quantity as sales\nfrom t1_tbl\n),\n\nt3_tbl as (\nselect\n user_id\n ,item_id\n ,timestamp\n ,behavior_type\n ,category_id\n ,price\n ,quantity\n ,coalesce(sales, 0) as sales\nfrom t2_tbl)\n\nselect\n *\n ,to_timestamp(timestamp) as event_timestamp\n ,strftime(to_timestamp(timestamp), '%Y-%m-%d') as event_date\nfrom t3_tbl;\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nuser_id\nitem_id\ntimestamp\nbehavior_type\ncategory_id\nprice\nquantity\nsales\nevent_timestamp\nevent_date\n\n\n\n\n0\n184439\n688483\n1512105722\nbuy\n1000858\n533\n5.0\n2665.0\n2017-12-01 14:22:02+09:00\n2017-12-01\n\n\n1\n860167\n3032030\n1511876592\npv\n1000858\n80\nNaN\n0.0\n2017-11-28 22:43:12+09:00\n2017-11-28\n\n\n2\n29019\n1174848\n1511952716\npv\n1000858\n898\nNaN\n0.0\n2017-11-29 19:51:56+09:00\n2017-11-29\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4999997\n461462\n1642299\n1511677441\npv\n999980\n349\nNaN\n0.0\n2017-11-26 15:24:01+09:00\n2017-11-26\n\n\n4999998\n891831\n2064564\n1512255716\npv\n999980\n614\nNaN\n0.0\n2017-12-03 08:01:56+09:00\n2017-12-03\n\n\n4999999\n10430\n2006178\n1512012994\npv\n999980\n228\nNaN\n0.0\n2017-11-30 12:36:34+09:00\n2017-11-30\n\n\n\n\n5000000 rows √ó 10 columns\n\n\n\n\n\nExample: Make a string column year_month that takes only the yyyy-mm part from event_timestamp.\nMost query engines have built-in functions to manipulate texts such as this one for duckdb. However, in this case, since event_timestamp is a timestmap, we need to convert its data type to string before applying the function substring by using cast([COLUMN] as [DATA TYPE]).\n\n\nCode\nquery = f\"\"\"\nselect setseed(0.112);\n\nwith t1_tbl as (\nselect \n *\n ,item_id % 1000 + 50 as price\n ,case when behavior_type = 'buy' then FLOOR(RANDOM() * 10) + 1 else null end as quantity\nfrom transaction_tbl\n),\n\nt2_tbl as (\nselect \n *\n ,price * quantity as sales\nfrom t1_tbl\n),\n\nt3_tbl as (\nselect\n user_id\n ,item_id\n ,timestamp\n ,behavior_type\n ,category_id\n ,price\n ,quantity\n ,coalesce(sales, 0) as sales\nfrom t2_tbl),\n\nt4_tbl as (\nselect\n *\n ,to_timestamp(timestamp) as event_timestamp\n ,strftime(to_timestamp(timestamp), '%Y-%m-%d') as event_date\nfrom t3_tbl)\n\nselect\n *\n ,substring(cast(event_timestamp as varchar),1,7) as year_month\nfrom t4_tbl;\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nuser_id\nitem_id\ntimestamp\nbehavior_type\ncategory_id\nprice\nquantity\nsales\nevent_timestamp\nevent_date\nyear_month\n\n\n\n\n0\n184439\n688483\n1512105722\nbuy\n1000858\n533\n5.0\n2665.0\n2017-12-01 14:22:02+09:00\n2017-12-01\n2017-12\n\n\n1\n860167\n3032030\n1511876592\npv\n1000858\n80\nNaN\n0.0\n2017-11-28 22:43:12+09:00\n2017-11-28\n2017-11\n\n\n2\n29019\n1174848\n1511952716\npv\n1000858\n898\nNaN\n0.0\n2017-11-29 19:51:56+09:00\n2017-11-29\n2017-11\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4999997\n461462\n1642299\n1511677441\npv\n999980\n349\nNaN\n0.0\n2017-11-26 15:24:01+09:00\n2017-11-26\n2017-11\n\n\n4999998\n891831\n2064564\n1512255716\npv\n999980\n614\nNaN\n0.0\n2017-12-03 08:01:56+09:00\n2017-12-03\n2017-12\n\n\n4999999\n10430\n2006178\n1512012994\npv\n999980\n228\nNaN\n0.0\n2017-11-30 12:36:34+09:00\n2017-11-30\n2017-11\n\n\n\n\n5000000 rows √ó 11 columns\n\n\n\n\n\nExample: Save the manipulations done so far as a view to be used later.\nYou‚Äôll notice that even with the with clauses, our query seems substantially more clunky now. We do not want to be re-writing these lines every time we reuse this set of results for other queries. Luckily, there is a solution called view. A view saves the query logic that can be used for other queries later. Unlike actual SQL tables, the data are not stored physically on your database, so the query saved to a view will still run every time it is called.\nThis query creates the view:\n\n\nCode\nquery = f\"\"\"\nselect setseed(0.112);\n\ncreate or replace view transaction_tbl_x as (\n\nwith t1_tbl as (\nselect \n *\n ,item_id % 1000 + 50 as price\n ,case when behavior_type = 'buy' then FLOOR(RANDOM() * 10) + 1 else null end as quantity\nfrom transaction_tbl\n),\n\nt2_tbl as (\nselect \n *\n ,price * quantity as sales\nfrom t1_tbl\n),\n\nt3_tbl as (\nselect\n user_id\n ,item_id\n ,timestamp\n ,behavior_type\n ,category_id\n ,price\n ,quantity\n ,coalesce(sales, 0) as sales\nfrom t2_tbl),\n\nt4_tbl as (\nselect\n *\n ,to_timestamp(timestamp) as event_timestamp\n ,strftime(to_timestamp(timestamp), '%Y-%m-%d') as event_date\nfrom t3_tbl)\n\nselect\n *\n ,substring(cast(event_timestamp as varchar),1,7) as year_month\nfrom t4_tbl\n)\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nCount\n\n\n\n\n\n\n\n\n\nThis query reuses it by a simple select *:\n\n\nCode\nquery = f\"\"\"\nselect setseed(0.112);\n\nselect * from transaction_tbl_x limit 100;\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nuser_id\nitem_id\ntimestamp\nbehavior_type\ncategory_id\nprice\nquantity\nsales\nevent_timestamp\nevent_date\nyear_month\n\n\n\n\n0\n184439\n688483\n1512105722\nbuy\n1000858\n533\n5.0\n2665.0\n2017-12-01 14:22:02+09:00\n2017-12-01\n2017-12\n\n\n1\n860167\n3032030\n1511876592\npv\n1000858\n80\nNaN\n0.0\n2017-11-28 22:43:12+09:00\n2017-11-28\n2017-11\n\n\n2\n29019\n1174848\n1511952716\npv\n1000858\n898\nNaN\n0.0\n2017-11-29 19:51:56+09:00\n2017-11-29\n2017-11\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n97\n109393\n336502\n1512302682\npv\n1003726\n552\nNaN\n0.0\n2017-12-03 21:04:42+09:00\n2017-12-03\n2017-12\n\n\n98\n928768\n2826670\n1511780591\npv\n1003726\n720\nNaN\n0.0\n2017-11-27 20:03:11+09:00\n2017-11-27\n2017-11\n\n\n99\n719622\n2297792\n1511703912\npv\n1003726\n842\nNaN\n0.0\n2017-11-26 22:45:12+09:00\n2017-11-26\n2017-11\n\n\n\n\n100 rows √ó 11 columns"
  },
  {
    "objectID": "posts/sql_almost/index.html#aggregation-aka-group-by",
    "href": "posts/sql_almost/index.html#aggregation-aka-group-by",
    "title": "(Almost) All SQL You Need",
    "section": "5. Aggregation aka Group By",
    "text": "5. Aggregation aka Group By\nWhen you have millions of rows of data, you do not want to look at them one by one; you want to know how they appear in aggregate„Éºhow many events there are for each type, which items are favorited the most/least, how many items users buy on average, and so on. In fact, you have already learned how to do some of this. count and count(distinct [COLUMN]) are examples of aggregation over the entire table. In this section, we will also learn to use group by to get aggregated values according to the columns we want.\n\nExample: How many events are there for each event type (behavior_type)?\nYou can order by the newly created nb_event column to sort event types in descending order according to how many events they have. As expected, it is views, add-to-carts, favorites, then purchases.\n\n\nCode\nquery = f\"\"\"\nselect setseed(0.112);\n\nselect \n behavior_type\n ,count(*) as nb_event\nfrom transaction_tbl_x\ngroup by behavior_type\norder by nb_event desc;\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nbehavior_type\nnb_event\n\n\n\n\n0\npv\n4478549\n\n\n1\ncart\n276307\n\n\n2\nfav\n144452\n\n\n3\nbuy\n100692\n\n\n\n\n\n\n\nOne neat trick is that you do not have to write the column names in group by or order by and use numbering instead; for instance, 1 means the first column selected, in this case behavior_type.\n\n\nCode\nquery = f\"\"\"\nselect setseed(0.112);\n\nselect \n behavior_type\n ,count(*) as nb_event\nfrom transaction_tbl_x\ngroup by 1\norder by 2 desc;\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nbehavior_type\nnb_event\n\n\n\n\n0\npv\n4478549\n\n\n1\ncart\n276307\n\n\n2\nfav\n144452\n\n\n3\nbuy\n100692\n\n\n\n\n\n\n\n\n\nExample: What are the top 10 items that got favorited by most number of unique customers?\nWhen used in conjunction with where, the where clause comes before group by.\n\n\nCode\nquery = f\"\"\"\nselect setseed(0.112);\n\nselect \n item_id\n ,count(distinct user_id) as nb_user\nfrom transaction_tbl_x\nwhere behavior_type = 'fav'\ngroup by 1\norder by 2 desc\nlimit 10;\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nitem_id\nnb_user\n\n\n\n\n0\n2331370\n43\n\n\n1\n3845720\n41\n\n\n2\n2279428\n40\n\n\n...\n...\n...\n\n\n7\n2364679\n37\n\n\n8\n3403645\n35\n\n\n9\n1783990\n33\n\n\n\n\n10 rows √ó 2 columns\n\n\n\n\n\nExample: What is the average, standard deviation, min, and max spend per user?\nFirst, we need to sum up all sales for each user then perform the avg, stddev, min and max aggregation over all users. We can also see quantiles using functions like approx_quantile([COLUMN], [QUANTILE]).\n\n\nCode\nquery = f\"\"\"\nselect setseed(0.112);\n\nselect\n min(spend) as min_spend\n ,approx_quantile(spend, 0.25) as p25\n ,avg(spend) as avg_spend\n ,stddev(spend) as std_spend\n ,approx_quantile(spend, 0.5) as p50\n ,approx_quantile(spend, 0.75) as p75\n ,max(spend) as max_spend\nfrom\n(select \n user_id\n ,sum(sales) as spend\nfrom transaction_tbl_x\ngroup by 1\n) a;\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nmin_spend\np25\navg_spend\nstd_spend\np50\np75\nmax_spend\n\n\n\n\n0\n0.0\n0.0\n343.119825\n1343.99162\n0.0\n0.0\n33513.0\n\n\n\n\n\n\n\n\n\nExample: How many percentage of customer purchased at least once?\nAs you might notice from the last example, most of the customers have spend equals zero. This is a typical distribution in a retail business where most users come to window shop and only a few will make a purchase. We can find out % of those who made at least one purchase by combining avg aggregation and the case when if-else logic.\n\n\nCode\nquery = f\"\"\"\nselect setseed(0.112);\n\nselect\n avg(case when spend &gt; 0 then 1 else 0 end) as conversion_rate\nfrom\n(select \n user_id\n ,sum(sales) as spend\nfrom transaction_tbl_x\ngroup by 1\n) a;\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nconversion_rate\n\n\n\n\n0\n0.102035\n\n\n\n\n\n\n\n\n\nExample: Give me only customers who have bought at least one item more expensive than $100.\nIn the same manner we use where clause to filter select, we can also use having to filter aggregations. This works exactly the same where as how you would do aggregation first then filter it with a subquery. Most modern query engines treat them the same way in terms of performance so pick whichever is more readable to you.\n\n\nCode\nquery = f\"\"\"\nselect setseed(0.112);\n\nselect \n user_id\n ,min(price) as min_price\nfrom transaction_tbl_x\nwhere behavior_type = 'buy'\ngroup by 1\nhaving min_price &gt; 100\norder by min_price;\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nuser_id\nmin_price\n\n\n\n\n0\n782669\n101\n\n\n1\n302287\n101\n\n\n2\n773094\n101\n\n\n...\n...\n...\n\n\n85577\n340274\n1049\n\n\n85578\n603498\n1049\n\n\n85579\n438867\n1049\n\n\n\n\n85580 rows √ó 2 columns\n\n\n\n\n\nExample: For each user who have favorited anything, give me a list of items they have favorited ordered by timestamp in ascending order.\nToday you might run into preparing a sequence dataset to train LLMs, in which case you want to concatenate a series of values from a column. Most modern SQL handles this such as Spark SQL and Presto. In duckdb, we can use string_agg and keep order from earliest to latest timestamp by using order by.\n\n\nCode\nquery = f\"\"\"\nselect setseed(0.112);\n\nselect \n user_id\n ,string_agg(item_id order by timestamp) as item_id_list\nfrom transaction_tbl_x\nwhere behavior_type = 'fav'\ngroup by 1\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nuser_id\nitem_id_list\n\n\n\n\n0\n744534\n2149136,88810,4629792\n\n\n1\n517461\n4325698,5109079,2036947\n\n\n2\n261004\n1289993,4431704\n\n\n...\n...\n...\n\n\n98434\n702058\n4753515\n\n\n98435\n865408\n2643630\n\n\n98436\n391171\n5154868\n\n\n\n\n98437 rows √ó 2 columns"
  },
  {
    "objectID": "posts/sql_almost/index.html#window-function",
    "href": "posts/sql_almost/index.html#window-function",
    "title": "(Almost) All SQL You Need",
    "section": "6. Window Function",
    "text": "6. Window Function\nAggregation is very powerful, but a major downside is that it collapses the total rows into aggregated values. This means that after using group by, you lose the individual detail of each original row. You can no longer easily ask questions about the sequence of events, like ‚ÄúWhat happened just before this?‚Äù or ‚ÄúWhat is the third item in this list?‚Äù. Window functions solve this by allowing you to perform calculations on a related set of rows (a ‚Äúwindow‚Äù) without making those rows disappear. They let you calculate things like rankings, running totals, or compare values across rows, all while keeping all your original data rows intact.\n\nExample: For each user, find their second-to-last event.\nWe might be able to use min/max aggregation with some subqueries to get the last event, but second-to-last event is a bit convoluted to retrieve with group by alone. This is trivial when we use row_number() over (partition by [WINDOW COLUMN] order by [ORDERING COLUMN] asc/desc) to get the ranking numbers (starting with 1), then filter only the rows we need (rnk=2).\n\n\nCode\nquery = f\"\"\"\nselect setseed(0.112);\n\nselect * from\n(select \n *\n ,row_number() over (partition by user_id order by timestamp desc) as rnk\nfrom transaction_tbl_x) a\nwhere rnk=2\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nuser_id\nitem_id\ntimestamp\nbehavior_type\ncategory_id\nprice\nquantity\nsales\nevent_timestamp\nevent_date\nyear_month\nrnk\n\n\n\n\n0\n6\n730246\n1512151025\npv\n4756105\n296\nNaN\n0.0\n2017-12-02 02:57:05+09:00\n2017-12-02\n2017-12\n2\n\n\n1\n58\n274807\n1512191821\npv\n4458428\n857\nNaN\n0.0\n2017-12-02 14:17:01+09:00\n2017-12-02\n2017-12\n2\n\n\n2\n73\n2561888\n1512224052\npv\n753984\n938\nNaN\n0.0\n2017-12-02 23:14:12+09:00\n2017-12-02\n2017-12\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n753406\n1017789\n797757\n1512179970\npv\n883960\n807\nNaN\n0.0\n2017-12-02 10:59:30+09:00\n2017-12-02\n2017-12\n2\n\n\n753407\n1017827\n4777442\n1512258856\npv\n3607361\n492\nNaN\n0.0\n2017-12-03 08:54:16+09:00\n2017-12-03\n2017-12\n2\n\n\n753408\n1017858\n850224\n1512291735\npv\n3800818\n274\nNaN\n0.0\n2017-12-03 18:02:15+09:00\n2017-12-03\n2017-12\n2\n\n\n\n\n753409 rows √ó 12 columns\n\n\n\nWe can confirm that these are one second-to-last event each from users who have a least 2 events.\n\n\nCode\nquery = f\"\"\"\nselect setseed(0.112);\n\nselect count(*) from\n(select \n user_id\n ,count(*) nb\nfrom transaction_tbl_x\ngroup by 1) a\nwhere nb&gt;=2\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\ncount_star()\n\n\n\n\n0\n753409\n\n\n\n\n\n\n\n\n\nExample: What is the first item in each category that each user view?\nWe can also have multiple partitions as window.\n\n\nCode\nquery = f\"\"\"\nselect setseed(0.112);\n\nselect * from\n(select \n user_id\n ,item_id\n ,row_number() over (partition by user_id, category_id order by timestamp asc) as rnk\nfrom transaction_tbl_x\nwhere behavior_type = 'pv') a\nwhere rnk=1;\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nuser_id\nitem_id\nrnk\n\n\n\n\n0\n7\n516760\n1\n\n\n1\n19\n3615870\n1\n\n\n2\n21\n4928897\n1\n\n\n...\n...\n...\n...\n\n\n3389298\n1017999\n3251062\n1\n\n\n3389299\n1018006\n2789562\n1\n\n\n3389300\n1018011\n3190817\n1\n\n\n\n\n3389301 rows √ó 3 columns\n\n\n\n\n\nExample: For each user, what was their previous action before a buy action?\nWe can use lag/lead to get value just before or after each row.\n\n\nCode\npd.set_option('display.max_rows', 10) \n\nquery = f\"\"\"\nselect setseed(0.112);\n\nselect * from \n(select\n    user_id,\n    event_timestamp,\n    behavior_type as current_event,\n    lag(behavior_type) over (partition by user_id order by event_timestamp asc) as previous_event\nfrom transaction_tbl_x\norder by user_id, event_timestamp asc) a\nwhere current_event = 'buy'\nlimit 10\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nuser_id\nevent_timestamp\ncurrent_event\nprevious_event\n\n\n\n\n0\n2\n2017-12-02 20:34:34+09:00\nbuy\npv\n\n\n1\n41\n2017-11-28 15:22:28+09:00\nbuy\npv\n\n\n2\n45\n2017-12-03 11:49:02+09:00\nbuy\npv\n\n\n3\n50\n2017-11-27 09:02:55+09:00\nbuy\nNone\n\n\n4\n50\n2017-12-02 11:35:55+09:00\nbuy\npv\n\n\n5\n50\n2017-12-03 20:56:57+09:00\nbuy\nbuy\n\n\n6\n59\n2017-11-27 01:29:32+09:00\nbuy\npv\n\n\n7\n62\n2017-12-03 16:30:30+09:00\nbuy\npv\n\n\n8\n68\n2017-11-27 19:59:34+09:00\nbuy\npv\n\n\n9\n80\n2017-11-25 22:52:00+09:00\nbuy\npv\n\n\n\n\n\n\n\nConfirm the query works by looking at user_id=50 (no event, pv, buy) and user_id=41 (single pv event).\n\n\nCode\npd.set_option('display.max_rows', 30) \n\nquery = f\"\"\"\nselect setseed(0.112);\n\nselect\n *\nfrom transaction_tbl_x\nwhere user_id in (50, 41)\norder by user_id, event_timestamp\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nuser_id\nitem_id\ntimestamp\nbehavior_type\ncategory_id\nprice\nquantity\nsales\nevent_timestamp\nevent_date\nyear_month\n\n\n\n\n0\n41\n2350782\n1511590506\nfav\n3576283\n832\nNaN\n0.0\n2017-11-25 15:15:06+09:00\n2017-11-25\n2017-11\n\n\n1\n41\n4810522\n1511592979\npv\n1464116\n572\nNaN\n0.0\n2017-11-25 15:56:19+09:00\n2017-11-25\n2017-11\n\n\n2\n41\n259923\n1511651601\npv\n4170419\n973\nNaN\n0.0\n2017-11-26 08:13:21+09:00\n2017-11-26\n2017-11\n\n\n3\n41\n4786486\n1511758455\npv\n2572604\n536\nNaN\n0.0\n2017-11-27 13:54:15+09:00\n2017-11-27\n2017-11\n\n\n4\n41\n460114\n1511850148\nbuy\n4804883\n164\n5.0\n820.0\n2017-11-28 15:22:28+09:00\n2017-11-28\n2017-11\n\n\n5\n41\n1876500\n1511850397\npv\n3158249\n550\nNaN\n0.0\n2017-11-28 15:26:37+09:00\n2017-11-28\n2017-11\n\n\n6\n41\n3599288\n1512050217\ncart\n1537669\n338\nNaN\n0.0\n2017-11-30 22:56:57+09:00\n2017-11-30\n2017-11\n\n\n7\n41\n2479959\n1512050346\npv\n3491350\n1009\nNaN\n0.0\n2017-11-30 22:59:06+09:00\n2017-11-30\n2017-11\n\n\n8\n41\n3095445\n1512072759\nfav\n4801426\n495\nNaN\n0.0\n2017-12-01 05:12:39+09:00\n2017-12-01\n2017-12\n\n\n9\n41\n3937435\n1512072770\npv\n4801426\n485\nNaN\n0.0\n2017-12-01 05:12:50+09:00\n2017-12-01\n2017-12\n\n\n10\n41\n2601044\n1512073082\npv\n2735466\n94\nNaN\n0.0\n2017-12-01 05:18:02+09:00\n2017-12-01\n2017-12\n\n\n11\n41\n3017816\n1512073131\npv\n2735466\n866\nNaN\n0.0\n2017-12-01 05:18:51+09:00\n2017-12-01\n2017-12\n\n\n12\n41\n3976745\n1512073512\npv\n3002561\n795\nNaN\n0.0\n2017-12-01 05:25:12+09:00\n2017-12-01\n2017-12\n\n\n13\n50\n1808993\n1511740975\nbuy\n4690421\n1043\n10.0\n10430.0\n2017-11-27 09:02:55+09:00\n2017-11-27\n2017-11\n\n\n14\n50\n1353441\n1511743131\ncart\n64179\n491\nNaN\n0.0\n2017-11-27 09:38:51+09:00\n2017-11-27\n2017-11\n\n\n15\n50\n1963474\n1511743387\npv\n3422001\n524\nNaN\n0.0\n2017-11-27 09:43:07+09:00\n2017-11-27\n2017-11\n\n\n16\n50\n972172\n1511854245\npv\n4756105\n222\nNaN\n0.0\n2017-11-28 16:30:45+09:00\n2017-11-28\n2017-11\n\n\n17\n50\n407777\n1511948105\npv\n4756105\n827\nNaN\n0.0\n2017-11-29 18:35:05+09:00\n2017-11-29\n2017-11\n\n\n18\n50\n2559047\n1512108229\npv\n4756105\n97\nNaN\n0.0\n2017-12-01 15:03:49+09:00\n2017-12-01\n2017-12\n\n\n19\n50\n3408121\n1512110060\npv\n1320293\n171\nNaN\n0.0\n2017-12-01 15:34:20+09:00\n2017-12-01\n2017-12\n\n\n20\n50\n4225949\n1512110662\npv\n1879194\n999\nNaN\n0.0\n2017-12-01 15:44:22+09:00\n2017-12-01\n2017-12\n\n\n21\n50\n1820775\n1512112710\ncart\n4537973\n825\nNaN\n0.0\n2017-12-01 16:18:30+09:00\n2017-12-01\n2017-12\n\n\n22\n50\n3096190\n1512179808\npv\n472273\n240\nNaN\n0.0\n2017-12-02 10:56:48+09:00\n2017-12-02\n2017-12\n\n\n23\n50\n5155205\n1512179846\ncart\n4762182\n255\nNaN\n0.0\n2017-12-02 10:57:26+09:00\n2017-12-02\n2017-12\n\n\n24\n50\n518956\n1512181946\npv\n4835206\n1006\nNaN\n0.0\n2017-12-02 11:32:26+09:00\n2017-12-02\n2017-12\n\n\n25\n50\n120958\n1512182155\nbuy\n4762182\n1008\n3.0\n3024.0\n2017-12-02 11:35:55+09:00\n2017-12-02\n2017-12\n\n\n26\n50\n4619331\n1512302217\nbuy\n3884119\n381\n7.0\n2667.0\n2017-12-03 20:56:57+09:00\n2017-12-03\n2017-12\n\n\n27\n50\n5045605\n1512302237\npv\n820727\n655\nNaN\n0.0\n2017-12-03 20:57:17+09:00\n2017-12-03\n2017-12\n\n\n\n\n\n\n\n\n\nExample: What is the contribution of each item to their overall category sales?\nWe can also use aggregations like sum in conjuction with window function.\n\n\nCode\npd.set_option('display.max_rows', 6) \n\nquery = f\"\"\"\nselect setseed(0.112);\n\nselect\n item_id\n ,category_id\n ,item_sales / category_sales as percentage_category_sales\nfrom (\n  select\n   item_id\n   ,category_id\n   ,item_sales\n   ,sum(item_sales) over (partition by category_id) as category_sales\n  from (\n   select\n    item_id\n    ,category_id\n    ,sum(sales) as item_sales\n    from transaction_tbl_x\n    group by 1,2\n  ) a\n) b\nwhere category_sales &gt; 0;\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nitem_id\ncategory_id\npercentage_category_sales\n\n\n\n\n0\n644316\n75275\n0.0\n\n\n1\n4356670\n75275\n0.0\n\n\n2\n45889\n75275\n0.0\n\n\n...\n...\n...\n...\n\n\n1219659\n4607802\n5078340\n0.0\n\n\n1219660\n4052466\n5078340\n0.0\n\n\n1219661\n104515\n5078340\n0.0\n\n\n\n\n1219662 rows √ó 3 columns"
  },
  {
    "objectID": "posts/sql_almost/index.html#concatenation-aka-union",
    "href": "posts/sql_almost/index.html#concatenation-aka-union",
    "title": "(Almost) All SQL You Need",
    "section": "7. Concatenation aka Union",
    "text": "7. Concatenation aka Union\nSometimes you want to concatenate multiple tables with the same set of columns (called schema) together. union all is simple concatenation whereas union will also deduplicate the rows for you, only returning rows that are not perfectly identical to one another.\n\nExample: Concatenate monthly summary of 2017-11 and 2017-12 together.\nWe do not need to worry about duplicates here so we can simply use union all. See how we can combine count, distinct and case when to get monthly acitve customers and purchasers.\n\n\nCode\nquery = f\"\"\"\nselect setseed(0.112);\n\nselect * from\n(select \n year_month\n ,count(distinct user_id) as nb_active_customer\n ,count(distinct case when behavior_type='buy' then user_id else null end) as nb_purchaser\n ,sum(sales) as total_sales\nfrom transaction_tbl_x\nwhere year_month = '2017-11'\ngroup by 1)\nunion all\n(select \n year_month\n ,count(distinct user_id) as nb_active_customer\n ,count(distinct case when behavior_type='buy' then user_id else null end) as nb_purchaser\n ,sum(sales) as total_sales\nfrom transaction_tbl_x\nwhere year_month = '2017-12'\ngroup by 1);\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nyear_month\nnb_active_customer\nnb_purchaser\ntotal_sales\n\n\n\n\n0\n2017-11\n759497\n58836\n192782711.0\n\n\n1\n2017-12\n679573\n34938\n111758189.0\n\n\n\n\n\n\n\n\n\nExample: Give me a list of unique users who made at least one purchase or viewed at least 5 unique items.\nConcatenate then deduplicate.\n\n\nCode\nquery = f\"\"\"\nselect setseed(0.112);\n\nselect \n user_id\nfrom transaction_tbl_x\nwhere behavior_type = 'buy'\ngroup by 1\nunion\nselect \n user_id\nfrom transaction_tbl_x\nwhere behavior_type = 'fav'\ngroup by 1\nhaving count(*)&gt;=5\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nuser_id\n\n\n\n\n0\n898251\n\n\n1\n576709\n\n\n2\n837520\n\n\n...\n...\n\n\n92122\n86617\n\n\n92123\n906571\n\n\n92124\n103138\n\n\n\n\n92125 rows √ó 1 columns\n\n\n\nSimple concatenation will give duplicates.\n\n\nCode\nquery = f\"\"\"\nselect setseed(0.112);\n\nselect \n user_id\nfrom transaction_tbl_x\nwhere behavior_type = 'buy'\ngroup by 1\nunion all\nselect \n user_id\nfrom transaction_tbl_x\nwhere behavior_type = 'fav'\ngroup by 1\nhaving count(*)&gt;=5\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nuser_id\n\n\n\n\n0\n286908\n\n\n1\n910422\n\n\n2\n669209\n\n\n...\n...\n\n\n92359\n251575\n\n\n92360\n598643\n\n\n92361\n906629\n\n\n\n\n92362 rows √ó 1 columns"
  },
  {
    "objectID": "posts/sql_almost/index.html#joins",
    "href": "posts/sql_almost/index.html#joins",
    "title": "(Almost) All SQL You Need",
    "section": "8. Joins",
    "text": "8. Joins\njoin connects data from one table to another based on columns they share. There are many types of joins but 95% of your life will revolve around left join and inner join.\n\nExample: Among users who are active (have at least one event) on 2017-12-03, how many percent were active in 2017-11\nleft join starts with all rows from the left-side table (the former one) and add rows from the right-side table (the latter one) to it, if and only if the rows meet the conditions given in the on clause. These conditions are usually for values in a column from the left-side table to be equal to, not equal to, or more/less than the ones in a column from the right-side table.\nBe sure to give aliases to columns you select and are joining on. Your query engine needs to know exactly from which table the columns came from if they have the same name.\n\n\nCode\nquery = f\"\"\"\nselect setseed(0.112);\n\nwith november_active_tbl as (\nselect\n user_id\n ,1 as active_in_november\nfrom transaction_tbl_x\nwhere year_month = '2017-11'\ngroup by 1,2\n),\n\ndec3_active_tbl as (\nselect\n user_id\nfrom transaction_tbl_x\nwhere event_date = '2017-12-03'\ngroup by 1\n)\n\nselect\n dec3.user_id\n ,active_in_november\nfrom dec3_active_tbl dec3\nleft join november_active_tbl nov\non dec3.user_id = nov.user_id;\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nuser_id\nactive_in_november\n\n\n\n\n0\n78674\n1\n\n\n1\n755902\n1\n\n\n2\n829454\n1\n\n\n...\n...\n...\n\n\n375424\n743851\n&lt;NA&gt;\n\n\n375425\n543372\n&lt;NA&gt;\n\n\n375426\n610566\n&lt;NA&gt;\n\n\n\n\n375427 rows √ó 2 columns\n\n\n\nAs you can see, users who were not active in 2017-11 will have null values in their active_in_november column. This is because we need to make sure that all rows from the left-side table (dec3_active_tbl) are there. Lastly, we can find the percentage by a simple aggregation. This is how you calculate percentage of returning users.\n\n\nCode\nquery = f\"\"\"\nselect setseed(0.112);\n\nwith november_active_tbl as (\nselect\n user_id\n ,1 as active_in_november\nfrom transaction_tbl_x\nwhere year_month = '2017-11'\ngroup by 1,2\n),\n\ndec3_active_tbl as (\nselect\n user_id\nfrom transaction_tbl_x\nwhere event_date = '2017-12-03'\ngroup by 1\n)\n\nselect\n avg(coalesce(active_in_november,0)) as percent_active_last_month\nfrom dec3_active_tbl dec3\nleft join november_active_tbl nov\non dec3.user_id = nov.user_id;\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\npercent_active_last_month\n\n\n\n\n0\n0.824562\n\n\n\n\n\n\n\n\n\nExample: What is the daily contribution to its total monthly sales, expressed as percentage?\ninner join is used when you only want rows where the on conditions are satisfied for both tables. In this case, we know that year_month, the key we are joining on, exists in both daily and monthly sales tables, so we can use inner join without fear of losing information. inner join has the best performance than left join so prioritize it if you can, especially if you are working with huge tables.\nOne sneaky thing you can do is that technically you can enter a condition only based on one table in the on clause such as daily_sales &gt; 0 below. It will have the same performance as when you do it on where clause.\n\n\nCode\npd.set_option('display.max_rows', 10) \n\nquery = f\"\"\"\nselect setseed(0.112);\n\nwith monthly_sales_tbl as (\nselect\n year_month\n ,sum(sales) as monthly_sales\nfrom transaction_tbl_x\ngroup by 1\nhaving sum(sales)&gt;0\n),\n\ndaily_sales_tbl as (\nselect\n year_month\n ,event_date\n ,sum(sales) as daily_sales\nfrom transaction_tbl_x\ngroup by 1,2\n)\n\nselect\n daily.year_month\n ,event_date\n ,daily_sales / monthly_sales as percentage_of_monthly_sales\nfrom daily_sales_tbl daily\ninner join monthly_sales_tbl monthly\non daily.year_month = monthly.year_month\n and daily_sales &gt; 0\norder by event_date;\n\"\"\"\n\nexecute_query(query)\n\n\n\n\n\n\n\n\n\nyear_month\nevent_date\npercentage_of_monthly_sales\n\n\n\n\n0\n2017-11\n2017-11-25\n0.149288\n\n\n1\n2017-11\n2017-11-26\n0.162279\n\n\n2\n2017-11\n2017-11-27\n0.177516\n\n\n3\n2017-11\n2017-11-28\n0.165055\n\n\n4\n2017-11\n2017-11-29\n0.171182\n\n\n5\n2017-11\n2017-11-30\n0.176354\n\n\n6\n2017-12\n2017-12-01\n0.284772\n\n\n7\n2017-12\n2017-12-02\n0.352953\n\n\n8\n2017-12\n2017-12-03\n0.346453\n\n\n9\n2017-12\n2017-12-04\n0.016650"
  },
  {
    "objectID": "posts/sql_almost/index.html#tribal-knowledge",
    "href": "posts/sql_almost/index.html#tribal-knowledge",
    "title": "(Almost) All SQL You Need",
    "section": "9. Tribal Knowledge",
    "text": "9. Tribal Knowledge\nWe have now gone through SQL techniques to accomplish most tasks as a SQL monkey. I would like to close with some tips and tricks I have learned over the years:\n\nGet your hands dirty. Whether it is the exercise or your own data. Get out there and make it happen!\nALWAYS SPECIFY THE PARTITIONS YOU NEED IN THE WHERE CLAUSE, and use the right data types.\nSanity check for data quality issues, namely duplicates, missing values, improbable values, and data types. Make sure values in columns you care about are distributed in a reasonable manner.\nWork with assumptions and experiments. Have a set of clear hypotheses about what you are trying to learn/do, compose the query, run it and record the results. Work incrementally and not all at once; lest you will regret it during the debugging process. Do not just randomly write queries and hope for the best. The rabbit hole is too deep.\nPerformance is king. Always pick a pattern that results in better performance when possible. For instance, inner join over left join, union all over union, do not use distinct if values are already unique, and so on.\nQuery code styling must be readable and consistent. In this day and age where a coding assistant can fix your code styling in a few seconds, the best code styling for your query is the one that is most readable to your team. Whether it is tab vs space identation, leading vs trailing commas, with vs subquery vs view, capitalized vs uncapitalized keywords, just pick one style and stick with it.\nWhen in doubt explain. Most query engines will show you how it plans to execute your queries. If you are a beginner, this might not be extremely helpful, but at least you can catch some simple things like if the partitions you specified are being used, is the engine making some unncessary data type conversion, which part of the process takes the moast time and so on. explain will only show you the plan but explain analyze will execute the query then tell you how it went.\n\n\n\nCode\nquery = \"\"\"\nexplain analyze\nselect * from transaction_tbl\nwhere item_id = 2067266\n and category_id = 4339722\n\"\"\"\n\nprint(execute_query(query)['explain_value'][0])\n\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ\n‚îÇ‚îÇ    Query Profiling Information    ‚îÇ‚îÇ\n‚îÇ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n explain analyze select * from transaction_tbl where item_id = 2067266  and category_id = 4339722 \n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ\n‚îÇ‚îÇ              Total Time: 0.0016s             ‚îÇ‚îÇ\n‚îÇ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ           QUERY           ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ      EXPLAIN_ANALYZE      ‚îÇ\n‚îÇ    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ   ‚îÇ\n‚îÇ           0 Rows          ‚îÇ\n‚îÇ          (0.00s)          ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ         PROJECTION        ‚îÇ\n‚îÇ    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ   ‚îÇ\n‚îÇ          user_id          ‚îÇ\n‚îÇ          item_id          ‚îÇ\n‚îÇ         timestamp         ‚îÇ\n‚îÇ       behavior_type       ‚îÇ\n‚îÇ        category_id        ‚îÇ\n‚îÇ                           ‚îÇ\n‚îÇ           7 Rows          ‚îÇ\n‚îÇ          (0.00s)          ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ         TABLE_SCAN        ‚îÇ\n‚îÇ    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ   ‚îÇ\n‚îÇ           Table:          ‚îÇ\n‚îÇ      transaction_tbl      ‚îÇ\n‚îÇ                           ‚îÇ\n‚îÇ   Type: Sequential Scan   ‚îÇ\n‚îÇ                           ‚îÇ\n‚îÇ        Projections:       ‚îÇ\n‚îÇ          item_id          ‚îÇ\n‚îÇ        category_id        ‚îÇ\n‚îÇ          user_id          ‚îÇ\n‚îÇ         timestamp         ‚îÇ\n‚îÇ       behavior_type       ‚îÇ\n‚îÇ                           ‚îÇ\n‚îÇ          Filters:         ‚îÇ\n‚îÇ      item_id=2067266      ‚îÇ\n‚îÇ    category_id=4339722    ‚îÇ\n‚îÇ                           ‚îÇ\n‚îÇ           7 Rows          ‚îÇ\n‚îÇ          (0.00s)          ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "chariblog - technical writings in applied science",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nAI Builders Survey 2025: What Five Years of Teaching Kids to Build Good AI Looks Like\n\n\n\nai-builders\n\neducation\n\nsurvey\n\n\n\n\n\n\n\n\n\n2025-07-21\n\n\ncstorm125\n\n\n\n\n\n\n\n\n\n\n\n\n(Almost) All SQL You Need\n\n\n\nsql\n\ntutorial\n\n\n\n\n\n\n\n\n\n2025-06-29\n\n\ncstorm125\n\n\n\n\n\n\n\n\n\n\n\n\nPredict How Much A Customer Will Spend\n\n\n\nretail\n\nzero-inflated\n\nlong/fat-tailed\n\nhurdle\n\n\n\n\n\n\n\n\n\n2024-11-25\n\n\ncstorm125\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Have a Robustly Interesting Career in Data Science\n\n\n\ncareer\n\n\n\n\n\n\n\n\n\n2024-11-23\n\n\ncstorm125\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Charin and I am a senior applied scientist at Amazon. My main focus is on estimating heterogeneity effects for customer targeting and personalization, using causal inference techniques and counterfactuals generated by large language models. This is a collection of technical writings I find useful."
  },
  {
    "objectID": "posts/sales_prediction/index.html",
    "href": "posts/sales_prediction/index.html",
    "title": "Predict How Much A Customer Will Spend",
    "section": "",
    "text": "I have spent nearly a decade as a data scientist in the retail sector, but I have been approaching customer spend predictions the wrong way until I attended Gregory M. Duncan‚Äôs lecture. Accurately predicting how much an individual customer will spend in the next X days enables key retail use cases such as personalized promotion (determine X in Buy-X-Get-Y), customer targeting for upselling (which customers have higher purchasing power), and early churn detection (customers do not spend as much as they should). What makes this problem particularly difficult is because the distribution of customer spending is both zero-inflated and long/fat-tailed. Intuitively, most customers who visit your store are not going to make a purchase and among those who do, there will be some super customers who purchase an outrageous amount more than the average customer. Some parametric models allow for zero-inflated outcomes such as Poisson, negative binomial, Conway-Maxwell-Poisson; however, they do not handle the long/fat-tailed explicitly. Even for non-parametric models such as decision tree ensembles, more resources (trees and splits) will be dedicated to separating zeros and handling outliers; this could lead to deterioration in performance. Using the real-world dataset UCI Online Retail, we will compare the performance of common approaches namely naive baseline regression, regression on winsorized outcome, regression on log-plus-one-transformed outcome to what Duncan suggested: hurdle model with Duan‚Äôs method. We will demonstrate why this approach outperforms the others in most evaluation metrics and why it might not in some.\nCode\nimport pandas as pd\nimport numpy as np\nimport random\nfrom ucimlrepo import fetch_ucirepo \nimport boto3\nimport json\nfrom tqdm.auto import tqdm\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom autogluon.tabular import TabularDataset, TabularPredictor\nimport seaborn as sns\n\n\nfrom sklearn.metrics import (\n    mean_squared_error, mean_absolute_error, r2_score, median_absolute_error,\n    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n)\nfrom scipy.stats import pearsonr, spearmanr, wasserstein_distance\nfrom statsmodels.stats.diagnostic import het_white\n\ndef calculate_regression_metrics(y_true, y_pred):\n    return {\n        'root_mean_squared_error': np.sqrt(mean_squared_error(y_true, y_pred)),\n        'mean_squared_error': mean_squared_error(y_true, y_pred),\n        'mean_absolute_error': mean_absolute_error(y_true, y_pred),\n        'r2': r2_score(y_true, y_pred),\n        'pearsonr': pearsonr(y_true, y_pred)[0],\n        'spearmanr': spearmanr(y_true, y_pred)[0],\n        'median_absolute_error': median_absolute_error(y_true, y_pred),\n        'earths_mover_distance': wasserstein_distance(y_true, y_pred)\n    }\n\ndef caluclate_classification_metrics(y_true, y_pred):\n    return {\n        'accuracy': accuracy_score(y_true, y_pred),\n        'precision': precision_score(y_true, y_pred, average='weighted'),\n        'recall': recall_score(y_true, y_pred, average='weighted'),\n        'f1_score': f1_score(y_true, y_pred, average='weighted'),\n        'confusion_matrix': confusion_matrix(y_true, y_pred)\n    }\n\ndef string_to_yearmon(date):\n    date = date.split()\n    date = date[0].split('/') + date[1].split(':')\n    date = date[2] + '-' + date[0].zfill(2) #+ '-' + date[1].zfill(2) + ' ' + date[3].zfill(2) + ':' + date[4].zfill(2)\n    return date\n\ndef call_llama(system_prompt, input):\n    template = f\"\"\"&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;{system_prompt}&lt;&lt;/SYS&gt;&gt;{input}[/INST]\"\"\"\n    client = boto3.client(service_name='bedrock-runtime',region_name='us-west-2')\n    body = json.dumps({\n        \"prompt\": template,\n        \"temperature\": 0.,\n        \"top_p\": 0.9,\n        \"max_gen_len\": 2048,\n    })\n    response = client.invoke_model(\n        body=body,\n        modelId='us.meta.llama3-2-90b-instruct-v1:0',\n        accept='application/json',\n        contentType='application/json'\n    )\n    response_body = json.loads(response['body'].read())\n    return response_body\n\ndef call_claude(system_prompt, input):\n    client = boto3.client(service_name='bedrock-runtime',region_name='us-west-2')\n    body=json.dumps(\n        {\n            \"anthropic_version\": \"bedrock-2023-05-31\",\n            \"max_tokens\": 2048,\n            \"messages\": [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": system_prompt + '\\n' + input,\n                    }\n                    ]\n                }\n                ]\n        }  \n    )  \n\n    \n    response = client.invoke_model(body=body, \n                                   modelId='anthropic.claude-3-5-sonnet-20241022-v2:0',\n                                   contentType='application/json',\n                                   accept='application/json')\n    response_body = json.loads(response.get('body').read())\n   \n    return response_body"
  },
  {
    "objectID": "posts/sales_prediction/index.html#this-is-not-a-drill-real-world-datasets-meticulous-feature-engineering-state-of-the-art-automl",
    "href": "posts/sales_prediction/index.html#this-is-not-a-drill-real-world-datasets-meticulous-feature-engineering-state-of-the-art-automl",
    "title": "Predict How Much A Customer Will Spend",
    "section": "This Is Not a Drill: Real-world Datasets, Meticulous Feature Engineering, State-of-the-art AutoML",
    "text": "This Is Not a Drill: Real-world Datasets, Meticulous Feature Engineering, State-of-the-art AutoML\nTo make this exercise as realistic as possible, we will use a real-world dataset (as opposed to a simulated one), perform as much feature engineering as we would in a real-world setting, and employ the best AutoML solution the market has to offer in AutoGluon.\n\n\nCode\nonline_retail = fetch_ucirepo(id=352) \ntransaction_df = online_retail['data']['original']\noriginal_nb = transaction_df.shape[0]\n\n#create yearmon for train-valid split\ntransaction_df['yearmon'] = transaction_df.InvoiceDate.map(string_to_yearmon)\n\n#get rid of transactions without cid\ntransaction_df = transaction_df[~transaction_df.CustomerID.isna()].reset_index(drop=True)\nhas_cid_nb = transaction_df.shape[0]\n\n#fill in unknown descriptions\ntransaction_df.Description = transaction_df.Description.fillna('UNKNOWN')\n\n#convert customer id to string\ntransaction_df['CustomerID'] = transaction_df['CustomerID'].map(lambda x: str(int(x)))\n\n#simplify by filtering unit price and quantity to be non-zero (get rid of discounts, cancellations, etc)\ntransaction_df = transaction_df[(transaction_df.UnitPrice&gt;0)&\\\n                                (transaction_df.Quantity&gt;0)].reset_index(drop=True)\nhas_sales_nb = transaction_df.shape[0]\n\n#add sales\ntransaction_df['Sales'] = transaction_df.UnitPrice * transaction_df.Quantity\n\n\nWe use the UCI Online Retail dataset, which contain transactions from a UK-based, non-store online retail from 2010-12 and 2011-12. We perform the following data processing:\n\nRemove transactions without CustomerID; from 541,909 to 406,829 transactions\nFilter out transactions where either UnitPrice or Quantity is less than zero; from 406,829 to 397,884 transactions\nFill in missing product Description with value UNKNOWN.\n\n\n\nCode\nprint(transaction_df.shape)\ntransaction_df.sample(5)\n\n\n(397884, 10)\n\n\n\n\n\n\n\n\n\nInvoiceNo\nStockCode\nDescription\nQuantity\nInvoiceDate\nUnitPrice\nCustomerID\nCountry\nyearmon\nSales\n\n\n\n\n276682\n570008\n22598\nCHRISTMAS MUSICAL ZINC TREE\n12\n10/7/2011 9:30\n0.85\n13359\nUnited Kingdom\n2011-10\n10.20\n\n\n2894\n536746\n22604\nSET OF 4 NAPKIN CHARMS CUTLERY\n6\n12/2/2010 13:39\n2.55\n16510\nUnited Kingdom\n2010-12\n15.30\n\n\n238815\n566361\n85123A\nWHITE HANGING HEART T-LIGHT HOLDER\n12\n9/12/2011 12:32\n2.95\n15253\nUnited Kingdom\n2011-09\n35.40\n\n\n72407\n545830\n47590B\nPINK HAPPY BIRTHDAY BUNTING\n3\n3/7/2011 13:10\n5.45\n17634\nUnited Kingdom\n2011-03\n16.35\n\n\n78138\n546538\n15044A\nPINK PAPER PARASOL\n6\n3/14/2011 14:51\n2.95\n16327\nUnited Kingdom\n2011-03\n17.70\n\n\n\n\n\n\n\nWe formulate the problem as predicting the sales (TargetSales) during Q4 2011 for each customers who bought at least one item during Q1-Q3 2011. Note that we are interested in predicting the spend per customer as accurately as possible; this is common for marketing use cases such as determining what spend threshold to give each customer in a promotion, targeting customers for upselling, or detecting early signs of churns. It is notably different from predicting total spend of all customers during a time period, which usually requires a different approach.\n\n\nCode\nfeature_period = {'start': '2011-01', 'end': '2011-09'}\noutcome_period = {'start': '2011-10', 'end': '2011-12'}\n\nfeature_transaction = transaction_df[(transaction_df.yearmon&gt;=feature_period['start'])&\\\n                                      (transaction_df.yearmon&lt;=feature_period['end'])]\noutcome_transaction = transaction_df[(transaction_df.yearmon&gt;=outcome_period['start'])&\\\n                                      (transaction_df.yearmon&lt;=outcome_period['end'])]\n\n#aggregate sales during outcome period\noutcome_sales = outcome_transaction.groupby('CustomerID').Sales.sum().reset_index()\n\n#aggregate sales during feature period\nfeature_sales = feature_transaction.groupby('CustomerID').Sales.sum().reset_index()\n\n#merge to get TargetSales including those who spent during feature period but not during outcome (zeroes)\noutcome_df = feature_sales[['CustomerID']].merge(outcome_sales, on='CustomerID', how='left')\noutcome_df['Sales'] = outcome_df['Sales'].fillna(0)\noutcome_df.columns = ['CustomerID', 'TargetSales']\n\n\nWe transform the transaction dataset into a customer-level dataset where we calculate features using transactions between 2011-01 to 2011-09 and outcome using transactions between 2011-10 to 2011-12, summing Quantity times UnitPrice. We left-join the customers in feature set to outcome set. This will result in the zero-inflated nature of the outcome as not all customers will come back in Q4. The distribution of non-zero sales is naturally long/fat-tailed with a few customers having extraordinarily high amount of sales in Q4. This resulted in a customer-level dataset with 3,438 customers.\n\n\nCode\n#confirm zero-inflated, long/fat-tailed\noutcome_df.TargetSales.describe(percentiles=[i/10 for i in range(10)])\n\n\ncount      3438.000000\nmean        666.245829\nstd        4016.843037\nmin           0.000000\n0%            0.000000\n10%           0.000000\n20%           0.000000\n30%           0.000000\n40%           0.000000\n50%         102.005000\n60%         263.006000\n70%         425.790000\n80%         705.878000\n90%        1273.611000\nmax      168469.600000\nName: TargetSales, dtype: float64\n\n\n\n\nCode\n#confirm zero-inflated, long/fat-tailed\noutcome_df[outcome_df.TargetSales&lt;=10_000].TargetSales.hist(bins=100)\n\n\n\n\n\n\n\n\n\nWe represent a customer using traditional RFM features namely recency of purchase, purchase days, total sales, number of distinct products purchased, number of distinct category purchased, customer tenure within 2011, average purchase frequency, average purchase value, and percentage of purchase across all 9 categories. This is based on data from Q1-Q3 2011.\nSince the UCI Online Retail dataset does not have a category but only contains descriptions over 3,000 items, we use LLaMA 3.2 90B to infer categories based on randomly selected 1,000 descriptions. This is to make the category preference representation for each customer, which is more tractable than including features about all 3,548 items. After that, we use Claude 3.5 v2 to label a category for each description as it performs structured output a little more reliably. The categories are:\n\nHome Decor\nKitchen and Dining\nFashion Accessories\nStationary and Gifts\nToys and Games\nSeasonal and Holiday\nPersonal Care and Wellness\nOutdoor and Garden\nOthers\n\n\n\nCode\ndescriptions = feature_transaction.Description.unique().tolist()\nprint(descriptions[:5])\n\n#randomize descriptions with seed 112 to get which categories we should use\nnp.random.seed(112)\nrandom_descriptions = np.random.choice(descriptions, 1000, replace=False)\n\nres = call_llama(\n    'You are a product categorization assistant at a retail website.',\n    'Given the following product descriptions, come up with a few product categories they should be classified into.'+'\\n'.join(random_descriptions)\n)\n\ncategories = [\n    'Home Decor',\n    'Kitchen and Dining',\n    'Fashion Accessories',\n    'Stationary and Gifts',\n    'Toys and Games',\n    'Seasonal and Holiday',\n    'Personal Care and Wellness',\n    'Outdoor and Garden',   \n]\n\nprint(res['generation'])\n\n\n['JUMBO BAG PINK POLKADOT', 'BLUE POLKADOT WRAP', 'RED RETROSPOT WRAP ', 'RECYCLING BAG RETROSPOT ', 'RED RETROSPOT SHOPPER BAG']\n &lt;&lt;SYS&gt;&gt;Based on the product descriptions, I would categorize them into the following categories:\n\n1. Home Decor:\n    * Wall art\n    * Decorative items (e.g. vases, figurines, etc.)\n    * Lighting (e.g. candles, lanterns, etc.)\n    * Textiles (e.g. throw pillows, blankets, etc.)\n2. Kitchen and Dining:\n    * Cookware and utensils\n    * Tableware (e.g. plates, cups, etc.)\n    * Kitchen decor (e.g. signs, magnets, etc.)\n    * Food and drink items (e.g. tea, coffee, etc.)\n3. Fashion and Accessories:\n    * Jewelry (e.g. necklaces, earrings, etc.)\n    * Handbags and wallets\n    * Clothing and accessories (e.g. scarves, hats, etc.)\n    * Beauty and personal care items (e.g. cosmetics, skincare, etc.)\n4. Stationery and Gifts:\n    * Greeting cards\n    * Gift wrap and bags\n    * Stationery (e.g. notebooks, pens, etc.)\n    * Gift items (e.g. mugs, keychains, etc.)\n5. Toys and Games:\n    * Toys (e.g. stuffed animals, puzzles, etc.)\n    * Games and puzzles\n    * Outdoor toys and games\n6. Seasonal and Holiday:\n    * Christmas decorations and gifts\n    * Easter decorations and gifts\n    * Halloween decorations and gifts\n    * Other seasonal and holiday items\n7. Office and School:\n    * Office supplies (e.g. pens, paper, etc.)\n    * School supplies (e.g. backpacks, lunchboxes, etc.)\n    * Desk accessories (e.g. paperweights, etc.)\n8. Garden and Outdoor:\n    * Gardening tools and supplies\n    * Outdoor decor (e.g. planters, etc.)\n    * Patio and outdoor furniture\n9. Baby and Kids:\n    * Baby clothing and accessories\n    * Kids' clothing and accessories\n    * Toys and games for kids\n    * Nursery decor and furniture\n\nNote that some products may fit into multiple categories, but I've tried to categorize them based on their primary function or theme.\n\n\n\n\nCode\n#loop through descriptions in batches of batch_size\nres_texts = []\nbatch_size = 100\nfor i in tqdm(range(0, len(descriptions), batch_size)):\n    batch = descriptions[i:i+batch_size]\n    d = \"\\n\".join(batch)\n    inp = f'''Categorize the following product descriptions into {\", \".join(categories)} or Others, if they do not fall into any. \nOnly answer in the following format:\n\n\"product description of product #1\"|\"product category classified into\"\n\"product description of product #2\"|\"product category classified into\"\n...\n\"product description of product #n\"|\"product category classified into\"\n\nHere are the product descriptions:\n{d}\n'''\n    while True:\n        res = call_claude('You are a product categorizer at a retail website', inp)\n        # if res['generation_token_count'] &gt; 1: #for llama\n        if res['usage']['output_tokens'] &gt; 1:\n            break\n        else:\n            print('Retrying...')\n            time.sleep(2)\n    res_text = res['content'][0]['text'].strip().split('\\n')\n        #for llama\n        # .replace('[SYS]','').replace('&lt;&lt;SYS&gt;&gt;','')\\\n        # .replace('[/SYS]','').replace('&lt;&lt;/SYS&gt;&gt;','')\\\n    if res_text!='':\n        res_texts.extend(res_text)\n\nwith open('../../data/sales_prediction/product_description_category.csv','w') as f:\n    f.write('\"product_description\"|\"category\"\\n')\n    for i in res_texts:\n        f.write(f'{i}\\n')\n\n\nHere is the share of product descriptions in each annotated category:\n\n\nCode\nproduct_description_category = pd.read_csv('../../data/sales_prediction/product_description_category.csv',\n                                           sep='|')\n\n#clean product_description\nproduct_description_category['Description'] = descriptions\nproduct_description_category.category.value_counts(normalize=True)\n\n\ncategory\nHome Decor                    0.328636\nKitchen and Dining            0.195885\nFashion Accessories           0.138670\nStationary and Gifts          0.116122\nSeasonal and Holiday          0.087373\nPersonal Care and Wellness    0.047351\nToys and Games                0.045096\nOutdoor and Garden            0.032976\nOthers                        0.007892\nName: proportion, dtype: float64\n\n\nWe merge the RFM features with preference features, that is share of sales in each category for every customer, then the outcome TargetSales to create the universe set for the problem.\n\n\nCode\nfeature_transaction_cat = feature_transaction.merge(product_description_category,\n                                                    how='inner',\n                                                    on = 'Description',)\nfeature_transaction.shape, feature_transaction_cat.shape\n\n#convert invoice date to datetime\nfeature_transaction_cat['InvoiceDate'] = pd.to_datetime(feature_transaction_cat['InvoiceDate'])\n\n# last date in feature set\ncurrent_date = feature_transaction_cat['InvoiceDate'].max()\n\n#rfm\ncustomer_features = feature_transaction_cat.groupby('CustomerID').agg({\n    'InvoiceDate': [\n        ('recency', lambda x: (current_date - x.max()).days),\n        ('first_purchase_date', 'min'),\n        ('purchase_day', 'nunique'),\n    ],\n    'InvoiceNo': [('nb_invoice', 'nunique')],\n    'Sales': [\n        ('total_sales', 'sum')\n    ],\n    'StockCode': [('nb_product', 'nunique')],\n    'category': [('nb_category', 'nunique')]\n}).reset_index()\n\n# Flatten column names\ncustomer_features.columns = [\n    'CustomerID',\n    'recency',\n    'first_purchase_date',\n    'purchase_day',\n    'nb_invoice',\n    'total_sales',\n    'nb_product',\n    'nb_category'\n]\n\ncustomer_features['customer_lifetime'] = (current_date - customer_features['first_purchase_date']).dt.days\ncustomer_features['avg_purchase_frequency'] = customer_features['customer_lifetime'] / customer_features['purchase_day']\ncustomer_features['avg_purchase_value'] = customer_features['total_sales'] / customer_features['purchase_day']\n\n#category preference\ncategory_sales = feature_transaction_cat.pivot_table(\n    values='Sales', \n    index='CustomerID', \n    columns='category', \n    aggfunc='sum', \n    fill_value=0\n)\ncategory_sales.columns = [i.lower().replace(' ','_') for i in category_sales.columns]\ncustomer_features = customer_features.merge(category_sales, on='CustomerID', how='left')\n\ntotal_sales = customer_features['total_sales']\nfor col in category_sales.columns:\n    percentage_col = f'per_{col}'\n    customer_features[percentage_col] = customer_features[col] / total_sales\n\nselected_features = [\n 'recency',\n 'purchase_day',\n 'total_sales',\n 'nb_product',\n 'nb_category',\n 'customer_lifetime',\n 'avg_purchase_frequency',\n 'avg_purchase_value',\n 'per_fashion_accessories',\n 'per_home_decor',\n 'per_kitchen_and_dining',\n 'per_others',\n 'per_outdoor_and_garden',\n 'per_personal_care_and_wellness',\n 'per_seasonal_and_holiday',\n 'per_stationary_and_gifts',\n 'per_toys_and_games']\n\noutcome_variable = 'TargetSales'\n\ncustomer_features = customer_features[[ 'CustomerID']+selected_features]\ndf = outcome_df.merge(customer_features, on='CustomerID').drop('CustomerID', axis=1)\nprint(df.shape)\ndf.sample(5)\n\n\n(3438, 18)\n\n\n\n\n\n\n\n\n\nTargetSales\nrecency\npurchase_day\ntotal_sales\nnb_product\nnb_category\ncustomer_lifetime\navg_purchase_frequency\navg_purchase_value\nper_fashion_accessories\nper_home_decor\nper_kitchen_and_dining\nper_others\nper_outdoor_and_garden\nper_personal_care_and_wellness\nper_seasonal_and_holiday\nper_stationary_and_gifts\nper_toys_and_games\n\n\n\n\n2606\n0.00\n53\n2\n597.48\n138\n8\n184\n92.000000\n298.740\n0.079383\n0.433973\n0.343710\n0.003465\n0.000000\n0.041357\n0.016570\n0.056688\n0.024854\n\n\n196\n0.00\n78\n2\n2209.85\n37\n6\n226\n113.000000\n1104.925\n0.030771\n0.275245\n0.628549\n0.000000\n0.021178\n0.022535\n0.000000\n0.021721\n0.000000\n\n\n2900\n3893.79\n10\n6\n4099.11\n78\n9\n172\n28.666667\n683.185\n0.003879\n0.761507\n0.104540\n0.003879\n0.012442\n0.014015\n0.051597\n0.043312\n0.004830\n\n\n2187\n0.00\n227\n1\n122.40\n1\n1\n227\n227.000000\n122.400\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n322\n0.00\n68\n1\n147.12\n3\n2\n68\n68.000000\n147.120\n0.881729\n0.118271\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n\n\n\n\n\nUnivariate correlation expectedly pinpoints total_sales in during Q1-Q3 2011 as the most predictive feature; however, we can see that it is still not very predictive. This shows that the problem is not a trivial one.\n\n\nCode\nprint(df[['TargetSales','total_sales']].corr())\n\n#target and most predictive variable\ndf[df.TargetSales&lt;=25_000].plot.scatter(x='TargetSales',y='total_sales')\n\n\n             TargetSales  total_sales\nTargetSales     1.000000     0.558558\ntotal_sales     0.558558     1.000000\n\n\n\n\n\n\n\n\n\nWe randomly split the dataset into train and test sets at 80/20 ratio. We also confirm the distribution of TargetSales is similar across percentiles between train and test and only different at the upper end.\n\n\nCode\n#split into train-valid sets\ntrain_df, test_df = train_test_split(df,\n                                      test_size=0.2, \n                                      random_state=112)\npd.concat([train_df.TargetSales.describe(percentiles=[i/10 for i in range(10)]).reset_index(),\ntest_df.TargetSales.describe(percentiles=[i/10 for i in range(10)]).reset_index(),], axis=1)\n\n\n\n\n\n\n\n\n\nindex\nTargetSales\nindex\nTargetSales\n\n\n\n\n0\ncount\n2750.000000\ncount\n688.000000\n\n\n1\nmean\n642.650436\nmean\n760.558808\n\n\n2\nstd\n4015.305436\nstd\n4024.524400\n\n\n3\nmin\n0.000000\nmin\n0.000000\n\n\n4\n0%\n0.000000\n0%\n0.000000\n\n\n5\n10%\n0.000000\n10%\n0.000000\n\n\n6\n20%\n0.000000\n20%\n0.000000\n\n\n7\n30%\n0.000000\n30%\n0.000000\n\n\n8\n40%\n0.000000\n40%\n0.000000\n\n\n9\n50%\n91.350000\n50%\n113.575000\n\n\n10\n60%\n260.308000\n60%\n277.836000\n\n\n11\n70%\n426.878000\n70%\n418.187000\n\n\n12\n80%\n694.164000\n80%\n759.582000\n\n\n13\n90%\n1272.997000\n90%\n1255.670000\n\n\n14\nmax\n168469.600000\nmax\n77099.380000"
  },
  {
    "objectID": "posts/sales_prediction/index.html#naive-baseline-regression",
    "href": "posts/sales_prediction/index.html#naive-baseline-regression",
    "title": "Predict How Much A Customer Will Spend",
    "section": "Naive Baseline Regression",
    "text": "Naive Baseline Regression\nThe most naive solution is to simply predict TargetSales based on the features. We use a stacked ensemble of LightGBM, CatBoost, XGBoost, Random Forest and Extra Trees via AutoGluon. We train with good_quality preset, stated to be ‚ÄúStronger than any other AutoML Framework‚Äù, for speedy training and inference but feel free to try more performant options. We exclude the neural-network models as they require further preprocessing of the features. We use an industry-grade, non-parametric model to be as close to a real use case as possible and make a point that the methodology works not only in a toy-dataset setup.\n\n\nCode\npreset = 'good_quality'\n\npredictor = TabularPredictor(label='TargetSales').fit(train_df[selected_features + ['TargetSales']], \n                                                      presets=preset,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\ntest_df['pred_baseline'] = predictor.predict(test_df[selected_features])\n\n\n\n\nCode\nmetric_baseline = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_baseline'])\nmetric_baseline['model'] = 'baseline'\nmetric_baseline\n\n\n{'root_mean_squared_error': 3162.478744240967,\n 'mean_squared_error': 10001271.807775924,\n 'mean_absolute_error': 715.6442657130541,\n 'r2': 0.3816166296854987,\n 'pearsonr': 0.6190719671013133,\n 'spearmanr': 0.47008461549340863,\n 'median_absolute_error': 232.98208312988282,\n 'earths_mover_distance': 287.77728784026124,\n 'model': 'baseline'}"
  },
  {
    "objectID": "posts/sales_prediction/index.html#regression-on-winsorized-outcome",
    "href": "posts/sales_prediction/index.html#regression-on-winsorized-outcome",
    "title": "Predict How Much A Customer Will Spend",
    "section": "Regression on Winsorized Outcome",
    "text": "Regression on Winsorized Outcome\n\n\nCode\noutlier_per = 0.99\noutlier_cap_train = train_df['TargetSales'].quantile(outlier_per)\n\n\nAn alternative approach to deal with long/fat-tailed outcome is to train on a winsorized outcome. In our case, we cap the outlier at 99.0% or TargetSales equals 7,180.81. While this solves the long/fat-tailed issues, it does not deal with zero inflation and also introduce bias to the outcome. This leads to better performance when tested on the winsorized outcome, but not so much on the original outcome.\n\n\nCode\n#winsorize\ntrain_df['TargetSales_win'] = train_df['TargetSales'].map(lambda x: outlier_cap_train if x&gt; outlier_cap_train else x)\ntest_df['TargetSales_win'] = test_df['TargetSales'].map(lambda x: outlier_cap_train if x&gt; outlier_cap_train else x)\n\npredictor = TabularPredictor(label='TargetSales_win').fit(train_df[selected_features+['TargetSales_win']],\n                                                      presets=preset,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\n\ntest_df['pred_winsorized'] = predictor.predict(test_df[selected_features])\n\n\n\n\nCode\nmetric_winsorized = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_winsorized'])\nmetric_winsorized['model'] = 'winsorized'\nmetric_winsorized\n\n\n{'root_mean_squared_error': 3623.576377551195,\n 'mean_squared_error': 13130305.76394704,\n 'mean_absolute_error': 627.7880071099414,\n 'r2': 0.18814697894155963,\n 'pearsonr': 0.5757989413256978,\n 'spearmanr': 0.504301956183441,\n 'median_absolute_error': 219.62248107910156,\n 'earths_mover_distance': 432.1288432991232,\n 'model': 'winsorized'}"
  },
  {
    "objectID": "posts/sales_prediction/index.html#regression-on-log-plus-one-transformed-outcome",
    "href": "posts/sales_prediction/index.html#regression-on-log-plus-one-transformed-outcome",
    "title": "Predict How Much A Customer Will Spend",
    "section": "Regression on Log-plus-one-transformed Outcome",
    "text": "Regression on Log-plus-one-transformed Outcome\nLog transformation handles long/fat-tailed distribution and is especially useful for certain models since the transformed distribution is closer normal. However, it cannot handle zero-valued outcome and oftentimes scientists end up adding 1 to the outcome (so often that numpy even has a function for it). This not only introduces bias to the prediction, but also does not solve the zero-inflation as it becomes one-inflation instead.\n\n\nCode\n#log\ntrain_df['TargetSales_log1p'] = train_df['TargetSales'].map(np.log1p)\ntest_df['TargetSales_log1p'] = test_df['TargetSales'].map(np.log1p)\n\n#from zero-inflated to one-inflated\ntrain_df['TargetSales_log1p'].hist()\n\n\n\n\n\n\n\n\n\nWe can see that this is the best performing approach so far, which is one of the reasons why so many scientists end up going for this not-entirely-correct approach.\n\n\nCode\npredictor = TabularPredictor(label='TargetSales_log1p').fit(train_df[selected_features+['TargetSales_log1p']],\n                                                      presets=preset,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\n\ntest_df['pred_log1p'] = predictor.predict(test_df[selected_features])\ntest_df['pred_log1p_expm1'] = test_df['pred_log1p'].map(np.expm1)\n\n\n\n\nCode\nmetric_log1p = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_log1p_expm1'])\nmetric_log1p['model'] = 'log1p'\nmetric_log1p\n\n\n{'root_mean_squared_error': 3725.342295894091,\n 'mean_squared_error': 13878175.221577456,\n 'mean_absolute_error': 618.9768466651894,\n 'r2': 0.14190585634701047,\n 'pearsonr': 0.5817166874396966,\n 'spearmanr': 0.5338156315937898,\n 'median_absolute_error': 89.55495441784018,\n 'earths_mover_distance': 581.0494444960044,\n 'model': 'log1p'}"
  },
  {
    "objectID": "posts/sales_prediction/index.html#hurdle-model",
    "href": "posts/sales_prediction/index.html#hurdle-model",
    "title": "Predict How Much A Customer Will Spend",
    "section": "Hurdle Model",
    "text": "Hurdle Model\nHurdle model is a two-stage approach that handles zero inflation by first having a classification model to predict if the outcome is zero or not, then a regression model, trained only on examples with actual non-zero outcomes, to fit a log-transformed outcome. When retransforming the predictions from log to non-log numbers, we perform correction of underestimation using Duan‚Äôs method. During inference time, we multiply the predictions from the classification and corrected regression model.\n\n\nCode\ntrain_df['has_purchase'] = train_df.TargetSales.map(lambda x: 1 if x&gt;0 else 0)\ntest_df['has_purchase'] = test_df.TargetSales.map(lambda x: 1 if x&gt;0 else 0)\n\npredictor_cls = TabularPredictor(label='has_purchase').fit(train_df[selected_features+['has_purchase']],\n                                                      presets=preset,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\ntest_df['pred_binary'] = predictor_cls.predict(test_df[selected_features])\n\n\nFor our splits, 51.42% of train and 53.05% of test include customers with non-zero purchase outcome. As with all two-stage approaches, we need to make sure the intermediate model performs reasonably in classifying zero/non-zero outcomes.\n\n\nCode\ncaluclate_classification_metrics(test_df['has_purchase'], test_df['pred_binary'])\n\n\n{'accuracy': 0.6918604651162791,\n 'precision': 0.6941069004479309,\n 'recall': 0.6918604651162791,\n 'f1_score': 0.6921418829824787,\n 'confusion_matrix': array([[229,  94],\n        [118, 247]])}\n\n\n\n\nCode\ntrain_df_nonzero = train_df[train_df.has_purchase==1].reset_index(drop=True)\ntest_df_nonzero = test_df[test_df.has_purchase==1].reset_index(drop=True)\n\n#log\ntrain_df_nonzero['TargetSales_log'] = train_df_nonzero['TargetSales'].map(np.log)\ntest_df_nonzero['TargetSales_log'] = test_df_nonzero['TargetSales'].map(np.log)\n\n\nAfter that, we perform log-transformed regression on the examples with non-zero outcome (1,414 examples in train). Without the need to worry about ln(0) outcome, the regression is much more straightforward albeit with fewer examples to train on.\n\n\nCode\ntrain_df_nonzero['TargetSales_log'].hist()\n\n\n\n\n\n\n\n\n\n\n\nCode\npredictor_reg = TabularPredictor(label='TargetSales_log').fit(train_df_nonzero[selected_features+['TargetSales_log']],\n                                                      presets=preset,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\ntest_df_nonzero['pred_log'] = predictor_reg.predict(test_df_nonzero[selected_features])\ntest_df_nonzero['pred_log_exp'] = test_df_nonzero['pred_log'].map(np.exp)\n\ntest_df['pred_log'] = predictor_reg.predict(test_df[selected_features])\ntest_df['pred_log_exp'] = test_df['pred_log'].map(np.exp)\n\ntest_df['pred_hurdle'] = test_df.pred_binary * test_df.pred_log_exp\n\n\nFor inference, we combine the binary prediction (purchase/no purchase) from the classification model with the re-transformed (exponentialized) numerical prediction from the regression model by simply multiplying them together. As you can see, this approach yields the best performance so far and this is where I used to think everything has been accounted for.\n\n\nCode\nmetric_hurdle = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_hurdle'])\nmetric_hurdle['model'] = 'hurdle'\nmetric_hurdle\n\n\n{'root_mean_squared_error': 3171.760744960863,\n 'mean_squared_error': 10060066.22327469,\n 'mean_absolute_error': 584.9162934881963,\n 'r2': 0.3779813431428882,\n 'pearsonr': 0.6769697889999318,\n 'spearmanr': 0.5107083593715698,\n 'median_absolute_error': 199.1780137692856,\n 'earths_mover_distance': 286.381442541919,\n 'model': 'hurdle'}"
  },
  {
    "objectID": "posts/sales_prediction/index.html#but-wait-there-is-more„Éºenter-naihua-duan",
    "href": "posts/sales_prediction/index.html#but-wait-there-is-more„Éºenter-naihua-duan",
    "title": "Predict How Much A Customer Will Spend",
    "section": "But Wait, There Is More„ÉºEnter Naihua Duan",
    "text": "But Wait, There Is More„ÉºEnter Naihua Duan\nIn the previous section, we have blissfully assumed that we can freely log-transform and re-transform the outcome during training and inference without any bias. This is not the case as there is a small bias generated in the process due to the error term.\n\\[ln(y) = f(X) + \\epsilon\\]\nwhere\n\n\\(y\\) is actual outcome.\n\\(X\\) is the features.\n\\(f(.)\\) is a trained model.\n\\(\\epsilon\\) is the error term.\n\nwhen re-transforming\n\\[\n\\begin{align}\ny &= exp(ln(y)) \\\\\n&= exp(f(X) + \\epsilon ) \\\\\n&= exp(f(X)) \\cdot exp(\\epsilon) \\\\\nE[y] &= E[exp(f(X))] \\cdot E[exp(\\epsilon)]\n\\end{align}\n\\]\nThe average treatment affect (ATE; \\(E[y]\\)) is underestimated by \\(E[exp(\\epsilon)]\\). Naihua Duan (ÊÆµ‰πÉËèØ), a Taiwanese biostatistician, suggested a consistent estimator of \\(E[exp(\\epsilon)]\\) in his 1983 work as\n\\[\n\\begin{align}\n\\hat \\lambda &= E[exp(ln(y) - ln(\\hat y))]\n\\end{align}\n\\]\nwhere\n\n\\(\\hat \\lambda\\) is the Duan‚Äôs smearing estimator of the bias from re-transformation \\(E[exp(\\epsilon)]\\)\n\\(\\hat y\\) is the prediction aka \\(f(X)\\)\n\nFun Fact: If you assume Duan were a western name, you would have been \npronouncing the method's name incorrectly since it should be [tw√†n]'s \nmethod, NOT /dw…ën/'s method.\nBefore we proceed, the formulation of Duan‚Äôs smearing estimator assumes that estimates of error terms (residuals) for log predictions be independent and identically distributed. Since we are dealing with individual customers, independence can be assumed. However, if we look at the plot of residuals vs predicted log values (based on training set), we can see that they do not look particularly identically distributed.\n\n\nCode\n#plot residual and predicted log value\ntrain_df_nonzero['pred_log'] = predictor_reg.predict(train_df_nonzero[selected_features])\ntrain_df_nonzero['residual_log'] = (train_df_nonzero['pred_log'] - train_df_nonzero['TargetSales_log'])\n\n# Create the scatter plot\nsns.scatterplot(x='pred_log', y='residual_log', data=train_df_nonzero)\n\n# Add the Lowess smoothing line\nsns.regplot(x='pred_log', y='residual_log', data=train_df_nonzero, scatter_kws={'alpha': 0.5}, line_kws={'color': 'red'})\n\n\n\n\n\n\n\n\n\nAlthough note that White test does not reject the null hypothesis of the residuals being homoscedastic in reference to the features. This counterintuitive result might stem from the fact that White test is assuming linear or quadratic relationships between outcome and features while the residuals are derived from a stacked ensemble of decision trees.\n\n\nCode\nwhite_stat, white_p_value, _, _ = het_white(train_df_nonzero['residual_log'], \n                                            train_df_nonzero[selected_features])\nprint(f\"White Test Statistic: {white_stat}\")\nprint(f\"P-value: {white_p_value}\")\n\n\nWhite Test Statistic: 129.31318320644837\nP-value: 0.8761278601130765\n\n\nOur choice is to either trust the White test and pretend assume everything is fine; or trust our eyes and replace the non-zero regression model with one that produces iid residuals such as generalized least squares (GLS) with heteroscedasticity-robust standard errors. The tradeoff is that often models that produce homoscedastic residuals perform worse in terms of predictive power (see example of GLS implementation in Assumption on Indepedent and Identically Distributed Residuals section of the notebook).\nAssuming we trust the White test, we can easily derive Duan‚Äôs smearing estimator by taking mean of error between actual and predicted TargetSales in the training set.\n\n\nCode\ntrain_df_nonzero['pred_log'] = predictor_reg.predict(train_df_nonzero[selected_features])\ntrain_df_nonzero['pred_log_exp'] = train_df_nonzero['pred_log'].map(np.exp)\n\nsmearing_estimator = np.mean(np.exp(train_df_nonzero['TargetSales_log'] - train_df_nonzero['pred_log']))\nsmearing_estimator\n\n\n1.2280991653046711\n\n\nWe multiply this to the predictions of the hurdle model to correct the underestimation due to re-transformation bias.\n\n\nCode\ntest_df['pred_log_exp_corrected'] = test_df['pred_log_exp'] * smearing_estimator\ntest_df['pred_hurdle_corrected'] = test_df.pred_binary * test_df.pred_log_exp_corrected\n\nmetric_hurdle_corrected = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_hurdle_corrected'])\nmetric_hurdle_corrected['model'] = 'hurdle_corrected'\nmetric_hurdle_corrected\n\n\n{'root_mean_squared_error': 3055.3207868281233,\n 'mean_squared_error': 9334985.110424023,\n 'mean_absolute_error': 613.3946643257099,\n 'r2': 0.42281345159207295,\n 'pearsonr': 0.6769697889999318,\n 'spearmanr': 0.5107083593715698,\n 'median_absolute_error': 232.55557358084502,\n 'earths_mover_distance': 241.61839859133218,\n 'model': 'hurdle_corrected'}"
  },
  {
    "objectID": "posts/sales_prediction/index.html#the-eval-bar",
    "href": "posts/sales_prediction/index.html#the-eval-bar",
    "title": "Predict How Much A Customer Will Spend",
    "section": "The Eval Bar",
    "text": "The Eval Bar\nWe can see that the hurdle model with Duan‚Äôs correction performs best across majority of the metrics. We will now deep dive on metrics where it did not to understand the caveats when taking this approach.\n\n\nCode\nmetric_df = pd.DataFrame([metric_baseline,\n                       metric_winsorized,\n                       metric_log1p,\n                       metric_hurdle,\n                       metric_hurdle_corrected,])\n\nrank_df = metric_df.copy()\nfor col in metric_df.columns.tolist()[:-1]:\n    if col in ['r2', 'pearsonr', 'spearmanr']:\n        rank_df[f'{col}_rank'] = rank_df[col].rank(ascending=False)\n    else:\n        rank_df[f'{col}_rank'] = rank_df[col].rank(ascending=True)\nrank_df = rank_df.drop(metric_df.columns.tolist()[:-1], axis=1)\nrank_df['avg_rank'] = rank_df.iloc[:,1:].mean(axis=1)\nrank_df.transpose()\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nmodel\nbaseline\nwinsorized\nlog1p\nhurdle\nhurdle_corrected\n\n\nroot_mean_squared_error_rank\n2.0\n4.0\n5.0\n3.0\n1.0\n\n\nmean_squared_error_rank\n2.0\n4.0\n5.0\n3.0\n1.0\n\n\nmean_absolute_error_rank\n5.0\n4.0\n3.0\n1.0\n2.0\n\n\nr2_rank\n2.0\n4.0\n5.0\n3.0\n1.0\n\n\npearsonr_rank\n3.0\n5.0\n4.0\n1.5\n1.5\n\n\nspearmanr_rank\n5.0\n4.0\n1.0\n2.5\n2.5\n\n\nmedian_absolute_error_rank\n5.0\n3.0\n1.0\n2.0\n4.0\n\n\nearths_mover_distance_rank\n3.0\n4.0\n5.0\n2.0\n1.0\n\n\navg_rank\n3.375\n4.0\n3.625\n2.25\n1.75\n\n\n\n\n\n\n\n\n\nCode\nmetric_df.transpose()\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nroot_mean_squared_error\n3162.478744\n3623.576378\n3725.342296\n3171.760745\n3055.320787\n\n\nmean_squared_error\n10001271.807776\n13130305.763947\n13878175.221577\n10060066.223275\n9334985.110424\n\n\nmean_absolute_error\n715.644266\n627.788007\n618.976847\n584.916293\n613.394664\n\n\nr2\n0.381617\n0.188147\n0.141906\n0.377981\n0.422813\n\n\npearsonr\n0.619072\n0.575799\n0.581717\n0.67697\n0.67697\n\n\nspearmanr\n0.470085\n0.504302\n0.533816\n0.510708\n0.510708\n\n\nmedian_absolute_error\n232.982083\n219.622481\n89.554954\n199.178014\n232.555574\n\n\nearths_mover_distance\n287.777288\n432.128843\n581.049444\n286.381443\n241.618399\n\n\nmodel\nbaseline\nwinsorized\nlog1p\nhurdle\nhurdle_corrected\n\n\n\n\n\n\n\n\nWhy Duan‚Äôs Correction Results in Slightly Worse MAE?\nDuan‚Äôs method adjusts for underestimation from re-transformation of log outcome. This could lead to smaller extreme errors, but more frequent occurrences of less extreme ones. We verify this hypothesis by comparing mean absolute error before and after transformation for errors originally under and over 99th percentile. We confirm that is the case for our problem.\n\n\nCode\nerr_hurdle = (test_df['TargetSales'] - test_df['pred_hurdle']).abs()\nerr_hurdle_corrected = (test_df['TargetSales'] - test_df['pred_hurdle_corrected']).abs()\n\nprint('Distribution of errors for Hurdle model without correction')\nerr_hurdle.describe(percentiles=[.25, .5, .75, .9, .95, .99]) \n\n\nDistribution of errors for Hurdle model without correction\n\n\ncount      688.000000\nmean       584.916293\nstd       3119.628924\nmin          0.000000\n25%          0.000000\n50%        199.178014\n75%        475.603446\n90%        862.530026\n95%       1237.540954\n99%       6763.777844\nmax      55731.205996\ndtype: float64\n\n\n\n\nCode\nprint('Hurdle Model without correction')\nprint(f'Mean absolute error under 99th percentile: {err_hurdle[err_hurdle&lt;6763.777844].mean()}')\nprint(f'Mean absolute error over 99th percentile: {err_hurdle[err_hurdle&gt;6763.777844].mean()}')\n\nprint('Hurdle Model with correction')\nprint(f'Mean absolute error under 99th percentile: {err_hurdle_corrected[err_hurdle&lt;6763.777844].mean()}')\nprint(f'Mean absolute error over 99th percentile: {err_hurdle_corrected[err_hurdle&gt;6763.777844].mean()}')\n\n\nHurdle Model without correction\nMean absolute error under 99th percentile: 355.4918014848842\nMean absolute error over 99th percentile: 22904.641872667555\nHurdle Model with correction\nMean absolute error under 99th percentile: 392.7718802742851\nMean absolute error over 99th percentile: 22076.839798471465\n\n\n\n\nImportance of Classification Model\nThe overperformance of log-transform regression over both hurdle model approarches in Spearman‚Äôs rank correlation and median absolute error demonstrates the importance of a classification model. At first glance, it is perplexing since we have just spent a large portion of this article to justify that hurdle models handle zero inflation better and re-transformation without Duan‚Äôs method is biased. However, it becomes clear once you compare performance of the hurdle model with a classification model (f1 = 0.69) and a hypothetical, perfect classification model. Other metrics also improved but not nearly as drastic as MedAE and Spearman‚Äôs rank correlation.\n\n\nCode\ntest_df['pred_hurdle_corrected_perfect_cls'] = test_df.has_purchase * test_df.pred_log_exp_corrected\nmetric_hurdle_corrected_perfect_cls = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_hurdle_corrected_perfect_cls'])\nmetric_hurdle_corrected_perfect_cls['model'] = 'hurdle_corrected_perfect_cls'\n\nmetric_df2 = pd.DataFrame([metric_baseline,\n                       metric_winsorized,\n                       metric_log1p,\n                       metric_hurdle,\n                       metric_hurdle_corrected,\n                       metric_hurdle_corrected_perfect_cls,])\nmetric_df2.transpose()\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\nroot_mean_squared_error\n3162.478744\n3623.576378\n3725.342296\n3171.760745\n3055.320787\n3030.854831\n\n\nmean_squared_error\n10001271.807776\n13130305.763947\n13878175.221577\n10060066.223275\n9334985.110424\n9186081.006625\n\n\nmean_absolute_error\n715.644266\n627.788007\n618.976847\n584.916293\n613.394664\n479.558294\n\n\nr2\n0.381617\n0.188147\n0.141906\n0.377981\n0.422813\n0.43202\n\n\npearsonr\n0.619072\n0.575799\n0.581717\n0.67697\n0.67697\n0.687639\n\n\nspearmanr\n0.470085\n0.504302\n0.533816\n0.510708\n0.510708\n0.929419\n\n\nmedian_absolute_error\n232.982083\n219.622481\n89.554954\n199.178014\n232.555574\n34.991964\n\n\nearths_mover_distance\n287.777288\n432.128843\n581.049444\n286.381443\n241.618399\n234.587018\n\n\nmodel\nbaseline\nwinsorized\nlog1p\nhurdle\nhurdle_corrected\nhurdle_corrected_perfect_cls\n\n\n\n\n\n\n\n\n\nRemember What Problem We Are Solving\nOne last thing to remember is that we are trying to predict sales of each individual customer, not total sales of all customers. If we look at aggregated mean or sum of actual sales vs predicted sales, baseline regression performs best by far. This is due to the fact that without any constraints a regressor only minimizes the MSE loss and usually ends up predicting values around the mean to balance between under- and over-predictions. However, this level of prediction is often not very useful as a single point. Imagine you want to give promotions with higher or lower spend thresholds to customers according to their purchasing power; you will not be able to do so with a model that is accurate on aggregate but not so much on individual customers.\n\n\nCode\ntest_df[['TargetSales','pred_baseline','pred_winsorized','pred_log1p_expm1','pred_hurdle','pred_hurdle_corrected']].mean()\n\n\nTargetSales              760.558808\npred_baseline            791.043945\npred_winsorized          508.281555\npred_log1p_expm1         186.200281\npred_hurdle              527.286811\npred_hurdle_corrected    647.560493\ndtype: float64"
  },
  {
    "objectID": "posts/sales_prediction/index.html#closing-remarks",
    "href": "posts/sales_prediction/index.html#closing-remarks",
    "title": "Predict How Much A Customer Will Spend",
    "section": "Closing Remarks",
    "text": "Closing Remarks\nAnd this is how you predict how much a customer will spend in the least wrong way. My hope is that you will not need to spend ten years in data science to find out how to do it like I did."
  },
  {
    "objectID": "posts/interesting_career/index.html",
    "href": "posts/interesting_career/index.html",
    "title": "How to Have a Robustly Interesting Career in Data Science",
    "section": "",
    "text": "I have been a data scientistüë®‚Äçüíª ever since around the time HBR popularized the term in its job sexiness article. My run started out of necessity, since back then it was very difficult to get a job when you were an Econ graduate with an eclectic mix of skills in statistics and programming; suddenly it became one of the most sought-after skill combinations on the market. It has been about a decade since. Lately, I have been asked about how to have a successful career in the field. Success is quite the subjective term, but let me try to formulate my thoughts on how to keep it interesting doing what I love in spite of external circumstances.\n\n\n\nfeatured_image\n\n\nObjective Function As any half decent scientist would do, let us begin at the desirable end state and work backwards. I have always optimized for business impact. I relish in seeing my models bring joy to customers, efficiency to selling partners, and productivity to fellow builders. Something as simple as seeing a widget on the home screen powered by one of my models makes my day. Thankfully, my data products have consistently delivered at least double-digit millions of dollars in value (cost savings and/or top-line uplifts) annually. And this keeps me sane in the midst of all the [insert-your-hype-keyword] FOMO. The key is to find an objective function that really matters to you, and not fall for the vanity metrics. For instance, I have written some research papers and did some fun open source projects, but a lot of these feel too abstract to quantify. If I were to try to optimize for them, I would likely end up chasing after citations, Github stars, or other arbitrary numbers. I would overfit them and the activities of writing papers and contributing to open source projects themselves would become secondary to the vanity metrics. This will not be the case if writing great research papers is something that really matters to you. You need to find an objective function that correlates almost perfectly with the joy you experience as a scientist.\nBuild with Stakes As you move to fulfill your objectives, it is important that you do so by building concrete products with high enough stakes. I strongly believe this is the only way to level up as a scientist. The anti-thesis to this is to get stuck in the learning loop; you keep taking online courses, overfitting toy datasets, hunting for certificates, and you wonder why your career is not going anywhere while the snakeoil vendors keep buying new sports cars. It is because these learning materials when taken in excess only serve to make you feel good about yourself. Yes, you learned something new and yes, you might have built some capstone projects. But these have virtually no consequence if you fail. In fact, they are structured in a way that it is more difficult for you to fail. You stop thinking for yourself and just enjoy the pseudo-intellectual force feeding. It comes as no surprise that you need to think for yourself to grow as a scientist. The stakes do not have to be monetary. It can be anything that matters if you fail to complete your tasks. Enter a competition that evaluates scientific prowess (avoid slide show contests), create an open source project that helps with your hobbies, or most likely propose a project at work that affects your compensation/promotion evaluation. Conduct experiments, write production codes, and document everything either as technical reports or research papers. Hold yourself to the highest standards.\nFind The Right Party The most important person in your career is your mentor. Ideally, you want to find the person whose objective function aligns well with yours and have a clear track records of building with stakes. At different stages, you might have more than one for different aspects such as one for business and another for research, or for different perspectives. But if I were to be honest, you would be extremely lucky to find one at all. Your mentor should not only be the person you aspire to be, but also someone you think you would have a chance to overcome in a fair fight one day. Once you have found such person, you would usually be surrounded by good company. Humans are social animals and no matter how hard you try to follow your objective function and hold yourself to the highest standards, it will be almost impossible without people with similar mentality by your side. This is the most luck-dependent component. Be grateful if you can find the right party and be the party people would like to join one day.\nIntegrity Every action has a price. We are in a privileged position to have a skillset that often determines the outcome of the business and rarely people question. You could gain a small but decisive advantage almost scotch-free; keep randomizing the seeds in validation splits and/or model initialization to get a marginally good result in offline evaluation to justify a launch, slice and dice the control and treatment groups to get a statistically significant result, pick a seemingly strong model that is out-of-domain to compare with your specifically finetuned model and call your model SoTA, the list goes on. But you should never do any of this, not only because you are a good person who does not want to lose sleep, but because you will eventually pay the price. I can guarantee there is always a price. Any useful scientist will call you out on any of the examples I gave; if not, time and repeated online experiments will expose your fudging of the numbers. Your choice is to keep job hopping before this happens and keep the career Ponzi scheme alive. Or simply be an honest scientist, enjoys the scientific process, learns from your mistakes, and grow.\nSide Quests Whatever you do, there is always a room for side quests. Your main quest is to satisfy your objective function, but as any good optimization method, exploration is needed to ensure a robust solution. A side quest is an excuse to try the new technology you have been raring to get your hands on, a low-stake confidence builder, and most importantly a great way to remind us how much we love the craft. These are things I hold closest to my heart as I navigate the fast-paced, uncertainty-filled landscape of data science. I have conducted a few dozens of model validation experiments and each one is never less anxiety-inducing than the last. We are in the business of results and results can be brutal. In these turbulent economic conditions, I hope these templates can be useful to you as much as they were to me in surviving some unpleasant situations that may be beyond our control and continuing to do the things we love.\nBe safe from scammers and snakeoil vendors. And I will see you around."
  },
  {
    "objectID": "notebook/sales_prediction.html",
    "href": "notebook/sales_prediction.html",
    "title": "Predict Zero-inflated and Long/fat-tailed Outcomes",
    "section": "",
    "text": "This notebook details how to predict a real-number outcome that is zero-inflated and long/fat-tailed such as sales prediction in retail. We provide baseline regression, regression trained using winsorized outcome, regression trained on log(y+1) outcome, and hurdle regression with and without Duan‚Äôs method."
  },
  {
    "objectID": "notebook/sales_prediction.html#import",
    "href": "notebook/sales_prediction.html#import",
    "title": "Predict Zero-inflated and Long/fat-tailed Outcomes",
    "section": "Import",
    "text": "Import\n\nimport pandas as pd\nimport numpy as np\nimport random\nfrom ucimlrepo import fetch_ucirepo \nimport boto3\nimport json\nfrom tqdm.auto import tqdm\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom autogluon.tabular import TabularDataset, TabularPredictor\nfrom sklearn.metrics import (\n    mean_squared_error, mean_absolute_error, r2_score, median_absolute_error,\n    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n)\n\nfrom scipy.stats import pearsonr, spearmanr, wasserstein_distance\nfrom statsmodels.stats.diagnostic import het_white\n\ndef calculate_regression_metrics(y_true, y_pred):\n    return {\n        'root_mean_squared_error': np.sqrt(mean_squared_error(y_true, y_pred)),\n        'mean_squared_error': mean_squared_error(y_true, y_pred),\n        'mean_absolute_error': mean_absolute_error(y_true, y_pred),\n        'r2': r2_score(y_true, y_pred),\n        'pearsonr': pearsonr(y_true, y_pred)[0],  \n        'spearmanr': spearmanr(y_true, y_pred)[0],\n        'median_absolute_error': median_absolute_error(y_true, y_pred),\n        'earths_mover_distance': wasserstein_distance(y_true, y_pred)\n    }\n\ndef caluclate_classification_metrics(y_true, y_pred):\n    return {\n        'accuracy': accuracy_score(y_true, y_pred),\n        'precision': precision_score(y_true, y_pred, average='weighted'),\n        'recall': recall_score(y_true, y_pred, average='weighted'),\n        'f1_score': f1_score(y_true, y_pred, average='weighted'),\n        'confusion_matrix': confusion_matrix(y_true, y_pred)\n    }\n\ndef string_to_yearmon(date):\n    date = date.split()\n    date = date[0].split('/') + date[1].split(':')\n    date = date[2] + '-' + date[0].zfill(2) #+ '-' + date[1].zfill(2) + ' ' + date[3].zfill(2) + ':' + date[4].zfill(2)\n    return date\n\ndef call_llama(system_prompt, input):\n    template = f\"\"\"&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;{system_prompt}&lt;&lt;/SYS&gt;&gt;{input}[/INST]\"\"\"\n    client = boto3.client(service_name='bedrock-runtime',region_name='us-west-2')\n    body = json.dumps({\n        \"prompt\": template,\n        \"temperature\": 0.,\n        \"top_p\": 0.9,\n        \"max_gen_len\": 2048,\n    })\n    response = client.invoke_model(\n        body=body,\n        modelId='us.meta.llama3-2-90b-instruct-v1:0',\n        accept='application/json',\n        contentType='application/json'\n    )\n    response_body = json.loads(response['body'].read())\n    return response_body\n\ndef call_claude(system_prompt, input):\n    client = boto3.client(service_name='bedrock-runtime',region_name='us-west-2')\n    body=json.dumps(\n        {\n            \"anthropic_version\": \"bedrock-2023-05-31\",\n            \"max_tokens\": 2048,\n            \"messages\": [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": system_prompt + '\\n' + input,\n                    }\n                    ]\n                }\n                ]\n        }  \n    )  \n\n    \n    response = client.invoke_model(body=body, \n                                   modelId='anthropic.claude-3-5-sonnet-20241022-v2:0',\n                                   contentType='application/json',\n                                   accept='application/json')\n    response_body = json.loads(response.get('body').read())\n   \n    return response_body\n\nfrom sklearn.preprocessing import StandardScaler\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\n\nclass Winsorizer:\n    def __init__(self, cols, percentile=99):\n        self.cols = cols  # List of columns to apply winsorization to\n        self.percentile = percentile  # Percentile to define the outliers\n        self.lower_bounds = {}  # To store the lower quantiles\n        self.upper_bounds = {}  # To store the upper quantiles\n\n    def fit(self, df):\n        \"\"\"Fit the winsorizer to the data, remembering the quantiles.\"\"\"\n        for col in self.cols:\n            lower = df[col].quantile(1 - self.percentile / 100)\n            upper = df[col].quantile(self.percentile / 100)\n            self.lower_bounds[col] = lower\n            self.upper_bounds[col] = upper\n    \n    def transform(self, df):\n        \"\"\"Apply winsorization to a new DataFrame using the learned quantiles.\"\"\"\n        for col in self.cols:\n            lower = self.lower_bounds[col]\n            upper = self.upper_bounds[col]\n            df[col] = np.clip(df[col], lower, upper)\n        return df\n\n    def fit_transform(self, df):\n        \"\"\"Fit the model and apply winsorization to the same DataFrame.\"\"\"\n        self.fit(df)\n        return self.transform(df)\n\ndef calculate_vif(df, cols):\n    X = df[cols]\n    X_with_const = add_constant(X)  # Add constant for VIF calculation\n    vif_data = pd.DataFrame()\n    vif_data['feature'] = X_with_const.columns\n    vif_data['VIF'] = [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]\n    return vif_data\n\nimport seaborn as sns\nimport statsmodels.api as sm"
  },
  {
    "objectID": "notebook/sales_prediction.html#dataset",
    "href": "notebook/sales_prediction.html#dataset",
    "title": "Predict Zero-inflated and Long/fat-tailed Outcomes",
    "section": "Dataset",
    "text": "Dataset\nWe use the UCI Online Retail dataset, which are transactions from a UK-based, non-store online retail from 2010-12-01 and 2011-12-09. We perform the following data processing:\n\nRemove transactions without CustomerID; from 541,909 to 406,829 transactions\nFilter out transactions where either UnitPrice or Quantity is less than zero; from 406,829 to 397,884 transactions\nFill in missing product Description with value UNKNOWN.\n\n\nonline_retail = fetch_ucirepo(id=352) \ntransaction_df = online_retail['data']['original']\ntransaction_df.shape\n\n(541909, 8)\n\n\n\n#create yearmon for train-valid split\ntransaction_df['yearmon'] = transaction_df.InvoiceDate.map(string_to_yearmon)\n\n#get rid of transactions without cid\ntransaction_df = transaction_df[~transaction_df.CustomerID.isna()].reset_index(drop=True)\n\n#fill in unknown descriptions\ntransaction_df.Description = transaction_df.Description.fillna('UNKNOWN')\n\n#convert customer id to string\ntransaction_df['CustomerID'] = transaction_df['CustomerID'].map(lambda x: str(int(x)))\n\ntransaction_df.shape\n\n(406829, 9)\n\n\n\n#check if still na\ntransaction_df.isna().mean()\n\nInvoiceNo      0.0\nStockCode      0.0\nDescription    0.0\nQuantity       0.0\nInvoiceDate    0.0\nUnitPrice      0.0\nCustomerID     0.0\nCountry        0.0\nyearmon        0.0\ndtype: float64\n\n\n\n#simplify by filtering unit price and quantity to be non-zero (get rid of discounts, cancellations, etc)\ntransaction_df = transaction_df[(transaction_df.UnitPrice&gt;0)&\\\n                                (transaction_df.Quantity&gt;0)].reset_index(drop=True)\n#add sales\ntransaction_df['Sales'] = transaction_df.UnitPrice * transaction_df.Quantity\ntransaction_df.shape\n\n(397884, 10)"
  },
  {
    "objectID": "notebook/sales_prediction.html#problem-formulation-and-outcome",
    "href": "notebook/sales_prediction.html#problem-formulation-and-outcome",
    "title": "Predict Zero-inflated and Long/fat-tailed Outcomes",
    "section": "Problem Formulation and Outcome",
    "text": "Problem Formulation and Outcome\nWe formulate the problem as predicting the sales (TargetSales) during Q4 2011 for each customers who bought at least one item during Q1-Q3 2011. Note that we are interested in predicting the actual sales number per customer as accurately as possible; this is common for marketing use cases such as determining what spend threshold to give each customer in a promotion, targeting customers for upselling, or detecting early signs of churns.\nWe transform the transaction dataset into a customer-level dataset where we calculate features using transactions between 2011-01 to 2011-09 and outcome using transactions between 2011-10 to 2011-12, summing Quantity times UnitPrice. We left-join the customers in feature set to outcome set. This will result in the zero-inflated nature of the outcome as not all customers will come back in Q4. The distribution of non-zero sales is naturally long/fat-tailed with a few customers having extraordinarily high amount of sales in Q4. This resulted in a customer-level dataset with 3,438 customers.\n\nfeature_period = {'start': '2011-01', 'end': '2011-09'}\noutcome_period = {'start': '2011-10', 'end': '2011-12'}\n\nfeature_transaction = transaction_df[(transaction_df.yearmon&gt;=feature_period['start'])&\\\n                                      (transaction_df.yearmon&lt;=feature_period['end'])]\noutcome_transaction = transaction_df[(transaction_df.yearmon&gt;=outcome_period['start'])&\\\n                                      (transaction_df.yearmon&lt;=outcome_period['end'])]\nfeature_transaction.shape, outcome_transaction.shape\n\n((240338, 10), (131389, 10))\n\n\n\n#aggregate sales during outcome period\noutcome_sales = outcome_transaction.groupby('CustomerID').Sales.sum().reset_index()\noutcome_sales\n\n\n\n\n\n\n\n\nCustomerID\nSales\n\n\n\n\n0\n12347\n1519.14\n\n\n1\n12349\n1757.55\n\n\n2\n12352\n311.73\n\n\n3\n12356\n58.35\n\n\n4\n12357\n6207.67\n\n\n...\n...\n...\n\n\n2555\n18276\n335.86\n\n\n2556\n18277\n110.38\n\n\n2557\n18282\n77.84\n\n\n2558\n18283\n974.21\n\n\n2559\n18287\n1072.00\n\n\n\n\n2560 rows √ó 2 columns\n\n\n\n\n#aggregate sales during feature period\nfeature_sales = feature_transaction.groupby('CustomerID').Sales.sum().reset_index()\nfeature_sales\n\n\n\n\n\n\n\n\nCustomerID\nSales\n\n\n\n\n0\n12346\n77183.60\n\n\n1\n12347\n2079.07\n\n\n2\n12348\n904.44\n\n\n3\n12350\n334.40\n\n\n4\n12352\n2194.31\n\n\n...\n...\n...\n\n\n3433\n18280\n180.60\n\n\n3434\n18281\n80.82\n\n\n3435\n18282\n100.21\n\n\n3436\n18283\n1120.67\n\n\n3437\n18287\n765.28\n\n\n\n\n3438 rows √ó 2 columns\n\n\n\n\n#merge to get TargetSales including those who spent during feature period but not during outcome (zeroes)\noutcome_df = feature_sales[['CustomerID']].merge(outcome_sales, on='CustomerID', how='left')\noutcome_df['Sales'] = outcome_df['Sales'].fillna(0)\noutcome_df.columns = ['CustomerID', 'TargetSales']\noutcome_df\n\n\n\n\n\n\n\n\nCustomerID\nTargetSales\n\n\n\n\n0\n12346\n0.00\n\n\n1\n12347\n1519.14\n\n\n2\n12348\n0.00\n\n\n3\n12350\n0.00\n\n\n4\n12352\n311.73\n\n\n...\n...\n...\n\n\n3433\n18280\n0.00\n\n\n3434\n18281\n0.00\n\n\n3435\n18282\n77.84\n\n\n3436\n18283\n974.21\n\n\n3437\n18287\n1072.00\n\n\n\n\n3438 rows √ó 2 columns\n\n\n\n\n#confirm zero-inflated, long/fat-tailed\noutcome_df.TargetSales.describe(percentiles=[i/10 for i in range(10)])\n\ncount      3438.000000\nmean        666.245829\nstd        4016.843037\nmin           0.000000\n0%            0.000000\n10%           0.000000\n20%           0.000000\n30%           0.000000\n40%           0.000000\n50%         102.005000\n60%         263.006000\n70%         425.790000\n80%         705.878000\n90%        1273.611000\nmax      168469.600000\nName: TargetSales, dtype: float64\n\n\n\n#confirm zero-inflated, long/fat-tailed\noutcome_df[outcome_df.TargetSales&lt;=10_000].TargetSales.hist(bins=100)"
  },
  {
    "objectID": "notebook/sales_prediction.html#feature",
    "href": "notebook/sales_prediction.html#feature",
    "title": "Predict Zero-inflated and Long/fat-tailed Outcomes",
    "section": "Feature",
    "text": "Feature\nWe represent a customer using traditional RFM features namely recency of purchase, purchase days, total sales, number of distinct products purchased, number of distinct category purchased, customer tenure within 2011, average purchase frequency, average purchase value, and percentage of purchase across all 9 categories. This is based on data from Q1-Q3 2011.\nSince the UCI Online Retail dataset does not have a category but only contains descriptions over 3,000 items, we use LLaMA 3.2 90B to infer categories based on randomly selected 1,000 descriptions. This is to make the category preference representation for each customer, which is more tractable than including features about all 3,000+ items. After that, we use the same model to label a category for each description. The categories are:\n\nHome Decor\nKitchen and Dining\nFashion Accessories\nStationary and Gifts\nToys and Games\nSeasonal and Holiday\nPersonal Care and Wellness\nOutdoor and Garden\nOthers\n\n\nClassify Description into Category\n\nfeature_transaction.Description.nunique()\n\n3548\n\n\n\nGet Category\n\ndescriptions = feature_transaction.Description.unique().tolist()\nprint(descriptions[:5])\n\n#randomize descriptions with seed 112 to get which categories we should use\nnp.random.seed(112)\nrandom_descriptions = np.random.choice(descriptions, 1000, replace=False)\nprint(random_descriptions[:5])\n\n['JUMBO BAG PINK POLKADOT', 'BLUE POLKADOT WRAP', 'RED RETROSPOT WRAP ', 'RECYCLING BAG RETROSPOT ', 'RED RETROSPOT SHOPPER BAG']\n['MODERN FLORAL STATIONERY SET' 'PURPLE BERTIE GLASS BEAD BAG CHARM'\n 'PARTY INVITES SPACEMAN' 'MONTANA DIAMOND CLUSTER EARRINGS'\n 'SKULLS  DESIGN  COTTON TOTE BAG']\n\n\n\n# res = call_llama(\n#     'You are a product categorization assistant at a retail website.',\n#     'Given the following product descriptions, come up with a few product categories they should be classified into.'+'\\n'.join(random_descriptions)\n#     )\n\n# print(res['generation'])\n\n\n# res\n\n\n# res = call_claude(\n#     'You are a product categorization assistant at a retail website.',\n#     'Given the following product descriptions, come up with a few product categories they should be classified into.'+'\\n'.join(random_descriptions)\n#     )\n\n# print(res['content'][0]['text'])\n\n\n# res\n\nLLaMA 3.2 90B Output:\n&lt;&lt;SYS&gt;&gt;Based on the product descriptions, I would categorize them into the following categories:\n\n1. Home Decor:\n    * Wall art\n    * Decorative items (e.g. vases, figurines, etc.)\n    * Lighting (e.g. candles, lanterns, etc.)\n    * Textiles (e.g. throw pillows, blankets, etc.)\n2. Kitchen and Dining:\n    * Cookware and utensils\n    * Tableware (e.g. plates, cups, etc.)\n    * Kitchen decor (e.g. signs, magnets, etc.)\n    * Food and drink items (e.g. tea, coffee, etc.)\n3. Fashion and Accessories:\n    * Jewelry (e.g. necklaces, earrings, etc.)\n    * Handbags and wallets\n    * Clothing and accessories (e.g. scarves, hats, etc.)\n4. Stationery and Gifts:\n    * Cards and gift wrap\n    * Stationery (e.g. notebooks, pens, etc.)\n    * Gift items (e.g. mugs, keychains, etc.)\n5. Toys and Games:\n    * Toys (e.g. stuffed animals, puzzles, etc.)\n    * Games and puzzles\n6. Seasonal and Holiday:\n    * Christmas decorations and gifts\n    * Easter decorations and gifts\n    * Other seasonal items (e.g. Halloween, etc.)\n7. Personal Care and Wellness:\n    * Beauty and personal care items (e.g. skincare, haircare, etc.)\n    * Wellness and self-care items (e.g. essential oils, etc.)\n8. Outdoor and Garden:\n    * Garden decor and accessories\n    * Outdoor furniture and decor\n    * Gardening tools and supplies\n\nNote that some products may fit into multiple categories, but I have assigned them to the one that seems most relevant.\nClaude 3.5 v2 Output\nBased on these product descriptions, I would suggest the following main product categories:\n\n1. Home Decor\n- Candle holders\n- Picture frames\n- Wall art & signs\n- Clocks\n- Cushions & covers\n- Storage items\n- Decorative objects\n\n2. Jewelry & Accessories\n- Necklaces\n- Bracelets\n- Earrings\n- Hair accessories\n- Bag charms\n- Key rings\n\n3. Garden & Outdoor\n- Plant pots\n- Garden tools\n- Outdoor decorations\n- Bird houses\n- Garden markers\n\n4. Kitchen & Dining\n- Tea sets\n- Mugs\n- Kitchen storage\n- Cutlery\n- Baking accessories\n- Tea towels\n\n5. Stationery & Paper Goods\n- Notebooks\n- Gift wrap\n- Cards\n- Paper decorations\n- Writing sets\n\n6. Party & Celebrations\n- Party supplies\n- Gift bags\n- Christmas decorations\n- Easter items\n- Birthday items\n\n7. Children's Items\n- Toys\n- Children's tableware\n- School supplies\n- Kids' accessories\n\n8. Fashion Accessories\n- Bags\n- Purses\n- Scarves\n- Travel accessories\n\n9. Bath & Beauty\n- Bathroom accessories\n- Toiletry bags\n- Beauty items\n\n10. Lighting\n- Lamps\n- String lights\n- Tea lights\n- Lanterns\n\nThese categories cover the main types of products in the list while providing logical groupings for customers to browse.\n\ncategories = [\n    'Home Decor',\n    'Kitchen and Dining',\n    'Fashion Accessories',\n    'Stationary and Gifts',\n    'Toys and Games',\n    'Seasonal and Holiday',\n    'Personal Care and Wellness',\n    'Outdoor and Garden',   \n]\n\nlen(categories)\n\n8\n\n\n\n\nAnnotate Category to Description\n\n# #loop through descriptions in batches of batch_size\n# res_texts = []\n# batch_size = 100\n# for i in tqdm(range(0, len(descriptions), batch_size)):\n#     batch = descriptions[i:i+batch_size]\n#     d = \"\\n\".join(batch)\n#     inp = f'''Categorize the following product descriptions into {\", \".join(categories)} or Others, if they do not fall into any. \n# Only answer in the following format:\n\n# \"product description of product #1\"|\"product category classified into\"\n# \"product description of product #2\"|\"product category classified into\"\n# ...\n# \"product description of product #n\"|\"product category classified into\"\n\n# Here are the product descriptions:\n# {d}\n# '''\n#     while True:\n#         res = call_claude('You are a product categorizer at a retail website', inp)\n#         # if res['generation_token_count'] &gt; 1: #for llama\n#         if res['usage']['output_tokens'] &gt; 1:\n#             break\n#         else:\n#             print('Retrying...')\n#             time.sleep(2)\n#     res_text = res['content'][0]['text'].strip().split('\\n')\n#         #for llama\n#         # .replace('[SYS]','').replace('&lt;&lt;SYS&gt;&gt;','')\\\n#         # .replace('[/SYS]','').replace('&lt;&lt;/SYS&gt;&gt;','')\\\n#     if res_text!='':\n#         res_texts.extend(res_text)\n\n\n# with open('../data/sales_prediction/product_description_category.csv','w') as f:\n#     f.write('\"product_description\"|\"category\"\\n')\n#     for i in res_texts:\n#         f.write(f'{i}\\n')\n\n\nproduct_description_category = pd.read_csv('../data/sales_prediction/product_description_category.csv',\n                                           sep='|')\n\n#clean product_description\nproduct_description_category['Description'] = descriptions\nproduct_description_category.category.value_counts(normalize=True)\n\ncategory\nHome Decor                    0.328636\nKitchen and Dining            0.195885\nFashion Accessories           0.138670\nStationary and Gifts          0.116122\nSeasonal and Holiday          0.087373\nPersonal Care and Wellness    0.047351\nToys and Games                0.045096\nOutdoor and Garden            0.032976\nOthers                        0.007892\nName: proportion, dtype: float64\n\n\n\nfeature_transaction_cat = feature_transaction.merge(product_description_category,\n                                                    how='inner',\n                                                    on = 'Description',)\nfeature_transaction.shape, feature_transaction_cat.shape\n\n((240338, 10), (240338, 12))\n\n\n\n\n\nRFM\n\n#convert invoice date to datetime\nfeature_transaction_cat['InvoiceDate'] = pd.to_datetime(feature_transaction_cat['InvoiceDate'])\n\n# last date in feature set\ncurrent_date = feature_transaction_cat['InvoiceDate'].max()\n\n#rfm\ncustomer_features = feature_transaction_cat.groupby('CustomerID').agg({\n    'InvoiceDate': [\n        ('recency', lambda x: (current_date - x.max()).days),\n        ('first_purchase_date', 'min'),\n        ('purchase_day', 'nunique'),\n    ],\n    'InvoiceNo': [('nb_invoice', 'nunique')],\n    'Sales': [\n        ('total_sales', 'sum')\n    ],\n    'StockCode': [('nb_product', 'nunique')],\n    'category': [('nb_category', 'nunique')]\n}).reset_index()\n\n# Flatten column names\ncustomer_features.columns = [\n    'CustomerID',\n    'recency',\n    'first_purchase_date',\n    'purchase_day',\n    'nb_invoice',\n    'total_sales',\n    'nb_product',\n    'nb_category'\n]\n\n\n#almost always one purchase a day\n(customer_features.purchase_day==customer_features.nb_invoice).mean()\n\n0.977021524141943\n\n\n\ncustomer_features['customer_lifetime'] = (current_date - customer_features['first_purchase_date']).dt.days\ncustomer_features['avg_purchase_frequency'] = customer_features['customer_lifetime'] / customer_features['purchase_day']\ncustomer_features['avg_purchase_value'] = customer_features['total_sales'] / customer_features['purchase_day']\n\n\n\nCategory Preference\n\n#category preference\ncategory_sales = feature_transaction_cat.pivot_table(\n    values='Sales', \n    index='CustomerID', \n    columns='category', \n    aggfunc='sum', \n    fill_value=0\n)\ncategory_sales.columns = [i.lower().replace(' ','_') for i in category_sales.columns]\ncustomer_features = customer_features.merge(category_sales, on='CustomerID', how='left')\n\ntotal_sales = customer_features['total_sales']\nfor col in category_sales.columns:\n    percentage_col = f'per_{col}'\n    customer_features[percentage_col] = customer_features[col] / total_sales\n\n\n#make sure the categories are not too sparse\n(customer_features.iloc[:,-9:]==0).mean()\n\nper_fashion_accessories           0.409831\nper_home_decor                    0.081734\nper_kitchen_and_dining            0.122455\nper_others                        0.765561\nper_outdoor_and_garden            0.507853\nper_personal_care_and_wellness    0.448226\nper_seasonal_and_holiday          0.369401\nper_stationary_and_gifts          0.305410\nper_toys_and_games                0.487202\ndtype: float64\n\n\n\n\nPutting Them All Together\n\nselected_features = [\n 'recency',\n 'purchase_day',\n 'total_sales',\n 'nb_product',\n 'nb_category',\n 'customer_lifetime',\n 'avg_purchase_frequency',\n 'avg_purchase_value',\n 'per_fashion_accessories',\n 'per_home_decor',\n 'per_kitchen_and_dining',\n 'per_others',\n 'per_outdoor_and_garden',\n 'per_personal_care_and_wellness',\n 'per_seasonal_and_holiday',\n 'per_stationary_and_gifts',\n 'per_toys_and_games']\n\noutcome_variable = 'TargetSales'\n\n\ncustomer_features = customer_features[[ 'CustomerID']+selected_features]\ncustomer_features.head()\n\n\n\n\n\n\n\n\nCustomerID\nrecency\npurchase_day\ntotal_sales\nnb_product\nnb_category\ncustomer_lifetime\navg_purchase_frequency\navg_purchase_value\nper_fashion_accessories\nper_home_decor\nper_kitchen_and_dining\nper_others\nper_outdoor_and_garden\nper_personal_care_and_wellness\nper_seasonal_and_holiday\nper_stationary_and_gifts\nper_toys_and_games\n\n\n\n\n0\n12346\n255\n1\n77183.60\n1\n1\n255\n255.000000\n77183.600000\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n1\n12347\n59\n4\n2079.07\n65\n7\n247\n61.750000\n519.767500\n0.145834\n0.204168\n0.294021\n0.000000\n0.005628\n0.147614\n0.000000\n0.073013\n0.129721\n\n\n2\n12348\n5\n3\n904.44\n10\n4\n248\n82.666667\n301.480000\n0.000000\n0.000000\n0.000000\n0.132679\n0.000000\n0.825970\n0.018796\n0.022555\n0.000000\n\n\n3\n12350\n239\n1\n334.40\n17\n7\n239\n239.000000\n334.400000\n0.240431\n0.202751\n0.116926\n0.172548\n0.000000\n0.118421\n0.000000\n0.059211\n0.089713\n\n\n4\n12352\n2\n7\n2194.31\n47\n8\n226\n32.285714\n313.472857\n0.000000\n0.196531\n0.246187\n0.474090\n0.013535\n0.016680\n0.008066\n0.024404\n0.020508"
  },
  {
    "objectID": "notebook/sales_prediction.html#merge-features-and-outcome",
    "href": "notebook/sales_prediction.html#merge-features-and-outcome",
    "title": "Predict Zero-inflated and Long/fat-tailed Outcomes",
    "section": "Merge Features and Outcome",
    "text": "Merge Features and Outcome\n\ncustomer_features.shape, outcome_df.shape\n\n((3438, 18), (3438, 2))\n\n\n\ndf = outcome_df.merge(customer_features, on='CustomerID').drop('CustomerID', axis=1)\ndf.shape\n\n(3438, 18)\n\n\n\n#correlations\ndf.iloc[:,1:].corr()\n\n\n\n\n\n\n\n\nrecency\npurchase_day\ntotal_sales\nnb_product\nnb_category\ncustomer_lifetime\navg_purchase_frequency\navg_purchase_value\nper_fashion_accessories\nper_home_decor\nper_kitchen_and_dining\nper_others\nper_outdoor_and_garden\nper_personal_care_and_wellness\nper_seasonal_and_holiday\nper_stationary_and_gifts\nper_toys_and_games\n\n\n\n\nrecency\n1.000000\n-0.299308\n-0.132344\n-0.287415\n-0.326772\n0.298853\n0.893973\n0.008823\n-0.020861\n0.022013\n0.057244\n-0.016069\n0.071268\n-0.082792\n-0.085681\n-0.017813\n-0.009686\n\n\npurchase_day\n-0.299308\n1.000000\n0.540253\n0.690345\n0.304621\n0.332109\n-0.331543\n0.027488\n0.030683\n0.018684\n0.025269\n0.004299\n-0.019992\n-0.035665\n-0.020392\n-0.045384\n-0.028187\n\n\ntotal_sales\n-0.132344\n0.540253\n1.000000\n0.400467\n0.137064\n0.156018\n-0.148762\n0.361138\n0.016511\n-0.013819\n0.047834\n0.006398\n-0.029353\n-0.011937\n-0.016724\n-0.029181\n-0.013139\n\n\nnb_product\n-0.287415\n0.690345\n0.400467\n1.000000\n0.555551\n0.265594\n-0.294923\n0.061039\n-0.003137\n-0.017516\n0.035615\n-0.006842\n-0.026371\n-0.005309\n-0.016586\n0.026716\n-0.010069\n\n\nnb_category\n-0.326772\n0.304621\n0.137064\n0.555551\n1.000000\n0.224232\n-0.321596\n0.019955\n0.004863\n-0.138372\n-0.039363\n0.055555\n0.041405\n0.075882\n0.015498\n0.152869\n0.111150\n\n\ncustomer_lifetime\n0.298853\n0.332109\n0.156018\n0.265594\n0.224232\n1.000000\n0.358431\n0.014933\n0.011220\n0.066111\n0.069175\n-0.019971\n0.029726\n-0.127865\n-0.120399\n-0.050320\n-0.036484\n\n\navg_purchase_frequency\n0.893973\n-0.331543\n-0.148762\n-0.294923\n-0.321596\n0.358431\n1.000000\n0.009157\n-0.016093\n0.027208\n0.037053\n-0.027413\n0.060369\n-0.070352\n-0.074799\n-0.000546\n-0.010612\n\n\navg_purchase_value\n0.008823\n0.027488\n0.361138\n0.061039\n0.019955\n0.014933\n0.009157\n1.000000\n-0.003187\n-0.056690\n0.076862\n0.015427\n-0.028884\n0.004225\n-0.000200\n-0.012729\n-0.002396\n\n\nper_fashion_accessories\n-0.020861\n0.030683\n0.016511\n-0.003137\n0.004863\n0.011220\n-0.016093\n-0.003187\n1.000000\n-0.254015\n-0.177775\n-0.010436\n-0.082834\n-0.038493\n-0.124719\n-0.068166\n-0.051486\n\n\nper_home_decor\n0.022013\n0.018684\n-0.013819\n-0.017516\n-0.138372\n0.066111\n0.027208\n-0.056690\n-0.254015\n1.000000\n-0.481983\n-0.155784\n-0.080637\n-0.158837\n-0.165964\n-0.262313\n-0.245759\n\n\nper_kitchen_and_dining\n0.057244\n0.025269\n0.047834\n0.035615\n-0.039363\n0.069175\n0.037053\n0.076862\n-0.177775\n-0.481983\n1.000000\n-0.013075\n-0.144698\n-0.117031\n-0.204235\n-0.173386\n-0.143931\n\n\nper_others\n-0.016069\n0.004299\n0.006398\n-0.006842\n0.055555\n-0.019971\n-0.027413\n0.015427\n-0.010436\n-0.155784\n-0.013075\n1.000000\n-0.062652\n0.014794\n-0.047940\n-0.033975\n-0.040421\n\n\nper_outdoor_and_garden\n0.071268\n-0.019992\n-0.029353\n-0.026371\n0.041405\n0.029726\n0.060369\n-0.028884\n-0.082834\n-0.080637\n-0.144698\n-0.062652\n1.000000\n-0.045639\n-0.077947\n-0.057297\n-0.001034\n\n\nper_personal_care_and_wellness\n-0.082792\n-0.035665\n-0.011937\n-0.005309\n0.075882\n-0.127865\n-0.070352\n0.004225\n-0.038493\n-0.158837\n-0.117031\n0.014794\n-0.045639\n1.000000\n-0.057926\n-0.025871\n-0.017022\n\n\nper_seasonal_and_holiday\n-0.085681\n-0.020392\n-0.016724\n-0.016586\n0.015498\n-0.120399\n-0.074799\n-0.000200\n-0.124719\n-0.165964\n-0.204235\n-0.047940\n-0.077947\n-0.057926\n1.000000\n-0.019418\n-0.042970\n\n\nper_stationary_and_gifts\n-0.017813\n-0.045384\n-0.029181\n0.026716\n0.152869\n-0.050320\n-0.000546\n-0.012729\n-0.068166\n-0.262313\n-0.173386\n-0.033975\n-0.057297\n-0.025871\n-0.019418\n1.000000\n0.172039\n\n\nper_toys_and_games\n-0.009686\n-0.028187\n-0.013139\n-0.010069\n0.111150\n-0.036484\n-0.010612\n-0.002396\n-0.051486\n-0.245759\n-0.143931\n-0.040421\n-0.001034\n-0.017022\n-0.042970\n0.172039\n1.000000\n\n\n\n\n\n\n\n\n#target and most predictive variable\ndf[df.TargetSales&lt;=25_000].plot.scatter(x='TargetSales',y='total_sales')"
  },
  {
    "objectID": "notebook/sales_prediction.html#train-test-splits",
    "href": "notebook/sales_prediction.html#train-test-splits",
    "title": "Predict Zero-inflated and Long/fat-tailed Outcomes",
    "section": "Train-Test Splits",
    "text": "Train-Test Splits\nWe randomly split the dataset into train and test sets at 80/20 ratio. We also confirm the distribution of TargetSales is similar across percentiles and only different at the upper end.\n\n#split into train-valid sets\ntrain_df, test_df = train_test_split(df,\n                                      test_size=0.2, \n                                      random_state=112)\n\n\npd.concat([train_df.TargetSales.describe(percentiles=[i/10 for i in range(10)]).reset_index(),\ntest_df.TargetSales.describe(percentiles=[i/10 for i in range(10)]).reset_index(),], axis=1)\n\n\n\n\n\n\n\n\nindex\nTargetSales\nindex\nTargetSales\n\n\n\n\n0\ncount\n2750.000000\ncount\n688.000000\n\n\n1\nmean\n642.650436\nmean\n760.558808\n\n\n2\nstd\n4015.305436\nstd\n4024.524400\n\n\n3\nmin\n0.000000\nmin\n0.000000\n\n\n4\n0%\n0.000000\n0%\n0.000000\n\n\n5\n10%\n0.000000\n10%\n0.000000\n\n\n6\n20%\n0.000000\n20%\n0.000000\n\n\n7\n30%\n0.000000\n30%\n0.000000\n\n\n8\n40%\n0.000000\n40%\n0.000000\n\n\n9\n50%\n91.350000\n50%\n113.575000\n\n\n10\n60%\n260.308000\n60%\n277.836000\n\n\n11\n70%\n426.878000\n70%\n418.187000\n\n\n12\n80%\n694.164000\n80%\n759.582000\n\n\n13\n90%\n1272.997000\n90%\n1255.670000\n\n\n14\nmax\n168469.600000\nmax\n77099.380000"
  },
  {
    "objectID": "notebook/sales_prediction.html#baseline-regression",
    "href": "notebook/sales_prediction.html#baseline-regression",
    "title": "Predict Zero-inflated and Long/fat-tailed Outcomes",
    "section": "Baseline Regression",
    "text": "Baseline Regression\nThe most naive solution is to predict TargetSales based on the features. We use a stacked ensemble of LightGBM, CatBoost, XGBoost, Random Forest and Extra Trees via AutoGluon. We train with good_quality preset, stated to be ‚ÄúStronger than any other AutoML Framework‚Äù, for speedy training and inference but feel free to try more performant option. We exclude the neural-network models as they require further preprocessing of the features.\nWe use an industry-grade, non-parametric model to be as close to a real use case as possible and make a point that our methodology works not only in a toy-dataset setup.\n\npreset = 'good_quality'\n\n\npredictor = TabularPredictor(label='TargetSales').fit(train_df[selected_features + ['TargetSales']], \n                                                      presets=preset,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20241214_134505\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.1.1\nPython Version:     3.9.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP Wed Oct 23 01:22:11 UTC 2024\nCPU Count:          64\nMemory Avail:       470.24 GB / 480.23 GB (97.9%)\nDisk Space Avail:   1451.64 GB / 1968.52 GB (73.7%)\n===================================================\nPresets specified: ['good_quality']\nSetting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\nStack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\nNote: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n    You can avoid this risk by setting `save_bag_folds=True`.\nDyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n    This is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n    Running DyStack for up to 900s of the 3600s of remaining time (25%).\n2024-12-14 13:45:05,288 INFO util.py:154 -- Outdated packages:\n  ipywidgets==7.6.5 found, needs ipywidgets&gt;=8\nRun `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n/home/charipol/miniconda3/lib/python3.9/site-packages/autogluon/tabular/predictor/predictor.py:1242: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: ValueError('ray==2.40.0 detected. 2.10.0 &lt;= ray &lt; 2.11.0 is required. You can use pip to install certain version of ray `pip install ray==2.10.0` ')\n  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n        Context path: \"AutogluonModels/ag-20241214_134505/ds_sub_fit/sub_fit_ho\"\nRunning DyStack sub-fit ...\nBeginning AutoGluon training ... Time limit = 900s\nAutoGluon will save models to \"AutogluonModels/ag-20241214_134505/ds_sub_fit/sub_fit_ho\"\nTrain Data Rows:    2444\nTrain Data Columns: 17\nLabel Column:       TargetSales\nProblem Type:       regression\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    481515.82 MB\n    Train Data (Original)  Memory Usage: 0.32 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    0.0s = Fit runtime\n    17 features in original data used to generate 17 features in processed data.\n    Train Data (Processed) Memory Usage: 0.32 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.06s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n}\nAutoGluon will fit 2 stack levels (L1 to L2) ...\nExcluded models: ['NN_TORCH', 'FASTAI'] (Specified by `excluded_model_types`)\nFitting 7 L1 models ...\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 599.62s of the 899.66s of remaining time.\nWill use sequential fold fitting strategy because import of ray failed. Reason: ray==2.40.0 detected. 2.10.0 &lt;= ray &lt; 2.11.0 is required. You can use pip to install certain version of ray `pip install ray==2.10.0` \n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -3990.4801   = Validation score   (-root_mean_squared_error)\n    6.36s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ... Training model for up to 593.17s of the 893.2s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -3921.7042   = Validation score   (-root_mean_squared_error)\n    5.31s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: RandomForestMSE_BAG_L1 ... Training model for up to 587.75s of the 887.78s of remaining time.\n    -4516.1791   = Validation score   (-root_mean_squared_error)\n    0.89s    = Training   runtime\n    0.17s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ... Training model for up to 586.59s of the 886.62s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -3857.3111   = Validation score   (-root_mean_squared_error)\n    10.49s   = Training   runtime\n    0.02s    = Validation runtime\nFitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 575.99s of the 876.03s of remaining time.\n    -3900.3038   = Validation score   (-root_mean_squared_error)\n    0.66s    = Training   runtime\n    0.17s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ... Training model for up to 575.08s of the 875.11s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -3941.3599   = Validation score   (-root_mean_squared_error)\n    6.1s     = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ... Training model for up to 568.89s of the 868.93s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -3912.54     = Validation score   (-root_mean_squared_error)\n    12.28s   = Training   runtime\n    0.01s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 856.55s of remaining time.\n    Ensemble Weights: {'CatBoost_BAG_L1': 0.6, 'ExtraTreesMSE_BAG_L1': 0.36, 'LightGBM_BAG_L1': 0.04}\n    -3835.4224   = Validation score   (-root_mean_squared_error)\n    0.02s    = Training   runtime\n    0.0s     = Validation runtime\nExcluded models: ['NN_TORCH', 'FASTAI'] (Specified by `excluded_model_types`)\nFitting 7 L2 models ...\nFitting model: LightGBMXT_BAG_L2 ... Training model for up to 856.47s of the 856.47s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -3941.7891   = Validation score   (-root_mean_squared_error)\n    4.54s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBM_BAG_L2 ... Training model for up to 851.87s of the 851.86s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\n\n[1000]  valid_set's rmse: 4314.99\n\n\n    -3894.7078   = Validation score   (-root_mean_squared_error)\n    5.58s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: RandomForestMSE_BAG_L2 ... Training model for up to 846.2s of the 846.19s of remaining time.\n    -4525.2057   = Validation score   (-root_mean_squared_error)\n    0.87s    = Training   runtime\n    0.17s    = Validation runtime\nFitting model: CatBoost_BAG_L2 ... Training model for up to 845.08s of the 845.07s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -3904.7749   = Validation score   (-root_mean_squared_error)\n    5.45s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 839.51s of the 839.5s of remaining time.\n    -3952.2022   = Validation score   (-root_mean_squared_error)\n    0.68s    = Training   runtime\n    0.17s    = Validation runtime\nFitting model: XGBoost_BAG_L2 ... Training model for up to 838.57s of the 838.56s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -3929.2019   = Validation score   (-root_mean_squared_error)\n    6.6s     = Training   runtime\n    0.04s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L2 ... Training model for up to 831.87s of the 831.86s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -3912.6409   = Validation score   (-root_mean_squared_error)\n    13.59s   = Training   runtime\n    0.01s    = Validation runtime\nFitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 818.17s of remaining time.\n    Ensemble Weights: {'CatBoost_BAG_L1': 0.36, 'ExtraTreesMSE_BAG_L1': 0.32, 'LightGBM_BAG_L2': 0.32}\n    -3823.1639   = Validation score   (-root_mean_squared_error)\n    0.03s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 81.64s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 2283.2 rows/s (306 batch size)\nAutomatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\nRefitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n    Models trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n    This process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n    To learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\nFitting 1 L1 models ...\nFitting model: LightGBMXT_BAG_L1_FULL ...\n    0.25s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBM_BAG_L1_FULL ...\n    0.21s    = Training   runtime\nFitting model: RandomForestMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.89s    = Training   runtime\n    0.17s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: CatBoost_BAG_L1_FULL ...\n    0.6s     = Training   runtime\nFitting model: ExtraTreesMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.66s    = Training   runtime\n    0.17s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: XGBoost_BAG_L1_FULL ...\n    0.12s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBMLarge_BAG_L1_FULL ...\n    0.34s    = Training   runtime\nFitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n    Ensemble Weights: {'CatBoost_BAG_L1': 0.6, 'ExtraTreesMSE_BAG_L1': 0.36, 'LightGBM_BAG_L1': 0.04}\n    0.02s    = Training   runtime\nFitting 1 L2 models ...\nFitting model: LightGBMXT_BAG_L2_FULL ...\n    0.21s    = Training   runtime\nFitting 1 L2 models ...\nFitting model: LightGBM_BAG_L2_FULL ...\n    0.31s    = Training   runtime\nFitting model: RandomForestMSE_BAG_L2_FULL | Skipping fit via cloning parent ...\n    0.87s    = Training   runtime\n    0.17s    = Validation runtime\nFitting 1 L2 models ...\nFitting model: CatBoost_BAG_L2_FULL ...\n    0.17s    = Training   runtime\nFitting model: ExtraTreesMSE_BAG_L2_FULL | Skipping fit via cloning parent ...\n    0.68s    = Training   runtime\n    0.17s    = Validation runtime\nFitting 1 L2 models ...\nFitting model: XGBoost_BAG_L2_FULL ...\n    0.19s    = Training   runtime\nFitting 1 L2 models ...\nFitting model: LightGBMLarge_BAG_L2_FULL ...\n    0.43s    = Training   runtime\nFitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n    Ensemble Weights: {'CatBoost_BAG_L1': 0.36, 'ExtraTreesMSE_BAG_L1': 0.32, 'LightGBM_BAG_L2': 0.32}\n    0.03s    = Training   runtime\nUpdated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\nRefit complete, total runtime = 3.45s ... Best model: \"WeightedEnsemble_L3_FULL\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241214_134505/ds_sub_fit/sub_fit_ho\")\nDeleting DyStack predictor artifacts (clean_up_fits=True) ...\nLeaderboard on holdout data (DyStack):\n                          model  score_holdout    score_val              eval_metric  pred_time_test  pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0          CatBoost_BAG_L1_FULL    -803.899801 -3857.311112  root_mean_squared_error        0.006679            NaN  0.597072                 0.006679                     NaN           0.597072            1       True          4\n1      WeightedEnsemble_L2_FULL    -813.259482 -3835.422447  root_mean_squared_error        0.120649            NaN  1.485833                 0.002517                     NaN           0.018860            2       True          8\n2          CatBoost_BAG_L2_FULL    -838.233626 -3904.774934  root_mean_squared_error        0.258556            NaN  3.238374                 0.006184                     NaN           0.173173            2       True         12\n3   RandomForestMSE_BAG_L1_FULL    -847.825565 -4516.179095  root_mean_squared_error        0.113628       0.174859  0.894842                 0.113628                0.174859           0.894842            1       True          3\n4     ExtraTreesMSE_BAG_L2_FULL    -890.912998 -3952.202176  root_mean_squared_error        0.360469            NaN  3.743573                 0.108097                0.171160           0.678372            2       True         13\n5     ExtraTreesMSE_BAG_L1_FULL    -922.896541 -3900.303809  root_mean_squared_error        0.109015       0.173588  0.658955                 0.109015                0.173588           0.658955            1       True          5\n6      WeightedEnsemble_L3_FULL    -977.887954 -3823.163850  root_mean_squared_error        0.260014            NaN  3.409128                 0.003530                     NaN           0.031533            3       True         16\n7          LightGBM_BAG_L1_FULL   -1086.123687 -3921.704247  root_mean_squared_error        0.002438            NaN  0.210945                 0.002438                     NaN           0.210945            1       True          2\n8   RandomForestMSE_BAG_L2_FULL   -1090.066132 -4525.205744  root_mean_squared_error        0.349192            NaN  3.933684                 0.096820                0.174712           0.868483            2       True         11\n9        LightGBMXT_BAG_L1_FULL   -1230.340360 -3990.480139  root_mean_squared_error        0.002607            NaN  0.245293                 0.002607                     NaN           0.245293            1       True          1\n10       LightGBMXT_BAG_L2_FULL   -1234.815155 -3941.789134  root_mean_squared_error        0.255407            NaN  3.276018                 0.003035                     NaN           0.210817            2       True          9\n11    LightGBMLarge_BAG_L1_FULL   -1345.024278 -3912.540001  root_mean_squared_error        0.004740            NaN  0.335057                 0.004740                     NaN           0.335057            1       True          7\n12    LightGBMLarge_BAG_L2_FULL   -1640.347524 -3912.640942  root_mean_squared_error        0.262513            NaN  3.497248                 0.010141                     NaN           0.432046            2       True         15\n13         LightGBM_BAG_L2_FULL   -1743.255667 -3894.707823  root_mean_squared_error        0.256483            NaN  3.377595                 0.004111                     NaN           0.312394            2       True         10\n14          XGBoost_BAG_L1_FULL   -2245.433966 -3941.359884  root_mean_squared_error        0.013265            NaN  0.123036                 0.013265                     NaN           0.123036            1       True          6\n15          XGBoost_BAG_L2_FULL   -2454.083373 -3929.201875  root_mean_squared_error        0.267445            NaN  3.256454                 0.015073                     NaN           0.191253            2       True         14\n    0    = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n    86s  = DyStack   runtime |  3514s    = Remaining runtime\nStarting main fit with num_stack_levels=0.\n    For future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\nBeginning AutoGluon training ... Time limit = 3514s\nAutoGluon will save models to \"AutogluonModels/ag-20241214_134505\"\nTrain Data Rows:    2750\nTrain Data Columns: 17\nLabel Column:       TargetSales\nProblem Type:       regression\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    480433.27 MB\n    Train Data (Original)  Memory Usage: 0.36 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    0.1s = Fit runtime\n    17 features in original data used to generate 17 features in processed data.\n    Train Data (Processed) Memory Usage: 0.36 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.08s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n}\nExcluded models: ['NN_TORCH', 'FASTAI'] (Specified by `excluded_model_types`)\nFitting 7 L1 models ...\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 3513.91s of the 3513.9s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\n\n[1000]  valid_set's rmse: 9800.07\n[2000]  valid_set's rmse: 9792.42\n[3000]  valid_set's rmse: 9791.47\n\n\n    -3713.1197   = Validation score   (-root_mean_squared_error)\n    8.47s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ... Training model for up to 3505.33s of the 3505.33s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\n\n[1000]  valid_set's rmse: 9561.64\n[2000]  valid_set's rmse: 9538.68\n\n\n    -3635.1505   = Validation score   (-root_mean_squared_error)\n    6.15s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: RandomForestMSE_BAG_L1 ... Training model for up to 3499.08s of the 3499.08s of remaining time.\n    -4135.0334   = Validation score   (-root_mean_squared_error)\n    0.82s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ... Training model for up to 3497.99s of the 3497.99s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -3669.0125   = Validation score   (-root_mean_squared_error)\n    18.54s   = Training   runtime\n    0.02s    = Validation runtime\nFitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 3479.34s of the 3479.34s of remaining time.\n    -3678.3921   = Validation score   (-root_mean_squared_error)\n    0.66s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ... Training model for up to 3478.41s of the 3478.41s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -3785.5048   = Validation score   (-root_mean_squared_error)\n    5.79s    = Training   runtime\n    0.04s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ... Training model for up to 3472.52s of the 3472.51s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -3704.5742   = Validation score   (-root_mean_squared_error)\n    11.91s   = Training   runtime\n    0.01s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 3460.51s of remaining time.\n    Ensemble Weights: {'LightGBM_BAG_L1': 0.5, 'ExtraTreesMSE_BAG_L1': 0.35, 'CatBoost_BAG_L1': 0.15}\n    -3608.5561   = Validation score   (-root_mean_squared_error)\n    0.02s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 53.55s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 6308.2 rows/s (344 batch size)\nAutomatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\nRefitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n    Models trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n    This process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n    To learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\nFitting 1 L1 models ...\nFitting model: LightGBMXT_BAG_L1_FULL ...\n    0.57s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBM_BAG_L1_FULL ...\n    0.36s    = Training   runtime\nFitting model: RandomForestMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.82s    = Training   runtime\n    0.18s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: CatBoost_BAG_L1_FULL ...\n    0.81s    = Training   runtime\nFitting model: ExtraTreesMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.66s    = Training   runtime\n    0.18s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: XGBoost_BAG_L1_FULL ...\n    0.16s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBMLarge_BAG_L1_FULL ...\n    0.29s    = Training   runtime\nFitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n    Ensemble Weights: {'LightGBM_BAG_L1': 0.5, 'ExtraTreesMSE_BAG_L1': 0.35, 'CatBoost_BAG_L1': 0.15}\n    0.02s    = Training   runtime\nUpdated best model to \"WeightedEnsemble_L2_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L2_FULL\" for predict() and predict_proba().\nRefit complete, total runtime = 2.49s ... Best model: \"WeightedEnsemble_L2_FULL\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241214_134505\")\n\n\n\ntest_df['pred_baseline'] = predictor.predict(test_df[selected_features])\n\n\nmetric_baseline = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_baseline'])\nmetric_baseline['model'] = 'baseline'\nmetric_baseline\n\n{'root_mean_squared_error': 3162.478744240967,\n 'mean_squared_error': 10001271.807775924,\n 'mean_absolute_error': 715.6442657130541,\n 'r2': 0.3816166296854987,\n 'pearsonr': 0.6190719671013133,\n 'spearmanr': 0.47008461549340863,\n 'median_absolute_error': 232.98208312988282,\n 'earths_mover_distance': 287.77728784026124,\n 'model': 'baseline'}"
  },
  {
    "objectID": "notebook/sales_prediction.html#regression-on-winsorized-outcome",
    "href": "notebook/sales_prediction.html#regression-on-winsorized-outcome",
    "title": "Predict Zero-inflated and Long/fat-tailed Outcomes",
    "section": "Regression on Winsorized Outcome",
    "text": "Regression on Winsorized Outcome\nOne possible approach to deal with long/fat-tailed outcome is to train on a winsorized outcome. This may lead to better performance when tested on a winsorized outcome but not so much on original outcome.\n\noutlier_per = 0.99\noutlier_cap_train = train_df['TargetSales'].quantile(outlier_per)\noutlier_cap_train\n\n7180.805199999947\n\n\n\n#winsorize\ntrain_df['TargetSales_win'] = train_df['TargetSales'].map(lambda x: outlier_cap_train if x&gt; outlier_cap_train else x)\ntest_df['TargetSales_win'] = test_df['TargetSales'].map(lambda x: outlier_cap_train if x&gt; outlier_cap_train else x)\n\n\npredictor = TabularPredictor(label='TargetSales_win').fit(train_df[selected_features+['TargetSales_win']],\n                                                      presets=preset,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20241214_134727\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.1.1\nPython Version:     3.9.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP Wed Oct 23 01:22:11 UTC 2024\nCPU Count:          64\nMemory Avail:       468.94 GB / 480.23 GB (97.6%)\nDisk Space Avail:   1451.56 GB / 1968.52 GB (73.7%)\n===================================================\nPresets specified: ['good_quality']\nSetting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\nStack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\nNote: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n    You can avoid this risk by setting `save_bag_folds=True`.\nDyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n    This is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n    Running DyStack for up to 900s of the 3600s of remaining time (25%).\n/home/charipol/miniconda3/lib/python3.9/site-packages/autogluon/tabular/predictor/predictor.py:1242: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: ValueError('ray==2.40.0 detected. 2.10.0 &lt;= ray &lt; 2.11.0 is required. You can use pip to install certain version of ray `pip install ray==2.10.0` ')\n  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n        Context path: \"AutogluonModels/ag-20241214_134727/ds_sub_fit/sub_fit_ho\"\nRunning DyStack sub-fit ...\nBeginning AutoGluon training ... Time limit = 900s\nAutoGluon will save models to \"AutogluonModels/ag-20241214_134727/ds_sub_fit/sub_fit_ho\"\nTrain Data Rows:    2444\nTrain Data Columns: 17\nLabel Column:       TargetSales_win\nProblem Type:       regression\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    480196.12 MB\n    Train Data (Original)  Memory Usage: 0.32 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    0.1s = Fit runtime\n    17 features in original data used to generate 17 features in processed data.\n    Train Data (Processed) Memory Usage: 0.32 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.07s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n}\nAutoGluon will fit 2 stack levels (L1 to L2) ...\nExcluded models: ['NN_TORCH', 'FASTAI'] (Specified by `excluded_model_types`)\nFitting 7 L1 models ...\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 599.8s of the 899.93s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -704.0735    = Validation score   (-root_mean_squared_error)\n    4.98s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ... Training model for up to 594.77s of the 894.89s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -700.8029    = Validation score   (-root_mean_squared_error)\n    4.27s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: RandomForestMSE_BAG_L1 ... Training model for up to 590.42s of the 890.54s of remaining time.\n    -708.5579    = Validation score   (-root_mean_squared_error)\n    0.74s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ... Training model for up to 589.41s of the 889.53s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -682.2162    = Validation score   (-root_mean_squared_error)\n    6.8s     = Training   runtime\n    0.02s    = Validation runtime\nFitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 582.52s of the 882.64s of remaining time.\n    -688.9972    = Validation score   (-root_mean_squared_error)\n    0.64s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ... Training model for up to 581.63s of the 881.75s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -710.5012    = Validation score   (-root_mean_squared_error)\n    5.15s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ... Training model for up to 576.4s of the 876.52s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -715.783     = Validation score   (-root_mean_squared_error)\n    16.49s   = Training   runtime\n    0.01s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 859.93s of remaining time.\n    Ensemble Weights: {'CatBoost_BAG_L1': 0.5, 'ExtraTreesMSE_BAG_L1': 0.25, 'XGBoost_BAG_L1': 0.2, 'LightGBMXT_BAG_L1': 0.05}\n    -677.7482    = Validation score   (-root_mean_squared_error)\n    0.03s    = Training   runtime\n    0.0s     = Validation runtime\nExcluded models: ['NN_TORCH', 'FASTAI'] (Specified by `excluded_model_types`)\nFitting 7 L2 models ...\nFitting model: LightGBMXT_BAG_L2 ... Training model for up to 859.84s of the 859.83s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -701.3347    = Validation score   (-root_mean_squared_error)\n    5.45s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBM_BAG_L2 ... Training model for up to 854.32s of the 854.31s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\n\n[1000]  valid_set's rmse: 622.335\n[2000]  valid_set's rmse: 619.896\n[3000]  valid_set's rmse: 619.36\n[4000]  valid_set's rmse: 619.228\n[5000]  valid_set's rmse: 619.165\n[6000]  valid_set's rmse: 619.15\n[7000]  valid_set's rmse: 619.144\n[8000]  valid_set's rmse: 619.142\n[9000]  valid_set's rmse: 619.14\n[10000] valid_set's rmse: 619.14\n\n\n    -669.5782    = Validation score   (-root_mean_squared_error)\n    16.28s   = Training   runtime\n    0.04s    = Validation runtime\nFitting model: RandomForestMSE_BAG_L2 ... Training model for up to 837.9s of the 837.89s of remaining time.\n    -702.8194    = Validation score   (-root_mean_squared_error)\n    0.88s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: CatBoost_BAG_L2 ... Training model for up to 836.75s of the 836.74s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -679.668     = Validation score   (-root_mean_squared_error)\n    13.36s   = Training   runtime\n    0.03s    = Validation runtime\nFitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 823.27s of the 823.26s of remaining time.\n    -688.2802    = Validation score   (-root_mean_squared_error)\n    0.68s    = Training   runtime\n    0.17s    = Validation runtime\nFitting model: XGBoost_BAG_L2 ... Training model for up to 822.32s of the 822.31s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -706.5666    = Validation score   (-root_mean_squared_error)\n    7.33s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L2 ... Training model for up to 814.89s of the 814.88s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -701.7902    = Validation score   (-root_mean_squared_error)\n    15.1s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 799.67s of remaining time.\n    Ensemble Weights: {'LightGBM_BAG_L2': 0.632, 'CatBoost_BAG_L1': 0.158, 'ExtraTreesMSE_BAG_L1': 0.105, 'XGBoost_BAG_L1': 0.105}\n    -664.9152    = Validation score   (-root_mean_squared_error)\n    0.03s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 100.43s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1980.5 rows/s (306 batch size)\nAutomatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\nRefitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n    Models trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n    This process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n    To learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\nFitting 1 L1 models ...\nFitting model: LightGBMXT_BAG_L1_FULL ...\n    0.32s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBM_BAG_L1_FULL ...\n    0.23s    = Training   runtime\nFitting model: RandomForestMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.74s    = Training   runtime\n    0.18s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: CatBoost_BAG_L1_FULL ...\n    0.29s    = Training   runtime\nFitting model: ExtraTreesMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.64s    = Training   runtime\n    0.18s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: XGBoost_BAG_L1_FULL ...\n    0.08s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBMLarge_BAG_L1_FULL ...\n    0.82s    = Training   runtime\nFitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n    Ensemble Weights: {'CatBoost_BAG_L1': 0.5, 'ExtraTreesMSE_BAG_L1': 0.25, 'XGBoost_BAG_L1': 0.2, 'LightGBMXT_BAG_L1': 0.05}\n    0.03s    = Training   runtime\nFitting 1 L2 models ...\nFitting model: LightGBMXT_BAG_L2_FULL ...\n    0.33s    = Training   runtime\nFitting 1 L2 models ...\nFitting model: LightGBM_BAG_L2_FULL ...\n    1.57s    = Training   runtime\nFitting model: RandomForestMSE_BAG_L2_FULL | Skipping fit via cloning parent ...\n    0.88s    = Training   runtime\n    0.18s    = Validation runtime\nFitting 1 L2 models ...\nFitting model: CatBoost_BAG_L2_FULL ...\n    0.62s    = Training   runtime\nFitting model: ExtraTreesMSE_BAG_L2_FULL | Skipping fit via cloning parent ...\n    0.68s    = Training   runtime\n    0.17s    = Validation runtime\nFitting 1 L2 models ...\nFitting model: XGBoost_BAG_L2_FULL ...\n    0.24s    = Training   runtime\nFitting 1 L2 models ...\nFitting model: LightGBMLarge_BAG_L2_FULL ...\n    0.68s    = Training   runtime\nFitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n    Ensemble Weights: {'LightGBM_BAG_L2': 0.632, 'CatBoost_BAG_L1': 0.158, 'ExtraTreesMSE_BAG_L1': 0.105, 'XGBoost_BAG_L1': 0.105}\n    0.03s    = Training   runtime\nUpdated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\nRefit complete, total runtime = 5.9s ... Best model: \"WeightedEnsemble_L3_FULL\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241214_134727/ds_sub_fit/sub_fit_ho\")\nDeleting DyStack predictor artifacts (clean_up_fits=True) ...\nLeaderboard on holdout data (DyStack):\n                          model  score_holdout   score_val              eval_metric  pred_time_test  pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0           XGBoost_BAG_L2_FULL    -673.227470 -706.566643  root_mean_squared_error        0.261725            NaN  3.356293                 0.014406                     NaN           0.243477            2       True         14\n1          CatBoost_BAG_L2_FULL    -685.276375 -679.668006  root_mean_squared_error        0.253890            NaN  3.729317                 0.006571                     NaN           0.616501            2       True         12\n2     ExtraTreesMSE_BAG_L2_FULL    -686.432329 -688.280166  root_mean_squared_error        0.384582            NaN  3.796237                 0.137263                0.174881           0.683421            2       True         13\n3      WeightedEnsemble_L2_FULL    -687.292057 -677.748155  root_mean_squared_error        0.129074            NaN  1.357070                 0.002933                     NaN           0.026227            2       True          8\n4          CatBoost_BAG_L1_FULL    -688.830702 -682.216238  root_mean_squared_error        0.008315            NaN  0.291358                 0.008315                     NaN           0.291358            1       True          4\n5   RandomForestMSE_BAG_L2_FULL    -690.155342 -702.819447  root_mean_squared_error        0.358528            NaN  3.991187                 0.111209                0.176151           0.878371            2       True         11\n6     LightGBMLarge_BAG_L2_FULL    -699.457560 -701.790157  root_mean_squared_error        0.256358            NaN  3.792534                 0.009039                     NaN           0.679718            2       True         15\n7      WeightedEnsemble_L3_FULL    -699.646914 -664.915201  root_mean_squared_error        0.269660            NaN  4.711405                 0.004106                     NaN           0.030340            3       True         16\n8   RandomForestMSE_BAG_L1_FULL    -700.107179 -708.557877  root_mean_squared_error        0.111719       0.175315  0.737258                 0.111719                0.175315           0.737258            1       True          3\n9     ExtraTreesMSE_BAG_L1_FULL    -701.853556 -688.997247  root_mean_squared_error        0.105658       0.176891  0.644825                 0.105658                0.176891           0.644825            1       True          5\n10          XGBoost_BAG_L1_FULL    -717.776000 -710.501170  root_mean_squared_error        0.008964            NaN  0.078643                 0.008964                     NaN           0.078643            1       True          6\n11       LightGBMXT_BAG_L2_FULL    -723.560168 -701.334719  root_mean_squared_error        0.251497            NaN  3.444983                 0.004178                     NaN           0.332167            2       True          9\n12         LightGBM_BAG_L1_FULL    -726.112842 -700.802863  root_mean_squared_error        0.002110            NaN  0.227411                 0.002110                     NaN           0.227411            1       True          2\n13         LightGBM_BAG_L2_FULL    -728.829307 -669.578190  root_mean_squared_error        0.265554            NaN  4.681065                 0.018236                     NaN           1.568249            2       True         10\n14       LightGBMXT_BAG_L1_FULL    -733.594747 -704.073534  root_mean_squared_error        0.003205            NaN  0.316018                 0.003205                     NaN           0.316018            1       True          1\n15    LightGBMLarge_BAG_L1_FULL    -766.964045 -715.782974  root_mean_squared_error        0.007349            NaN  0.817303                 0.007349                     NaN           0.817303            1       True          7\n    0    = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n    107s     = DyStack   runtime |  3493s    = Remaining runtime\nStarting main fit with num_stack_levels=0.\n    For future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\nBeginning AutoGluon training ... Time limit = 3493s\nAutoGluon will save models to \"AutogluonModels/ag-20241214_134727\"\nTrain Data Rows:    2750\nTrain Data Columns: 17\nLabel Column:       TargetSales_win\nProblem Type:       regression\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    479639.42 MB\n    Train Data (Original)  Memory Usage: 0.36 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    0.1s = Fit runtime\n    17 features in original data used to generate 17 features in processed data.\n    Train Data (Processed) Memory Usage: 0.36 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.08s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n}\nExcluded models: ['NN_TORCH', 'FASTAI'] (Specified by `excluded_model_types`)\nFitting 7 L1 models ...\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 3492.88s of the 3492.88s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -710.5609    = Validation score   (-root_mean_squared_error)\n    5.2s     = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ... Training model for up to 3487.59s of the 3487.59s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -696.6213    = Validation score   (-root_mean_squared_error)\n    4.91s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: RandomForestMSE_BAG_L1 ... Training model for up to 3482.58s of the 3482.58s of remaining time.\n    -706.2702    = Validation score   (-root_mean_squared_error)\n    0.77s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ... Training model for up to 3481.53s of the 3481.53s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -668.1395    = Validation score   (-root_mean_squared_error)\n    8.92s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 3472.5s of the 3472.5s of remaining time.\n    -688.8913    = Validation score   (-root_mean_squared_error)\n    0.62s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ... Training model for up to 3471.6s of the 3471.6s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -699.3326    = Validation score   (-root_mean_squared_error)\n    5.72s    = Training   runtime\n    0.04s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ... Training model for up to 3465.77s of the 3465.77s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -714.8496    = Validation score   (-root_mean_squared_error)\n    14.1s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 3451.57s of remaining time.\n    Ensemble Weights: {'CatBoost_BAG_L1': 0.833, 'XGBoost_BAG_L1': 0.125, 'ExtraTreesMSE_BAG_L1': 0.042}\n    -667.3394    = Validation score   (-root_mean_squared_error)\n    0.02s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 41.44s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 4132.1 rows/s (344 batch size)\nAutomatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\nRefitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n    Models trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n    This process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n    To learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\nFitting 1 L1 models ...\nFitting model: LightGBMXT_BAG_L1_FULL ...\n    0.33s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBM_BAG_L1_FULL ...\n    0.29s    = Training   runtime\nFitting model: RandomForestMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.77s    = Training   runtime\n    0.18s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: CatBoost_BAG_L1_FULL ...\n    0.43s    = Training   runtime\nFitting model: ExtraTreesMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.62s    = Training   runtime\n    0.18s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: XGBoost_BAG_L1_FULL ...\n    0.12s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBMLarge_BAG_L1_FULL ...\n    0.67s    = Training   runtime\nFitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n    Ensemble Weights: {'CatBoost_BAG_L1': 0.833, 'XGBoost_BAG_L1': 0.125, 'ExtraTreesMSE_BAG_L1': 0.042}\n    0.02s    = Training   runtime\nUpdated best model to \"WeightedEnsemble_L2_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L2_FULL\" for predict() and predict_proba().\nRefit complete, total runtime = 2.18s ... Best model: \"WeightedEnsemble_L2_FULL\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241214_134727\")\n\n\n\ntest_df['pred_winsorized'] = predictor.predict(test_df[selected_features])\n\n\nmetric_winsorized = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_winsorized'])\nmetric_winsorized['model'] = 'winsorized'\nmetric_winsorized\n\n{'root_mean_squared_error': 3623.576377551195,\n 'mean_squared_error': 13130305.76394704,\n 'mean_absolute_error': 627.7880071099414,\n 'r2': 0.18814697894155963,\n 'pearsonr': 0.5757989413256978,\n 'spearmanr': 0.504301956183441,\n 'median_absolute_error': 219.62248107910156,\n 'earths_mover_distance': 432.1288432991232,\n 'model': 'winsorized'}\n\n\n\ncalculate_regression_metrics(test_df['TargetSales_win'], test_df['pred_winsorized'])\n\n{'root_mean_squared_error': 673.4846433338375,\n 'mean_squared_error': 453581.5648065064,\n 'mean_absolute_error': 376.77603327273135,\n 'r2': 0.6171771763549553,\n 'pearsonr': 0.7865724180212539,\n 'spearmanr': 0.504299950810919,\n 'median_absolute_error': 218.8311004638672,\n 'earths_mover_distance': 181.1168694619127}"
  },
  {
    "objectID": "notebook/sales_prediction.html#log1p-regression",
    "href": "notebook/sales_prediction.html#log1p-regression",
    "title": "Predict Zero-inflated and Long/fat-tailed Outcomes",
    "section": "Log1p Regression",
    "text": "Log1p Regression\nLog transformation handles long/fat-tailed distribution and is especially useful for certain models since the transformed distribution is roughly normal. However, it cannot handle zero-valued outcome and oftentimes scientists end up adding 1 to the outcome (so often that numpy even has a function for it). This not only introduces bias to the prediction, but also does not solve the zero-inflation as it becomes one-inflation instead.\n\n#log\ntrain_df['TargetSales_log1p'] = train_df['TargetSales'].map(np.log1p)\ntest_df['TargetSales_log1p'] = test_df['TargetSales'].map(np.log1p)\n\n\n#from zero-inflated to one-inflated\ntrain_df['TargetSales_log1p'].hist()\n\n\n\n\n\n\n\n\n\npredictor = TabularPredictor(label='TargetSales_log1p').fit(train_df[selected_features+['TargetSales_log1p']],\n                                                      presets=preset,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20241214_134958\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.1.1\nPython Version:     3.9.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP Wed Oct 23 01:22:11 UTC 2024\nCPU Count:          64\nMemory Avail:       468.19 GB / 480.23 GB (97.5%)\nDisk Space Avail:   1451.46 GB / 1968.52 GB (73.7%)\n===================================================\nPresets specified: ['good_quality']\nSetting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\nStack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\nNote: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n    You can avoid this risk by setting `save_bag_folds=True`.\nDyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n    This is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n    Running DyStack for up to 900s of the 3600s of remaining time (25%).\n/home/charipol/miniconda3/lib/python3.9/site-packages/autogluon/tabular/predictor/predictor.py:1242: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: ValueError('ray==2.40.0 detected. 2.10.0 &lt;= ray &lt; 2.11.0 is required. You can use pip to install certain version of ray `pip install ray==2.10.0` ')\n  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n        Context path: \"AutogluonModels/ag-20241214_134958/ds_sub_fit/sub_fit_ho\"\nRunning DyStack sub-fit ...\nBeginning AutoGluon training ... Time limit = 900s\nAutoGluon will save models to \"AutogluonModels/ag-20241214_134958/ds_sub_fit/sub_fit_ho\"\nTrain Data Rows:    2444\nTrain Data Columns: 17\nLabel Column:       TargetSales_log1p\nProblem Type:       regression\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    479431.23 MB\n    Train Data (Original)  Memory Usage: 0.32 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    0.0s = Fit runtime\n    17 features in original data used to generate 17 features in processed data.\n    Train Data (Processed) Memory Usage: 0.32 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.06s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n}\nAutoGluon will fit 2 stack levels (L1 to L2) ...\nExcluded models: ['NN_TORCH', 'FASTAI'] (Specified by `excluded_model_types`)\nFitting 7 L1 models ...\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 599.81s of the 899.94s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -2.7823  = Validation score   (-root_mean_squared_error)\n    4.54s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ... Training model for up to 595.21s of the 895.34s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -2.8135  = Validation score   (-root_mean_squared_error)\n    3.9s     = Training   runtime\n    0.01s    = Validation runtime\nFitting model: RandomForestMSE_BAG_L1 ... Training model for up to 591.22s of the 891.35s of remaining time.\n    -2.8314  = Validation score   (-root_mean_squared_error)\n    0.8s     = Training   runtime\n    0.18s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ... Training model for up to 590.14s of the 890.26s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -2.7808  = Validation score   (-root_mean_squared_error)\n    5.75s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 584.28s of the 884.41s of remaining time.\n    -2.8158  = Validation score   (-root_mean_squared_error)\n    0.68s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ... Training model for up to 583.31s of the 883.44s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -2.8362  = Validation score   (-root_mean_squared_error)\n    5.3s     = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ... Training model for up to 577.92s of the 878.05s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -2.8628  = Validation score   (-root_mean_squared_error)\n    12.04s   = Training   runtime\n    0.01s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 865.92s of remaining time.\n    Ensemble Weights: {'CatBoost_BAG_L1': 0.478, 'LightGBMXT_BAG_L1': 0.435, 'ExtraTreesMSE_BAG_L1': 0.087}\n    -2.7756  = Validation score   (-root_mean_squared_error)\n    0.02s    = Training   runtime\n    0.0s     = Validation runtime\nExcluded models: ['NN_TORCH', 'FASTAI'] (Specified by `excluded_model_types`)\nFitting 7 L2 models ...\nFitting model: LightGBMXT_BAG_L2 ... Training model for up to 865.86s of the 865.85s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\n\n[1000]  valid_set's rmse: 2.69718\n[2000]  valid_set's rmse: 2.65909\n[3000]  valid_set's rmse: 2.65127\n[4000]  valid_set's rmse: 2.64831\n[5000]  valid_set's rmse: 2.64686\n[6000]  valid_set's rmse: 2.64619\n[7000]  valid_set's rmse: 2.64592\n[8000]  valid_set's rmse: 2.64577\n[9000]  valid_set's rmse: 2.64574\n[10000] valid_set's rmse: 2.64573\n[1000]  valid_set's rmse: 2.69753\n[2000]  valid_set's rmse: 2.65357\n[3000]  valid_set's rmse: 2.64919\n[4000]  valid_set's rmse: 2.64645\n[5000]  valid_set's rmse: 2.64479\n[6000]  valid_set's rmse: 2.64422\n[7000]  valid_set's rmse: 2.6439\n[8000]  valid_set's rmse: 2.64387\n[9000]  valid_set's rmse: 2.64384\n[10000] valid_set's rmse: 2.64383\n[1000]  valid_set's rmse: 2.71185\n[2000]  valid_set's rmse: 2.66777\n[3000]  valid_set's rmse: 2.65641\n[4000]  valid_set's rmse: 2.65399\n[5000]  valid_set's rmse: 2.65317\n[6000]  valid_set's rmse: 2.65306\n[7000]  valid_set's rmse: 2.65305\n[1000]  valid_set's rmse: 2.73753\n[2000]  valid_set's rmse: 2.69713\n[3000]  valid_set's rmse: 2.68365\n[4000]  valid_set's rmse: 2.67838\n[5000]  valid_set's rmse: 2.67756\n[6000]  valid_set's rmse: 2.67725\n[7000]  valid_set's rmse: 2.67727\n[1000]  valid_set's rmse: 2.51029\n[2000]  valid_set's rmse: 2.46542\n[3000]  valid_set's rmse: 2.45449\n[4000]  valid_set's rmse: 2.45128\n[5000]  valid_set's rmse: 2.44983\n[6000]  valid_set's rmse: 2.44947\n[7000]  valid_set's rmse: 2.44914\n[8000]  valid_set's rmse: 2.44909\n[9000]  valid_set's rmse: 2.4491\n[10000] valid_set's rmse: 2.44914\n\n\n    -2.6944  = Validation score   (-root_mean_squared_error)\n    52.47s   = Training   runtime\n    0.08s    = Validation runtime\nFitting model: LightGBM_BAG_L2 ... Training model for up to 813.19s of the 813.18s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\n\n[1000]  valid_set's rmse: 2.64699\n[2000]  valid_set's rmse: 2.64384\n[3000]  valid_set's rmse: 2.64347\n[4000]  valid_set's rmse: 2.64345\n[1000]  valid_set's rmse: 2.6483\n[1000]  valid_set's rmse: 2.87823\n\n\n    -2.7598  = Validation score   (-root_mean_squared_error)\n    9.99s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: RandomForestMSE_BAG_L2 ... Training model for up to 803.1s of the 803.09s of remaining time.\n    -2.8088  = Validation score   (-root_mean_squared_error)\n    0.75s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: CatBoost_BAG_L2 ... Training model for up to 802.07s of the 802.06s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -2.759   = Validation score   (-root_mean_squared_error)\n    92.85s   = Training   runtime\n    0.04s    = Validation runtime\nFitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 709.08s of the 709.07s of remaining time.\n    -2.778   = Validation score   (-root_mean_squared_error)\n    0.66s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: XGBoost_BAG_L2 ... Training model for up to 708.14s of the 708.13s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -2.7901  = Validation score   (-root_mean_squared_error)\n    6.46s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L2 ... Training model for up to 701.58s of the 701.57s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\n\n[1000]  valid_set's rmse: 2.76447\n[1000]  valid_set's rmse: 2.80146\n[2000]  valid_set's rmse: 2.8014\n[3000]  valid_set's rmse: 2.8014\n[4000]  valid_set's rmse: 2.8014\n[1000]  valid_set's rmse: 2.88446\n[2000]  valid_set's rmse: 2.88439\n\n\n    -2.7943  = Validation score   (-root_mean_squared_error)\n    45.94s   = Training   runtime\n    0.04s    = Validation runtime\nFitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 655.49s of remaining time.\n    Ensemble Weights: {'LightGBMXT_BAG_L2': 0.72, 'CatBoost_BAG_L1': 0.12, 'LightGBMXT_BAG_L1': 0.04, 'LightGBM_BAG_L2': 0.04, 'XGBoost_BAG_L2': 0.04, 'LightGBMLarge_BAG_L2': 0.04}\n    -2.6854  = Validation score   (-root_mean_squared_error)\n    0.03s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 244.6s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1078.2 rows/s (306 batch size)\nAutomatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\nRefitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n    Models trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n    This process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n    To learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\nFitting 1 L1 models ...\nFitting model: LightGBMXT_BAG_L1_FULL ...\n    0.23s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBM_BAG_L1_FULL ...\n    0.17s    = Training   runtime\nFitting model: RandomForestMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.8s     = Training   runtime\n    0.18s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: CatBoost_BAG_L1_FULL ...\n    0.21s    = Training   runtime\nFitting model: ExtraTreesMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.68s    = Training   runtime\n    0.18s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: XGBoost_BAG_L1_FULL ...\n    0.08s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBMLarge_BAG_L1_FULL ...\n    0.37s    = Training   runtime\nFitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n    Ensemble Weights: {'CatBoost_BAG_L1': 0.478, 'LightGBMXT_BAG_L1': 0.435, 'ExtraTreesMSE_BAG_L1': 0.087}\n    0.02s    = Training   runtime\nFitting 1 L2 models ...\nFitting model: LightGBMXT_BAG_L2_FULL ...\n    4.94s    = Training   runtime\nFitting 1 L2 models ...\nFitting model: LightGBM_BAG_L2_FULL ...\n    0.76s    = Training   runtime\nFitting model: RandomForestMSE_BAG_L2_FULL | Skipping fit via cloning parent ...\n    0.75s    = Training   runtime\n    0.18s    = Validation runtime\nFitting 1 L2 models ...\nFitting model: CatBoost_BAG_L2_FULL ...\n    3.03s    = Training   runtime\nFitting model: ExtraTreesMSE_BAG_L2_FULL | Skipping fit via cloning parent ...\n    0.66s    = Training   runtime\n    0.18s    = Validation runtime\nFitting 1 L2 models ...\nFitting model: XGBoost_BAG_L2_FULL ...\n    0.16s    = Training   runtime\nFitting 1 L2 models ...\nFitting model: LightGBMLarge_BAG_L2_FULL ...\n    3.93s    = Training   runtime\nFitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n    Ensemble Weights: {'LightGBMXT_BAG_L2': 0.72, 'CatBoost_BAG_L1': 0.12, 'LightGBMXT_BAG_L1': 0.04, 'LightGBM_BAG_L2': 0.04, 'XGBoost_BAG_L2': 0.04, 'LightGBMLarge_BAG_L2': 0.04}\n    0.03s    = Training   runtime\nUpdated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\nRefit complete, total runtime = 14.81s ... Best model: \"WeightedEnsemble_L3_FULL\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241214_134958/ds_sub_fit/sub_fit_ho\")\nDeleting DyStack predictor artifacts (clean_up_fits=True) ...\nLeaderboard on holdout data (DyStack):\n                          model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0          CatBoost_BAG_L1_FULL      -2.885523  -2.780750  root_mean_squared_error        0.019700            NaN   0.210621                 0.019700                     NaN           0.210621            1       True          4\n1      WeightedEnsemble_L2_FULL      -2.894191  -2.775644  root_mean_squared_error        0.155056            NaN   1.140924                 0.002549                     NaN           0.019073            2       True          8\n2   RandomForestMSE_BAG_L2_FULL      -2.894937  -2.808770  root_mean_squared_error        0.407366            NaN   3.299085                 0.126307                0.179813           0.754930            2       True         11\n3     ExtraTreesMSE_BAG_L2_FULL      -2.896741  -2.777983  root_mean_squared_error        0.400050            NaN   3.203568                 0.118991                0.182625           0.659413            2       True         13\n4        LightGBMXT_BAG_L1_FULL      -2.908863  -2.782297  root_mean_squared_error        0.002410            NaN   0.229015                 0.002410                     NaN           0.229015            1       True          1\n5          CatBoost_BAG_L2_FULL      -2.922107  -2.759026  root_mean_squared_error        0.290997            NaN   5.576536                 0.009938                     NaN           3.032381            2       True         12\n6          LightGBM_BAG_L2_FULL      -2.931031  -2.759814  root_mean_squared_error        0.291885            NaN   3.309105                 0.010826                     NaN           0.764950            2       True         10\n7           XGBoost_BAG_L2_FULL      -2.938193  -2.790059  root_mean_squared_error        0.292573            NaN   2.701340                 0.011514                     NaN           0.157185            2       True         14\n8      WeightedEnsemble_L3_FULL      -2.942265  -2.685363  root_mean_squared_error        0.414771            NaN  12.371993                 0.005430                     NaN           0.029768            3       True         16\n9     ExtraTreesMSE_BAG_L1_FULL      -2.946022  -2.815757  root_mean_squared_error        0.130396       0.183574   0.682215                 0.130396                0.183574           0.682215            1       True          5\n10         LightGBM_BAG_L1_FULL      -2.953480  -2.813496  root_mean_squared_error        0.001765            NaN   0.174514                 0.001765                     NaN           0.174514            1       True          2\n11          XGBoost_BAG_L1_FULL      -2.972277  -2.836214  root_mean_squared_error        0.010018            NaN   0.076469                 0.010018                     NaN           0.076469            1       True          6\n12    LightGBMLarge_BAG_L2_FULL      -2.977587  -2.794323  root_mean_squared_error        0.327995            NaN   6.475816                 0.046936                     NaN           3.931662            2       True         15\n13  RandomForestMSE_BAG_L1_FULL      -2.985264  -2.831375  root_mean_squared_error        0.111768       0.181522   0.798739                 0.111768                0.181522           0.798739            1       True          3\n14       LightGBMXT_BAG_L2_FULL      -2.995407  -2.694352  root_mean_squared_error        0.340065            NaN   7.488428                 0.059006                     NaN           4.944273            2       True          9\n15    LightGBMLarge_BAG_L1_FULL      -3.050660  -2.862792  root_mean_squared_error        0.005002            NaN   0.372581                 0.005002                     NaN           0.372581            1       True          7\n    0    = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n    260s     = DyStack   runtime |  3340s    = Remaining runtime\nStarting main fit with num_stack_levels=0.\n    For future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\nBeginning AutoGluon training ... Time limit = 3340s\nAutoGluon will save models to \"AutogluonModels/ag-20241214_134958\"\nTrain Data Rows:    2750\nTrain Data Columns: 17\nLabel Column:       TargetSales_log1p\nProblem Type:       regression\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    479126.25 MB\n    Train Data (Original)  Memory Usage: 0.36 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    0.0s = Fit runtime\n    17 features in original data used to generate 17 features in processed data.\n    Train Data (Processed) Memory Usage: 0.36 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.07s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n}\nExcluded models: ['NN_TORCH', 'FASTAI'] (Specified by `excluded_model_types`)\nFitting 7 L1 models ...\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 3339.69s of the 3339.68s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -2.7861  = Validation score   (-root_mean_squared_error)\n    5.11s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ... Training model for up to 3334.48s of the 3334.48s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -2.8189  = Validation score   (-root_mean_squared_error)\n    4.72s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: RandomForestMSE_BAG_L1 ... Training model for up to 3329.67s of the 3329.67s of remaining time.\n    -2.8468  = Validation score   (-root_mean_squared_error)\n    0.72s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ... Training model for up to 3328.66s of the 3328.66s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -2.7963  = Validation score   (-root_mean_squared_error)\n    5.43s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 3323.13s of the 3323.13s of remaining time.\n    -2.8191  = Validation score   (-root_mean_squared_error)\n    0.66s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ... Training model for up to 3322.18s of the 3322.18s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -2.8365  = Validation score   (-root_mean_squared_error)\n    5.51s    = Training   runtime\n    0.04s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ... Training model for up to 3316.57s of the 3316.57s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -2.8667  = Validation score   (-root_mean_squared_error)\n    12.3s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 3304.17s of remaining time.\n    Ensemble Weights: {'LightGBMXT_BAG_L1': 0.714, 'CatBoost_BAG_L1': 0.143, 'ExtraTreesMSE_BAG_L1': 0.143}\n    -2.7845  = Validation score   (-root_mean_squared_error)\n    0.02s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 35.65s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 6632.0 rows/s (344 batch size)\nAutomatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\nRefitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n    Models trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n    This process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n    To learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\nFitting 1 L1 models ...\nFitting model: LightGBMXT_BAG_L1_FULL ...\n    0.21s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBM_BAG_L1_FULL ...\n    0.19s    = Training   runtime\nFitting model: RandomForestMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.72s    = Training   runtime\n    0.19s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: CatBoost_BAG_L1_FULL ...\n    0.18s    = Training   runtime\nFitting model: ExtraTreesMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.66s    = Training   runtime\n    0.19s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: XGBoost_BAG_L1_FULL ...\n    0.09s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBMLarge_BAG_L1_FULL ...\n    0.41s    = Training   runtime\nFitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n    Ensemble Weights: {'LightGBMXT_BAG_L1': 0.714, 'CatBoost_BAG_L1': 0.143, 'ExtraTreesMSE_BAG_L1': 0.143}\n    0.02s    = Training   runtime\nUpdated best model to \"WeightedEnsemble_L2_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L2_FULL\" for predict() and predict_proba().\nRefit complete, total runtime = 1.41s ... Best model: \"WeightedEnsemble_L2_FULL\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241214_134958\")\n\n\n\ntest_df['pred_log1p'] = predictor.predict(test_df[selected_features])\ntest_df['pred_log1p_expm1'] = test_df['pred_log1p'].map(np.expm1)\n\n\nmetric_log1p = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_log1p_expm1'])\nmetric_log1p['model'] = 'log1p'\nmetric_log1p\n\n{'root_mean_squared_error': 3725.342295894091,\n 'mean_squared_error': 13878175.221577456,\n 'mean_absolute_error': 618.9768466651894,\n 'r2': 0.14190585634701047,\n 'pearsonr': 0.5817166874396966,\n 'spearmanr': 0.5338156315937898,\n 'median_absolute_error': 89.55495441784018,\n 'earths_mover_distance': 581.0494444960044,\n 'model': 'log1p'}\n\n\n\ncalculate_regression_metrics(test_df['TargetSales_log1p'], test_df['pred_log1p'])\n\n{'root_mean_squared_error': 2.720047847858299,\n 'mean_squared_error': 7.398660294638562,\n 'mean_absolute_error': 2.418601533469381,\n 'r2': 0.30252750020590236,\n 'pearsonr': 0.5507740732825224,\n 'spearmanr': 0.5338156315937898,\n 'median_absolute_error': 2.349368453025818,\n 'earths_mover_distance': 1.8552344547363062}"
  },
  {
    "objectID": "notebook/sales_prediction.html#hurdle-model",
    "href": "notebook/sales_prediction.html#hurdle-model",
    "title": "Predict Zero-inflated and Long/fat-tailed Outcomes",
    "section": "Hurdle Model",
    "text": "Hurdle Model\nHurdle model is a two-stage approach that handles zero inflation by first having a classification model to predict if the outcome is zero or not, then a regression model, trained only on examples with actual non-zero outcomes, to fit a log-transformed outcome. When retransforming the predictions from log to non-log numbers, we perform correction of underestimation using Duan‚Äôs method. During inference time, we multiply the predictions from the classification and regression model.\n\nBinary Classification\n\ntrain_df['has_purchase'] = train_df.TargetSales.map(lambda x: 1 if x&gt;0 else 0)\ntest_df['has_purchase'] = test_df.TargetSales.map(lambda x: 1 if x&gt;0 else 0)\n\n\ntrain_df['has_purchase'].mean(), test_df['has_purchase'].mean()\n\n(0.5141818181818182, 0.5305232558139535)\n\n\n\npredictor_cls = TabularPredictor(label='has_purchase').fit(train_df[selected_features+['has_purchase']],\n                                                      presets=preset,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20241214_135456\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.1.1\nPython Version:     3.9.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP Wed Oct 23 01:22:11 UTC 2024\nCPU Count:          64\nMemory Avail:       467.88 GB / 480.23 GB (97.4%)\nDisk Space Avail:   1451.34 GB / 1968.52 GB (73.7%)\n===================================================\nPresets specified: ['good_quality']\nSetting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\nStack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\nNote: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n    You can avoid this risk by setting `save_bag_folds=True`.\nDyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n    This is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n    Running DyStack for up to 900s of the 3600s of remaining time (25%).\n/home/charipol/miniconda3/lib/python3.9/site-packages/autogluon/tabular/predictor/predictor.py:1242: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: ValueError('ray==2.40.0 detected. 2.10.0 &lt;= ray &lt; 2.11.0 is required. You can use pip to install certain version of ray `pip install ray==2.10.0` ')\n  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n        Context path: \"AutogluonModels/ag-20241214_135456/ds_sub_fit/sub_fit_ho\"\nRunning DyStack sub-fit ...\nBeginning AutoGluon training ... Time limit = 900s\nAutoGluon will save models to \"AutogluonModels/ag-20241214_135456/ds_sub_fit/sub_fit_ho\"\nTrain Data Rows:    2444\nTrain Data Columns: 17\nLabel Column:       has_purchase\nProblem Type:       binary\nPreprocessing data ...\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    479111.52 MB\n    Train Data (Original)  Memory Usage: 0.32 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    0.1s = Fit runtime\n    17 features in original data used to generate 17 features in processed data.\n    Train Data (Processed) Memory Usage: 0.32 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.08s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n}\nAutoGluon will fit 2 stack levels (L1 to L2) ...\nExcluded models: ['NN_TORCH', 'FASTAI'] (Specified by `excluded_model_types`)\nFitting 9 L1 models ...\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 599.8s of the 899.92s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.6944   = Validation score   (accuracy)\n    4.76s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ... Training model for up to 594.97s of the 895.09s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.687    = Validation score   (accuracy)\n    4.01s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: RandomForestGini_BAG_L1 ... Training model for up to 590.87s of the 890.99s of remaining time.\n    0.6661   = Validation score   (accuracy)\n    0.88s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: RandomForestEntr_BAG_L1 ... Training model for up to 589.72s of the 889.84s of remaining time.\n    0.6543   = Validation score   (accuracy)\n    0.88s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ... Training model for up to 588.58s of the 888.7s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.698    = Validation score   (accuracy)\n    8.46s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 580.02s of the 880.14s of remaining time.\n    0.6731   = Validation score   (accuracy)\n    0.85s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 578.92s of the 879.04s of remaining time.\n    0.6751   = Validation score   (accuracy)\n    0.83s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ... Training model for up to 577.82s of the 877.94s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.6874   = Validation score   (accuracy)\n    4.94s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ... Training model for up to 572.75s of the 872.87s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.6739   = Validation score   (accuracy)\n    16.57s   = Training   runtime\n    0.01s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 856.19s of remaining time.\n    Ensemble Weights: {'CatBoost_BAG_L1': 1.0}\n    0.698    = Validation score   (accuracy)\n    0.17s    = Training   runtime\n    0.0s     = Validation runtime\nExcluded models: ['NN_TORCH', 'FASTAI'] (Specified by `excluded_model_types`)\nFitting 9 L2 models ...\nFitting model: LightGBMXT_BAG_L2 ... Training model for up to 855.98s of the 855.97s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.7017   = Validation score   (accuracy)\n    4.74s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBM_BAG_L2 ... Training model for up to 851.17s of the 851.16s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.7062   = Validation score   (accuracy)\n    4.62s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: RandomForestGini_BAG_L2 ... Training model for up to 846.46s of the 846.45s of remaining time.\n    0.6849   = Validation score   (accuracy)\n    0.86s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: RandomForestEntr_BAG_L2 ... Training model for up to 845.32s of the 845.31s of remaining time.\n    0.6845   = Validation score   (accuracy)\n    0.9s     = Training   runtime\n    0.18s    = Validation runtime\nFitting model: CatBoost_BAG_L2 ... Training model for up to 844.16s of the 844.15s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.7074   = Validation score   (accuracy)\n    12.05s   = Training   runtime\n    0.02s    = Validation runtime\nFitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 831.99s of the 831.98s of remaining time.\n    0.6845   = Validation score   (accuracy)\n    0.84s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 830.87s of the 830.86s of remaining time.\n    0.6796   = Validation score   (accuracy)\n    0.81s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: XGBoost_BAG_L2 ... Training model for up to 829.78s of the 829.77s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.7029   = Validation score   (accuracy)\n    5.77s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L2 ... Training model for up to 823.88s of the 823.87s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.705    = Validation score   (accuracy)\n    17.18s   = Training   runtime\n    0.01s    = Validation runtime\nFitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 806.57s of remaining time.\n    Ensemble Weights: {'CatBoost_BAG_L2': 0.957, 'CatBoost_BAG_L1': 0.043}\n    0.7079   = Validation score   (accuracy)\n    0.34s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 93.82s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1613.2 rows/s (306 batch size)\nAutomatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\nRefitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n    Models trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n    This process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n    To learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\nFitting 1 L1 models ...\nFitting model: LightGBMXT_BAG_L1_FULL ...\n    0.27s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBM_BAG_L1_FULL ...\n    0.21s    = Training   runtime\nFitting model: RandomForestGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.88s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: RandomForestEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.88s    = Training   runtime\n    0.18s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: CatBoost_BAG_L1_FULL ...\n    0.17s    = Training   runtime\nFitting model: ExtraTreesGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.85s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: ExtraTreesEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.83s    = Training   runtime\n    0.19s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: XGBoost_BAG_L1_FULL ...\n    0.1s     = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBMLarge_BAG_L1_FULL ...\n    0.93s    = Training   runtime\nFitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n    Ensemble Weights: {'CatBoost_BAG_L1': 1.0}\n    0.17s    = Training   runtime\nFitting 1 L2 models ...\nFitting model: LightGBMXT_BAG_L2_FULL ...\n    0.23s    = Training   runtime\nFitting 1 L2 models ...\nFitting model: LightGBM_BAG_L2_FULL ...\n    0.23s    = Training   runtime\nFitting model: RandomForestGini_BAG_L2_FULL | Skipping fit via cloning parent ...\n    0.86s    = Training   runtime\n    0.18s    = Validation runtime\nFitting model: RandomForestEntr_BAG_L2_FULL | Skipping fit via cloning parent ...\n    0.9s     = Training   runtime\n    0.18s    = Validation runtime\nFitting 1 L2 models ...\nFitting model: CatBoost_BAG_L2_FULL ...\n    0.58s    = Training   runtime\nFitting model: ExtraTreesGini_BAG_L2_FULL | Skipping fit via cloning parent ...\n    0.84s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: ExtraTreesEntr_BAG_L2_FULL | Skipping fit via cloning parent ...\n    0.81s    = Training   runtime\n    0.18s    = Validation runtime\nFitting 1 L2 models ...\nFitting model: XGBoost_BAG_L2_FULL ...\n    0.12s    = Training   runtime\nFitting 1 L2 models ...\nFitting model: LightGBMLarge_BAG_L2_FULL ...\n    0.89s    = Training   runtime\nFitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n    Ensemble Weights: {'CatBoost_BAG_L2': 0.957, 'CatBoost_BAG_L1': 0.043}\n    0.34s    = Training   runtime\nUpdated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\nRefit complete, total runtime = 4.63s ... Best model: \"WeightedEnsemble_L3_FULL\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241214_135456/ds_sub_fit/sub_fit_ho\")\nDeleting DyStack predictor artifacts (clean_up_fits=True) ...\nLeaderboard on holdout data (DyStack):\n                           model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0           LightGBM_BAG_L1_FULL       0.699346   0.686989    accuracy        0.001600            NaN  0.208330                 0.001600                     NaN           0.208330            1       True          2\n1           CatBoost_BAG_L1_FULL       0.699346   0.698036    accuracy        0.003851            NaN  0.167360                 0.003851                     NaN           0.167360            1       True          5\n2       WeightedEnsemble_L2_FULL       0.699346   0.698036    accuracy        0.005369            NaN  0.340888                 0.001517                     NaN           0.173529            2       True         10\n3     ExtraTreesEntr_BAG_L1_FULL       0.692810   0.675123    accuracy        0.128038       0.188885  0.825164                 0.128038                0.188885           0.825164            1       True          7\n4         LightGBMXT_BAG_L2_FULL       0.692810   0.701718    accuracy        0.585924            NaN  5.335749                 0.003118                     NaN           0.228724            2       True         11\n5           CatBoost_BAG_L2_FULL       0.692810   0.707447    accuracy        0.588336            NaN  5.688442                 0.005529                     NaN           0.581416            2       True         15\n6       WeightedEnsemble_L3_FULL       0.692810   0.707856    accuracy        0.591025            NaN  6.027241                 0.002690                     NaN           0.338799            3       True         20\n7     ExtraTreesGini_BAG_L1_FULL       0.689542   0.673077    accuracy        0.135107       0.184615  0.849001                 0.135107                0.184615           0.849001            1       True          6\n8   RandomForestEntr_BAG_L1_FULL       0.683007   0.654255    accuracy        0.151755       0.179378  0.880783                 0.151755                0.179378           0.880783            1       True          4\n9   RandomForestGini_BAG_L2_FULL       0.679739   0.684943    accuracy        0.707727            NaN  5.968028                 0.124921                0.182446           0.861002            2       True         13\n10    ExtraTreesGini_BAG_L2_FULL       0.679739   0.684534    accuracy        0.714077            NaN  5.943708                 0.131271                0.187909           0.836683            2       True         16\n11          LightGBM_BAG_L2_FULL       0.676471   0.706219    accuracy        0.585907            NaN  5.337967                 0.003100                     NaN           0.230941            2       True         12\n12    ExtraTreesEntr_BAG_L2_FULL       0.676471   0.679624    accuracy        0.713373            NaN  5.920607                 0.130566                0.184792           0.813582            2       True         17\n13           XGBoost_BAG_L1_FULL       0.673203   0.687398    accuracy        0.010908            NaN  0.096438                 0.010908                     NaN           0.096438            1       True          8\n14           XGBoost_BAG_L2_FULL       0.673203   0.702946    accuracy        0.595513            NaN  5.224690                 0.012707                     NaN           0.117664            2       True         18\n15  RandomForestEntr_BAG_L2_FULL       0.673203   0.684534    accuracy        0.720246            NaN  6.002285                 0.137439                0.180509           0.895260            2       True         14\n16        LightGBMXT_BAG_L1_FULL       0.666667   0.694354    accuracy        0.002619            NaN  0.272291                 0.002619                     NaN           0.272291            1       True          1\n17     LightGBMLarge_BAG_L1_FULL       0.663399   0.673895    accuracy        0.007822            NaN  0.927927                 0.007822                     NaN           0.927927            1       True          9\n18  RandomForestGini_BAG_L1_FULL       0.660131   0.666121    accuracy        0.141106       0.181869  0.879733                 0.141106                0.181869           0.879733            1       True          3\n19     LightGBMLarge_BAG_L2_FULL       0.656863   0.704992    accuracy        0.593734            NaN  5.992391                 0.010928                     NaN           0.885365            2       True         19\n    0    = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n    100s     = DyStack   runtime |  3500s    = Remaining runtime\nStarting main fit with num_stack_levels=0.\n    For future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\nBeginning AutoGluon training ... Time limit = 3500s\nAutoGluon will save models to \"AutogluonModels/ag-20241214_135456\"\nTrain Data Rows:    2750\nTrain Data Columns: 17\nLabel Column:       has_purchase\nProblem Type:       binary\nPreprocessing data ...\nSelected class &lt;--&gt; label mapping:  class 1 = 1, class 0 = 0\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    478823.35 MB\n    Train Data (Original)  Memory Usage: 0.36 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    0.1s = Fit runtime\n    17 features in original data used to generate 17 features in processed data.\n    Train Data (Processed) Memory Usage: 0.36 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.07s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n}\nExcluded models: ['NN_TORCH', 'FASTAI'] (Specified by `excluded_model_types`)\nFitting 9 L1 models ...\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 3500.15s of the 3500.14s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.6964   = Validation score   (accuracy)\n    4.26s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ... Training model for up to 3495.79s of the 3495.79s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.6884   = Validation score   (accuracy)\n    4.1s     = Training   runtime\n    0.01s    = Validation runtime\nFitting model: RandomForestGini_BAG_L1 ... Training model for up to 3491.61s of the 3491.6s of remaining time.\n    0.6615   = Validation score   (accuracy)\n    0.89s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: RandomForestEntr_BAG_L1 ... Training model for up to 3490.43s of the 3490.43s of remaining time.\n    0.6644   = Validation score   (accuracy)\n    0.84s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ... Training model for up to 3489.34s of the 3489.34s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.6935   = Validation score   (accuracy)\n    7.5s     = Training   runtime\n    0.02s    = Validation runtime\nFitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 3481.73s of the 3481.73s of remaining time.\n    0.6738   = Validation score   (accuracy)\n    0.83s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 3480.63s of the 3480.62s of remaining time.\n    0.6716   = Validation score   (accuracy)\n    0.85s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ... Training model for up to 3479.5s of the 3479.49s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.6789   = Validation score   (accuracy)\n    5.27s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ... Training model for up to 3474.1s of the 3474.1s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    0.6738   = Validation score   (accuracy)\n    17.2s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 3456.77s of remaining time.\n    Ensemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n    0.6964   = Validation score   (accuracy)\n    0.18s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 43.68s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 43309.0 rows/s (344 batch size)\nAutomatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\nRefitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n    Models trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n    This process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n    To learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\nFitting 1 L1 models ...\nFitting model: LightGBMXT_BAG_L1_FULL ...\n    0.19s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBM_BAG_L1_FULL ...\n    0.21s    = Training   runtime\nFitting model: RandomForestGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.89s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: RandomForestEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.84s    = Training   runtime\n    0.19s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: CatBoost_BAG_L1_FULL ...\n    0.1s     = Training   runtime\nFitting model: ExtraTreesGini_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.83s    = Training   runtime\n    0.19s    = Validation runtime\nFitting model: ExtraTreesEntr_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.85s    = Training   runtime\n    0.19s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: XGBoost_BAG_L1_FULL ...\n    0.12s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBMLarge_BAG_L1_FULL ...\n    0.93s    = Training   runtime\nFitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n    Ensemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n    0.18s    = Training   runtime\nUpdated best model to \"LightGBMXT_BAG_L1_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"LightGBMXT_BAG_L1_FULL\" for predict() and predict_proba().\nRefit complete, total runtime = 2.02s ... Best model: \"LightGBMXT_BAG_L1_FULL\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241214_135456\")\n\n\n\ntest_df['pred_binary'] = predictor_cls.predict(test_df[selected_features])\n\n\ncaluclate_classification_metrics(test_df['has_purchase'], test_df['pred_binary'])\n\n{'accuracy': 0.6918604651162791,\n 'precision': 0.6941069004479309,\n 'recall': 0.6918604651162791,\n 'f1_score': 0.6921418829824787,\n 'confusion_matrix': array([[229,  94],\n        [118, 247]])}\n\n\n\n\nRegression on Non-Zero Outcome\n\ntrain_df_nonzero = train_df[train_df.has_purchase==1].reset_index(drop=True)\ntest_df_nonzero = test_df[test_df.has_purchase==1].reset_index(drop=True)\n\ntrain_df_nonzero.shape, test_df_nonzero.shape\n\n((1414, 21), (365, 31))\n\n\n\n#log\ntrain_df_nonzero['TargetSales_log'] = train_df_nonzero['TargetSales'].map(np.log)\ntest_df_nonzero['TargetSales_log'] = test_df_nonzero['TargetSales'].map(np.log)\n\n\ntrain_df_nonzero['TargetSales_log'].hist()\n\n\n\n\n\n\n\n\n\npredictor_reg = TabularPredictor(label='TargetSales_log').fit(train_df_nonzero[selected_features+['TargetSales_log']],\n                                                      presets=preset,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\n\nNo path specified. Models will be saved in: \"AutogluonModels/ag-20241214_161346\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.1.1\nPython Version:     3.9.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP Wed Oct 23 01:22:11 UTC 2024\nCPU Count:          64\nMemory Avail:       466.78 GB / 480.23 GB (97.2%)\nDisk Space Avail:   1450.75 GB / 1968.52 GB (73.7%)\n===================================================\nPresets specified: ['good_quality']\nSetting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\nStack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\nNote: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n    You can avoid this risk by setting `save_bag_folds=True`.\nDyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n    This is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n    Running DyStack for up to 900s of the 3600s of remaining time (25%).\n/home/charipol/miniconda3/lib/python3.9/site-packages/autogluon/tabular/predictor/predictor.py:1242: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: ValueError('ray==2.40.0 detected. 2.10.0 &lt;= ray &lt; 2.11.0 is required. You can use pip to install certain version of ray `pip install ray==2.10.0` ')\n  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n        Context path: \"AutogluonModels/ag-20241214_161346/ds_sub_fit/sub_fit_ho\"\nRunning DyStack sub-fit ...\nBeginning AutoGluon training ... Time limit = 900s\nAutoGluon will save models to \"AutogluonModels/ag-20241214_161346/ds_sub_fit/sub_fit_ho\"\nTrain Data Rows:    1256\nTrain Data Columns: 17\nLabel Column:       TargetSales_log\nProblem Type:       regression\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    477983.00 MB\n    Train Data (Original)  Memory Usage: 0.16 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    0.0s = Fit runtime\n    17 features in original data used to generate 17 features in processed data.\n    Train Data (Processed) Memory Usage: 0.16 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.05s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n}\nAutoGluon will fit 2 stack levels (L1 to L2) ...\nExcluded models: ['NN_TORCH', 'FASTAI'] (Specified by `excluded_model_types`)\nFitting 7 L1 models ...\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 599.81s of the 899.94s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -0.7829  = Validation score   (-root_mean_squared_error)\n    5.83s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ... Training model for up to 593.9s of the 894.03s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -0.7864  = Validation score   (-root_mean_squared_error)\n    5.68s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: RandomForestMSE_BAG_L1 ... Training model for up to 588.14s of the 888.27s of remaining time.\n    -0.8046  = Validation score   (-root_mean_squared_error)\n    0.7s     = Training   runtime\n    0.15s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ... Training model for up to 587.17s of the 887.3s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -0.7747  = Validation score   (-root_mean_squared_error)\n    7.49s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 579.61s of the 879.74s of remaining time.\n    -0.7815  = Validation score   (-root_mean_squared_error)\n    0.62s    = Training   runtime\n    0.15s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ... Training model for up to 578.75s of the 878.88s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -0.7865  = Validation score   (-root_mean_squared_error)\n    5.41s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ... Training model for up to 573.25s of the 873.38s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\n\n[1000]  valid_set's rmse: 0.732165\n[2000]  valid_set's rmse: 0.730646\n[3000]  valid_set's rmse: 0.730279\n[4000]  valid_set's rmse: 0.730248\n[5000]  valid_set's rmse: 0.730238\n[6000]  valid_set's rmse: 0.730238\n[7000]  valid_set's rmse: 0.730238\n\n\n    -0.8114  = Validation score   (-root_mean_squared_error)\n    42.21s   = Training   runtime\n    0.03s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 831.05s of remaining time.\n    Ensemble Weights: {'CatBoost_BAG_L1': 0.455, 'ExtraTreesMSE_BAG_L1': 0.227, 'XGBoost_BAG_L1': 0.182, 'LightGBM_BAG_L1': 0.136}\n    -0.7698  = Validation score   (-root_mean_squared_error)\n    0.02s    = Training   runtime\n    0.0s     = Validation runtime\nExcluded models: ['NN_TORCH', 'FASTAI'] (Specified by `excluded_model_types`)\nFitting 7 L2 models ...\nFitting model: LightGBMXT_BAG_L2 ... Training model for up to 830.98s of the 830.98s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -0.7955  = Validation score   (-root_mean_squared_error)\n    5.59s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBM_BAG_L2 ... Training model for up to 825.34s of the 825.33s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\n\n[1000]  valid_set's rmse: 0.776795\n\n\n    -0.7972  = Validation score   (-root_mean_squared_error)\n    7.51s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: RandomForestMSE_BAG_L2 ... Training model for up to 817.73s of the 817.72s of remaining time.\n    -0.8017  = Validation score   (-root_mean_squared_error)\n    0.67s    = Training   runtime\n    0.15s    = Validation runtime\nFitting model: CatBoost_BAG_L2 ... Training model for up to 816.81s of the 816.8s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -0.7824  = Validation score   (-root_mean_squared_error)\n    5.55s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 811.18s of the 811.18s of remaining time.\n    -0.7873  = Validation score   (-root_mean_squared_error)\n    0.61s    = Training   runtime\n    0.15s    = Validation runtime\nFitting model: XGBoost_BAG_L2 ... Training model for up to 810.31s of the 810.3s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -0.8012  = Validation score   (-root_mean_squared_error)\n    6.75s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L2 ... Training model for up to 803.46s of the 803.45s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -0.8115  = Validation score   (-root_mean_squared_error)\n    15.94s   = Training   runtime\n    0.01s    = Validation runtime\nFitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 787.41s of remaining time.\n    Ensemble Weights: {'CatBoost_BAG_L1': 0.412, 'ExtraTreesMSE_BAG_L1': 0.176, 'LightGBM_BAG_L1': 0.118, 'XGBoost_BAG_L1': 0.118, 'XGBoost_BAG_L2': 0.118, 'LightGBM_BAG_L2': 0.059}\n    -0.7689  = Validation score   (-root_mean_squared_error)\n    0.03s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 112.67s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 945.3 rows/s (157 batch size)\nAutomatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\nRefitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n    Models trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n    This process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n    To learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\nFitting 1 L1 models ...\nFitting model: LightGBMXT_BAG_L1_FULL ...\n    0.43s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBM_BAG_L1_FULL ...\n    0.4s     = Training   runtime\nFitting model: RandomForestMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.7s     = Training   runtime\n    0.15s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: CatBoost_BAG_L1_FULL ...\n    0.38s    = Training   runtime\nFitting model: ExtraTreesMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.62s    = Training   runtime\n    0.15s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: XGBoost_BAG_L1_FULL ...\n    0.13s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBMLarge_BAG_L1_FULL ...\n    3.38s    = Training   runtime\nFitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n    Ensemble Weights: {'CatBoost_BAG_L1': 0.455, 'ExtraTreesMSE_BAG_L1': 0.227, 'XGBoost_BAG_L1': 0.182, 'LightGBM_BAG_L1': 0.136}\n    0.02s    = Training   runtime\nFitting 1 L2 models ...\nFitting model: LightGBMXT_BAG_L2_FULL ...\n    0.4s     = Training   runtime\nFitting 1 L2 models ...\nFitting model: LightGBM_BAG_L2_FULL ...\n    0.61s    = Training   runtime\nFitting model: RandomForestMSE_BAG_L2_FULL | Skipping fit via cloning parent ...\n    0.67s    = Training   runtime\n    0.15s    = Validation runtime\nFitting 1 L2 models ...\nFitting model: CatBoost_BAG_L2_FULL ...\n    0.2s     = Training   runtime\nFitting model: ExtraTreesMSE_BAG_L2_FULL | Skipping fit via cloning parent ...\n    0.61s    = Training   runtime\n    0.15s    = Validation runtime\nFitting 1 L2 models ...\nFitting model: XGBoost_BAG_L2_FULL ...\n    0.3s     = Training   runtime\nFitting 1 L2 models ...\nFitting model: LightGBMLarge_BAG_L2_FULL ...\n    0.84s    = Training   runtime\nFitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n    Ensemble Weights: {'CatBoost_BAG_L1': 0.412, 'ExtraTreesMSE_BAG_L1': 0.176, 'LightGBM_BAG_L1': 0.118, 'XGBoost_BAG_L1': 0.118, 'XGBoost_BAG_L2': 0.118, 'LightGBM_BAG_L2': 0.059}\n    0.03s    = Training   runtime\nUpdated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\nRefit complete, total runtime = 7.83s ... Best model: \"WeightedEnsemble_L3_FULL\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241214_161346/ds_sub_fit/sub_fit_ho\")\nDeleting DyStack predictor artifacts (clean_up_fits=True) ...\nLeaderboard on holdout data (DyStack):\n                          model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0     ExtraTreesMSE_BAG_L1_FULL      -0.839346  -0.781522  root_mean_squared_error        0.130818       0.146218  0.616536                 0.130818                0.146218           0.616536            1       True          5\n1     ExtraTreesMSE_BAG_L2_FULL      -0.853589  -0.787303  root_mean_squared_error        0.446297            NaN  6.640192                 0.132555                0.153409           0.613806            2       True         13\n2      WeightedEnsemble_L3_FULL      -0.857556  -0.768871  root_mean_squared_error        0.340568            NaN  6.967287                 0.005255                     NaN           0.026960            3       True         16\n3   RandomForestMSE_BAG_L1_FULL      -0.861175  -0.804629  root_mean_squared_error        0.122124       0.147519  0.702981                 0.122124                0.147519           0.702981            1       True          3\n4      WeightedEnsemble_L2_FULL      -0.862117  -0.769820  root_mean_squared_error        0.157042            NaN  1.533079                 0.004246                     NaN           0.017303            2       True          8\n5        LightGBMXT_BAG_L1_FULL      -0.864283  -0.782882  root_mean_squared_error        0.002461            NaN  0.427038                 0.002461                     NaN           0.427038            1       True          1\n6          CatBoost_BAG_L2_FULL      -0.866735  -0.782409  root_mean_squared_error        0.322393            NaN  6.225857                 0.008651                     NaN           0.199471            2       True         12\n7   RandomForestMSE_BAG_L2_FULL      -0.867588  -0.801697  root_mean_squared_error        0.442486            NaN  6.695999                 0.128743                0.152188           0.669613            2       True         11\n8        LightGBMXT_BAG_L2_FULL      -0.867632  -0.795484  root_mean_squared_error        0.317761            NaN  6.431228                 0.004019                     NaN           0.404842            2       True          9\n9           XGBoost_BAG_L2_FULL      -0.867990  -0.801243  root_mean_squared_error        0.327944            NaN  6.329812                 0.014201                     NaN           0.303426            2       True         14\n10         LightGBM_BAG_L2_FULL      -0.869884  -0.797155  root_mean_squared_error        0.321112            NaN  6.636901                 0.007370                     NaN           0.610516            2       True         10\n11         LightGBM_BAG_L1_FULL      -0.872511  -0.786431  root_mean_squared_error        0.001978            NaN  0.396514                 0.001978                     NaN           0.396514            1       True          2\n12         CatBoost_BAG_L1_FULL      -0.876613  -0.774745  root_mean_squared_error        0.007943            NaN  0.375822                 0.007943                     NaN           0.375822            1       True          4\n13          XGBoost_BAG_L1_FULL      -0.908105  -0.786510  root_mean_squared_error        0.012056            NaN  0.126903                 0.012056                     NaN           0.126903            1       True          6\n14    LightGBMLarge_BAG_L2_FULL      -0.929002  -0.811482  root_mean_squared_error        0.322513            NaN  6.862701                 0.008771                     NaN           0.836315            2       True         15\n15    LightGBMLarge_BAG_L1_FULL      -0.943116  -0.811385  root_mean_squared_error        0.036361            NaN  3.380592                 0.036361                     NaN           3.380592            1       True          7\n    1    = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n    121s     = DyStack   runtime |  3479s    = Remaining runtime\nStarting main fit with num_stack_levels=1.\n    For future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\nBeginning AutoGluon training ... Time limit = 3479s\nAutoGluon will save models to \"AutogluonModels/ag-20241214_161346\"\nTrain Data Rows:    1414\nTrain Data Columns: 17\nLabel Column:       TargetSales_log\nProblem Type:       regression\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    478011.59 MB\n    Train Data (Original)  Memory Usage: 0.18 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', []) : 12 | ['total_sales', 'avg_purchase_frequency', 'avg_purchase_value', 'per_fashion_accessories', 'per_home_decor', ...]\n        ('int', [])   :  5 | ['recency', 'purchase_day', 'nb_product', 'nb_category', 'customer_lifetime']\n    0.0s = Fit runtime\n    17 features in original data used to generate 17 features in processed data.\n    Train Data (Processed) Memory Usage: 0.18 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.06s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n    To change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n}\nAutoGluon will fit 2 stack levels (L1 to L2) ...\nExcluded models: ['NN_TORCH', 'FASTAI'] (Specified by `excluded_model_types`)\nFitting 7 L1 models ...\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 2318.53s of the 3478.66s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -0.7909  = Validation score   (-root_mean_squared_error)\n    5.92s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBM_BAG_L1 ... Training model for up to 2312.52s of the 3472.65s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -0.7919  = Validation score   (-root_mean_squared_error)\n    5.67s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: RandomForestMSE_BAG_L1 ... Training model for up to 2306.77s of the 3466.9s of remaining time.\n    -0.8074  = Validation score   (-root_mean_squared_error)\n    0.67s    = Training   runtime\n    0.15s    = Validation runtime\nFitting model: CatBoost_BAG_L1 ... Training model for up to 2305.84s of the 3465.97s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -0.783   = Validation score   (-root_mean_squared_error)\n    6.6s     = Training   runtime\n    0.02s    = Validation runtime\nFitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 2299.16s of the 3459.29s of remaining time.\n    -0.7902  = Validation score   (-root_mean_squared_error)\n    0.6s     = Training   runtime\n    0.15s    = Validation runtime\nFitting model: XGBoost_BAG_L1 ... Training model for up to 2298.3s of the 3458.44s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -0.8026  = Validation score   (-root_mean_squared_error)\n    5.14s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2293.07s of the 3453.2s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -0.8166  = Validation score   (-root_mean_squared_error)\n    14.87s   = Training   runtime\n    0.01s    = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 3438.24s of remaining time.\n    Ensemble Weights: {'CatBoost_BAG_L1': 0.5, 'ExtraTreesMSE_BAG_L1': 0.25, 'LightGBM_BAG_L1': 0.188, 'LightGBMXT_BAG_L1': 0.062}\n    -0.78    = Validation score   (-root_mean_squared_error)\n    0.02s    = Training   runtime\n    0.0s     = Validation runtime\nExcluded models: ['NN_TORCH', 'FASTAI'] (Specified by `excluded_model_types`)\nFitting 7 L2 models ...\nFitting model: LightGBMXT_BAG_L2 ... Training model for up to 3438.17s of the 3438.16s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\n\n[1000]  valid_set's rmse: 0.741321\n[2000]  valid_set's rmse: 0.735104\n[1000]  valid_set's rmse: 0.756407\n[1000]  valid_set's rmse: 0.712855\n\n\n    -0.7869  = Validation score   (-root_mean_squared_error)\n    9.95s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBM_BAG_L2 ... Training model for up to 3428.15s of the 3428.14s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -0.7932  = Validation score   (-root_mean_squared_error)\n    6.95s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: RandomForestMSE_BAG_L2 ... Training model for up to 3421.11s of the 3421.1s of remaining time.\n    -0.8097  = Validation score   (-root_mean_squared_error)\n    0.7s     = Training   runtime\n    0.15s    = Validation runtime\nFitting model: CatBoost_BAG_L2 ... Training model for up to 3420.15s of the 3420.15s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -0.7847  = Validation score   (-root_mean_squared_error)\n    9.03s    = Training   runtime\n    0.02s    = Validation runtime\nFitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 3411.04s of the 3411.04s of remaining time.\n    -0.795   = Validation score   (-root_mean_squared_error)\n    0.6s     = Training   runtime\n    0.15s    = Validation runtime\nFitting model: XGBoost_BAG_L2 ... Training model for up to 3410.19s of the 3410.18s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n    -0.8102  = Validation score   (-root_mean_squared_error)\n    6.21s    = Training   runtime\n    0.04s    = Validation runtime\nFitting model: LightGBMLarge_BAG_L2 ... Training model for up to 3403.88s of the 3403.87s of remaining time.\n    Fitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy\n\n\n[1000]  valid_set's rmse: 0.75982\n[2000]  valid_set's rmse: 0.759573\n[1000]  valid_set's rmse: 0.838684\n[2000]  valid_set's rmse: 0.838386\n\n\n    -0.8107  = Validation score   (-root_mean_squared_error)\n    31.41s   = Training   runtime\n    0.02s    = Validation runtime\nFitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 3372.32s of remaining time.\n    Ensemble Weights: {'CatBoost_BAG_L1': 0.333, 'LightGBMXT_BAG_L2': 0.25, 'LightGBM_BAG_L2': 0.167, 'LightGBM_BAG_L1': 0.083, 'ExtraTreesMSE_BAG_L1': 0.083, 'LightGBMLarge_BAG_L2': 0.083}\n    -0.7746  = Validation score   (-root_mean_squared_error)\n    0.04s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 106.52s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1175.2 rows/s (177 batch size)\nAutomatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\nRefitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n    Models trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n    This process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n    To learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\nFitting 1 L1 models ...\nFitting model: LightGBMXT_BAG_L1_FULL ...\n    0.43s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBM_BAG_L1_FULL ...\n    0.4s     = Training   runtime\nFitting model: RandomForestMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.67s    = Training   runtime\n    0.15s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: CatBoost_BAG_L1_FULL ...\n    0.29s    = Training   runtime\nFitting model: ExtraTreesMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n    0.6s     = Training   runtime\n    0.15s    = Validation runtime\nFitting 1 L1 models ...\nFitting model: XGBoost_BAG_L1_FULL ...\n    0.09s    = Training   runtime\nFitting 1 L1 models ...\nFitting model: LightGBMLarge_BAG_L1_FULL ...\n    0.73s    = Training   runtime\nFitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n    Ensemble Weights: {'CatBoost_BAG_L1': 0.5, 'ExtraTreesMSE_BAG_L1': 0.25, 'LightGBM_BAG_L1': 0.188, 'LightGBMXT_BAG_L1': 0.062}\n    0.02s    = Training   runtime\nFitting 1 L2 models ...\nFitting model: LightGBMXT_BAG_L2_FULL ...\n    0.83s    = Training   runtime\nFitting 1 L2 models ...\nFitting model: LightGBM_BAG_L2_FULL ...\n    0.54s    = Training   runtime\nFitting model: RandomForestMSE_BAG_L2_FULL | Skipping fit via cloning parent ...\n    0.7s     = Training   runtime\n    0.15s    = Validation runtime\nFitting 1 L2 models ...\nFitting model: CatBoost_BAG_L2_FULL ...\n    0.39s    = Training   runtime\nFitting model: ExtraTreesMSE_BAG_L2_FULL | Skipping fit via cloning parent ...\n    0.6s     = Training   runtime\n    0.15s    = Validation runtime\nFitting 1 L2 models ...\nFitting model: XGBoost_BAG_L2_FULL ...\n    0.18s    = Training   runtime\nFitting 1 L2 models ...\nFitting model: LightGBMLarge_BAG_L2_FULL ...\n    2.46s    = Training   runtime\nFitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n    Ensemble Weights: {'CatBoost_BAG_L1': 0.333, 'LightGBMXT_BAG_L2': 0.25, 'LightGBM_BAG_L2': 0.167, 'LightGBM_BAG_L1': 0.083, 'ExtraTreesMSE_BAG_L1': 0.083, 'LightGBMLarge_BAG_L2': 0.083}\n    0.04s    = Training   runtime\nUpdated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\nRefit complete, total runtime = 7.11s ... Best model: \"WeightedEnsemble_L3_FULL\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20241214_161346\")\n\n\n\ntest_df_nonzero['pred_log'] = predictor_reg.predict(test_df_nonzero[selected_features])\ntest_df_nonzero['pred_log_exp'] = test_df_nonzero['pred_log'].map(np.exp)\n\ntest_df['pred_log'] = predictor_reg.predict(test_df[selected_features])\ntest_df['pred_log_exp'] = test_df['pred_log'].map(np.exp)\n\n\ncalculate_regression_metrics(test_df_nonzero['TargetSales'], test_df_nonzero['pred_log_exp'])\n\n{'root_mean_squared_error': 4330.443144695726,\n 'mean_squared_error': 18752737.82944221,\n 'mean_absolute_error': 880.0418223064565,\n 'r2': 0.3647576298877435,\n 'pearsonr': 0.6756393928483335,\n 'spearmanr': 0.5762190201444638,\n 'median_absolute_error': 243.0658528752748,\n 'earths_mover_distance': 546.7166312173882}\n\n\n\ntest_df['pred_hurdle'] = test_df.pred_binary * test_df.pred_log_exp\n\n\nmetric_hurdle = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_hurdle'])\nmetric_hurdle['model'] = 'hurdle'\nmetric_hurdle\n\n{'root_mean_squared_error': 3171.760744960863,\n 'mean_squared_error': 10060066.22327469,\n 'mean_absolute_error': 584.9162934881963,\n 'r2': 0.3779813431428882,\n 'pearsonr': 0.6769697889999318,\n 'spearmanr': 0.5107083593715698,\n 'median_absolute_error': 199.1780137692856,\n 'earths_mover_distance': 286.381442541919,\n 'model': 'hurdle'}\n\n\n\n\nDuan‚Äôs Method\nWhen predicting a log-transformed outcome, we typically want to re-transform the predictions to non-log numbers by applying the exponential function. However, this ignores a small bias due to the error term in the process.\n\\[ln(y) = f(X) + \\epsilon\\]\nwhere * \\(y\\) is actual outcome. * \\(X\\) is the features. * \\(f(.)\\) is a trained model. * \\(\\epsilon\\) is the error term.\nwhen re-transforming \\[\n\\begin{align}\ny &= exp(ln(y)) \\\\\n&= exp(f(X) + \\epsilon ) \\\\\n&= exp(f(X)) \\cdot exp(\\epsilon) \\\\\nE[y] &= E[exp(f(X))] \\cdot E[exp(\\epsilon)]\n\\end{align}\n\\]\nDuan estimates the E[\\(exp(\\epsilon)\\)] as \\[\n\\begin{align}\n\\hat \\lambda &= E[exp(ln(y) - ln(\\hat y))]\n\\end{align}\n\\]\nwhere * \\(\\hat \\lambda\\) is the Duan‚Äôs smearing estimator of the bias from re-transformation \\(E[exp(\\epsilon)]\\) * \\(\\hat y\\) is the prediction aka \\(f(X)\\)\n\ntrain_df_nonzero['pred_log'] = predictor_reg.predict(train_df_nonzero[selected_features])\ntrain_df_nonzero['pred_log_exp'] = train_df_nonzero['pred_log'].map(np.exp)\n\nsmearing_estimator = np.mean(np.exp(train_df_nonzero['TargetSales_log'] - train_df_nonzero['pred_log']))\nsmearing_estimator\n\n1.2280991653046711\n\n\n\nnp.exp(train_df_nonzero['TargetSales_log'] - train_df_nonzero['pred_log']).hist()\n\n\n\n\n\n\n\n\n\ntest_df['pred_log_exp_corrected'] = test_df['pred_log_exp'] * smearing_estimator\ntest_df['pred_hurdle_corrected'] = test_df.pred_binary * test_df.pred_log_exp_corrected\n\n\nmetric_hurdle_corrected = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_hurdle_corrected'])\nmetric_hurdle_corrected['model'] = 'hurdle_corrected'\nmetric_hurdle_corrected\n\n{'root_mean_squared_error': 3055.3207868281233,\n 'mean_squared_error': 9334985.110424023,\n 'mean_absolute_error': 613.3946643257099,\n 'r2': 0.42281345159207295,\n 'pearsonr': 0.6769697889999318,\n 'spearmanr': 0.5107083593715698,\n 'median_absolute_error': 232.55557358084502,\n 'earths_mover_distance': 241.61839859133218,\n 'model': 'hurdle_corrected'}\n\n\n\n\nAssumption on Independent and Identically Distributed Residuals\nBut not so fast, the formulation of Duan‚Äôs smearing estimator assumes that estimates of error terms (residuals) for log predictions be independent and identically distributed. Since we are dealing with individual customers, independence can be assumed. However, if we look at the plot of residuals vs predicted log values, we can see that they are not identically distributed.\n\n#plot residual and predicted log value\ntrain_df_nonzero['pred_log'] = predictor_reg.predict(train_df_nonzero[selected_features])\ntrain_df_nonzero['residual_log'] = (train_df_nonzero['pred_log'] - train_df_nonzero['TargetSales_log'])\n\n# Create the scatter plot\nsns.scatterplot(x='pred_log', y='residual_log', data=train_df_nonzero)\n\n# Add the Lowess smoothing line\nsns.regplot(x='pred_log', y='residual_log', data=train_df_nonzero, scatter_kws={'alpha': 0.5}, line_kws={'color': 'red'})\n\n\n\n\n\n\n\n\nAlthough note that White test does not reject the null hypothesis of the residuals being homoscedastic in reference to the features. This counterintuitive result might stem from the fact that White test is assuming linear or quadratic relationships between outcome and features while the residuals are derived from a stacked ensemble of decision trees.\n\nwhite_stat, white_p_value, _, _ = het_white(train_df_nonzero['residual_log'], \n                                            train_df_nonzero[selected_features])\nprint(f\"White Test Statistic: {white_stat}\")\nprint(f\"P-value: {white_p_value}\")\n\nWhite Test Statistic: 129.31318320644837\nP-value: 0.8761278601130765\n\n\nOur choice is to either trust the White test and pretend assume everything is fine; or trust our eyes and replace the non-zero regression model with one that produces iid residuals such as generalized least squares (GLS) with heteroscedasticity-robust standard errors. In order to satisfy the assumptions of GLS, we perform winsorization, standardization and verify multicollinearity among the features.\n\ntrain_df_nonzero_processed = train_df_nonzero.copy()\n\n#winsorize at 99%\nwinsorizer = Winsorizer(cols=selected_features, percentile=99)\nwinsorizer.fit(train_df_nonzero_processed)\ntrain_df_nonzero_processed = winsorizer.transform(train_df_nonzero_processed)\n\n#standard scaling\nscaler = StandardScaler()\nscaler.fit(train_df_nonzero_processed[selected_features])\ntrain_df_nonzero_processed[selected_features] = scaler.transform(train_df_nonzero_processed[selected_features])\n\n#check vif\nvif_data = calculate_vif(train_df_nonzero_processed, selected_features)\n\n# Print the VIF for each feature\nprint(vif_data)\n\n# Filter out features with high VIF (e.g., VIF &gt; 10 suggests multicollinearity)\nhigh_vif_features = vif_data[vif_data['VIF'] &gt; 10]\nprint(\"High VIF features:\", high_vif_features)\n\n                           feature        VIF\n0                            const   1.000000\n1                          recency   3.976800\n2                     purchase_day   6.193391\n3                      total_sales   5.837235\n4                       nb_product   2.386644\n5                      nb_category   2.691338\n6                customer_lifetime   2.271404\n7           avg_purchase_frequency   5.291575\n8               avg_purchase_value   2.449152\n9          per_fashion_accessories   5.826894\n10                  per_home_decor  15.806683\n11          per_kitchen_and_dining  12.053308\n12                      per_others   1.779531\n13          per_outdoor_and_garden   2.729103\n14  per_personal_care_and_wellness   2.908001\n15        per_seasonal_and_holiday   4.234883\n16        per_stationary_and_gifts   4.252687\n17              per_toys_and_games   3.185269\nHigh VIF features:                    feature        VIF\n10          per_home_decor  15.806683\n11  per_kitchen_and_dining  12.053308\n\n\n\n# Calculate VIF after dropping highly correlated features\nselected_features_no_corr = [i for i in selected_features if i!='per_kitchen_and_dining']\nvif_data = calculate_vif(train_df_nonzero_processed.drop('per_kitchen_and_dining',axis=1), \n                         selected_features_no_corr)\n\n# Print the VIF for each feature\nprint(vif_data)\n\n# Filter out features with high VIF (e.g., VIF &gt; 10 suggests multicollinearity)\nhigh_vif_features = vif_data[vif_data['VIF'] &gt; 10]\nprint(\"High VIF features:\", high_vif_features)\n\n                           feature       VIF\n0                            const  1.000000\n1                          recency  3.975755\n2                     purchase_day  6.193274\n3                      total_sales  5.835455\n4                       nb_product  2.355212\n5                      nb_category  2.238039\n6                customer_lifetime  2.265888\n7           avg_purchase_frequency  5.288665\n8               avg_purchase_value  2.447088\n9          per_fashion_accessories  1.271944\n10                  per_home_decor  1.642046\n11                      per_others  1.172240\n12          per_outdoor_and_garden  1.122673\n13  per_personal_care_and_wellness  1.141183\n14        per_seasonal_and_holiday  1.180130\n15        per_stationary_and_gifts  1.310125\n16              per_toys_and_games  1.198112\nHigh VIF features: Empty DataFrame\nColumns: [feature, VIF]\nIndex: []\n\n\n\ny = train_df_nonzero_processed['TargetSales_log']\nX = train_df_nonzero_processed[selected_features_no_corr]\nX = sm.add_constant(X)\n\ngls_model = sm.GLS(y, X)\ngls_results = gls_model.fit(cov_type='HC3')\n\n# 4. Print the summary of the regression\nprint(gls_results.summary())\n\n                            GLS Regression Results                            \n==============================================================================\nDep. Variable:        TargetSales_log   R-squared:                       0.460\nModel:                            GLS   Adj. R-squared:                  0.454\nMethod:                 Least Squares   F-statistic:                     75.24\nDate:                Sat, 14 Dec 2024   Prob (F-statistic):          2.27e-175\nTime:                        16:56:46   Log-Likelihood:                -1661.3\nNo. Observations:                1414   AIC:                             3357.\nDf Residuals:                    1397   BIC:                             3446.\nDf Model:                          16                                         \nCovariance Type:                  HC3                                         \n==================================================================================================\n                                     coef    std err          z      P&gt;|z|      [0.025      0.975]\n--------------------------------------------------------------------------------------------------\nconst                              6.3127      0.021    299.031      0.000       6.271       6.354\nrecency                            0.0054      0.047      0.114      0.909      -0.087       0.098\npurchase_day                       0.3697      0.046      7.977      0.000       0.279       0.460\ntotal_sales                        0.1438      0.049      2.905      0.004       0.047       0.241\nnb_product                        -0.0293      0.032     -0.929      0.353      -0.091       0.033\nnb_category                        0.1163      0.034      3.448      0.001       0.050       0.182\ncustomer_lifetime                 -0.0170      0.032     -0.524      0.600      -0.081       0.047\navg_purchase_frequency            -0.0103      0.052     -0.197      0.844      -0.113       0.092\navg_purchase_value                 0.3817      0.034     11.257      0.000       0.315       0.448\nper_fashion_accessories           -0.0216      0.030     -0.724      0.469      -0.080       0.037\nper_home_decor                    -0.0063      0.034     -0.188      0.851      -0.072       0.060\nper_others                        -0.0165      0.023     -0.706      0.480      -0.062       0.029\nper_outdoor_and_garden            -0.0397      0.022     -1.768      0.077      -0.084       0.004\nper_personal_care_and_wellness    -0.0042      0.025     -0.167      0.867      -0.054       0.045\nper_seasonal_and_holiday          -0.0796      0.025     -3.248      0.001      -0.128      -0.032\nper_stationary_and_gifts           0.0166      0.024      0.698      0.485      -0.030       0.063\nper_toys_and_games                -0.0230      0.026     -0.900      0.368      -0.073       0.027\n==============================================================================\nOmnibus:                      162.402   Durbin-Watson:                   2.022\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             1272.986\nSkew:                           0.198   Prob(JB):                    3.75e-277\nKurtosis:                       7.631   Cond. No.                         6.59\n==============================================================================\n\nNotes:\n[1] Standard Errors are heteroscedasticity robust (HC3)\n\n\n\n#plot residual and predicted log value\ntrain_df_nonzero_processed['pred_log_gls'] = gls_results.predict(X).reset_index(drop=True)\ntrain_df_nonzero_processed['residual_log_gls'] = (train_df_nonzero_processed['pred_log_gls'] - train_df_nonzero_processed['TargetSales_log'])\ntrain_df_nonzero_processed.plot.scatter(x='pred_log_gls', y='residual_log_gls')\n\n\n\n\n\n\n\n\n\nwhite_stat, white_p_value, _, _ = het_white(train_df_nonzero_processed['residual_log_gls'], \n                                            X)\nprint(f\"White Test Statistic: {white_stat}\")\nprint(f\"P-value: {white_p_value}\")\n\nWhite Test Statistic: 135.0623196454856\nP-value: 0.8343390085729777\n\n\n\n#preprocess test set\ntest_df_processed = test_df.copy()\n\n#winsorize at 99%\ntest_df_processed = winsorizer.transform(test_df_processed)\n\n#standard scaling\ntest_df_processed[selected_features] = scaler.transform(test_df_processed[selected_features])\n\n#drop highly correlated features\ntest_df_processed = test_df_processed.drop('per_kitchen_and_dining', axis=1)\n\n#infer\nX_test = test_df_processed[selected_features_no_corr]\nX_test = sm.add_constant(X_test)\ntest_df_processed['pred_log_gls'] = gls_results.predict(X_test)\n\n\nsmearing_estimator_gls = np.mean(np.exp(train_df_nonzero_processed['TargetSales_log'] - train_df_nonzero_processed['pred_log_gls']))\nsmearing_estimator_gls\n\n1.8597808961776598\n\n\n\ntest_df['pred_log_exp_gls'] = test_df_processed['pred_log_gls'].map(np.exp)\n\ntest_df['pred_log_exp_gls_corrected'] = test_df['pred_log_exp_gls'] * smearing_estimator_gls\ntest_df['pred_hurdle_gls_corrected'] = test_df.pred_binary * test_df.pred_log_exp_gls_corrected\n\nmetric_hurdle_gls_corrected = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_hurdle_gls_corrected'])\nmetric_hurdle_gls_corrected['model'] = 'hurdle_gls_corrected'\nmetric_hurdle_gls_corrected\n\n{'root_mean_squared_error': 5289.79271849163,\n 'mean_squared_error': 27981907.004607063,\n 'mean_absolute_error': 970.7645208697954,\n 'r2': -0.73013455627538,\n 'pearsonr': 0.7076845600551718,\n 'spearmanr': 0.5187878933164471,\n 'median_absolute_error': 339.03363694351526,\n 'earths_mover_distance': 556.014141615775,\n 'model': 'hurdle_gls_corrected'}"
  },
  {
    "objectID": "notebook/sales_prediction.html#evaluation",
    "href": "notebook/sales_prediction.html#evaluation",
    "title": "Predict Zero-inflated and Long/fat-tailed Outcomes",
    "section": "Evaluation",
    "text": "Evaluation\nWe can see that the hurdle_corrected method performs best across all metrics except for 1) mean absolute error where it performs about 5% worse than hurdle method without the correction and 2) median absolute error where it only performs better than baseline regression and 3) Spearman‚Äôs rank correlation where it underperforms log1p by 4%; correlations are tied between the two Hurdle methods by definition since we multiply Duan‚Äôs smearing estimator to hurdle predictions to get hurdle_corrected.\n\nmetric_df = pd.DataFrame([metric_baseline,\n                       metric_winsorized,\n                       metric_log1p,\n                       metric_hurdle,\n                       metric_hurdle_corrected,])\n\n\nrank_df = metric_df.copy()\nfor col in metric_df.columns.tolist()[:-1]:\n    if col in ['r2', 'pearsonr', 'spearmanr']:\n        rank_df[f'{col}_rank'] = rank_df[col].rank(ascending=False)\n    else:\n        rank_df[f'{col}_rank'] = rank_df[col].rank(ascending=True)\nrank_df = rank_df.drop(metric_df.columns.tolist()[:-1], axis=1)\nrank_df['avg_rank'] = rank_df.iloc[:,1:].mean(axis=1)\nrank_df.transpose()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nmodel\nbaseline\nwinsorized\nlog1p\nhurdle\nhurdle_corrected\n\n\nroot_mean_squared_error_rank\n2.0\n4.0\n5.0\n3.0\n1.0\n\n\nmean_squared_error_rank\n2.0\n4.0\n5.0\n3.0\n1.0\n\n\nmean_absolute_error_rank\n5.0\n4.0\n3.0\n1.0\n2.0\n\n\nr2_rank\n2.0\n4.0\n5.0\n3.0\n1.0\n\n\npearsonr_rank\n3.0\n5.0\n4.0\n1.5\n1.5\n\n\nspearmanr_rank\n5.0\n4.0\n1.0\n2.5\n2.5\n\n\nmedian_absolute_error_rank\n5.0\n3.0\n1.0\n2.0\n4.0\n\n\nearths_mover_distance_rank\n3.0\n4.0\n5.0\n2.0\n1.0\n\n\navg_rank\n3.375\n4.0\n3.625\n2.25\n1.75\n\n\n\n\n\n\n\n\nmetric_df.transpose()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nroot_mean_squared_error\n3162.478744\n3623.576378\n3725.342296\n3171.760745\n3055.320787\n\n\nmean_squared_error\n10001271.807776\n13130305.763947\n13878175.221577\n10060066.223275\n9334985.110424\n\n\nmean_absolute_error\n715.644266\n627.788007\n618.976847\n584.916293\n613.394664\n\n\nr2\n0.381617\n0.188147\n0.141906\n0.377981\n0.422813\n\n\npearsonr\n0.619072\n0.575799\n0.581717\n0.67697\n0.67697\n\n\nspearmanr\n0.470085\n0.504302\n0.533816\n0.510708\n0.510708\n\n\nmedian_absolute_error\n232.982083\n219.622481\n89.554954\n199.178014\n232.555574\n\n\nearths_mover_distance\n287.777288\n432.128843\n581.049444\n286.381443\n241.618399\n\n\nmodel\nbaseline\nwinsorized\nlog1p\nhurdle\nhurdle_corrected\n\n\n\n\n\n\n\n\nWhy hurdle Outperforms hurdle_corrected in MAE?\nDuan‚Äôs method adjusts for underestimation from retransformation of log outcome. This could lead to smaller extreme errors but more less extreme ones. We verify this hypothesis by comparing mean absolute error before and after transformation for errors originally under and over 99th percentile. We confirm that is the case for this problem.\n\nerr_hurdle = (test_df['TargetSales'] - test_df['pred_hurdle']).abs()\nerr_hurdle_corrected = (test_df['TargetSales'] - test_df['pred_hurdle_corrected']).abs()\n\n\nerr_hurdle.describe(percentiles=[.25, .5, .75, .9, .95, .99]) \n\ncount      688.000000\nmean       584.916293\nstd       3119.628924\nmin          0.000000\n25%          0.000000\n50%        199.178014\n75%        475.603446\n90%        862.530026\n95%       1237.540954\n99%       6763.777844\nmax      55731.205996\ndtype: float64\n\n\n\nerr_hurdle[err_hurdle&lt;6763.777844].mean(),\\\nerr_hurdle[err_hurdle&gt;6763.777844].mean(),\n\n(355.4918014848842, 22904.641872667555)\n\n\n\nerr_hurdle_corrected[err_hurdle&lt;6763.777844].mean(),\\\nerr_hurdle_corrected[err_hurdle&gt;6763.777844].mean(),\n\n(392.7718802742851, 22076.839798471465)\n\n\n\n\nWhy log1p Performs So Much Better than Others in MedAE?\nIt is for similar reasons that hurdle outperforms hurdle_corrected in MedAE; however, log1p performs twice better than other approaches (it also slightly outperforms hurdle models in Spearman‚Äôs rank correlation), especially the Hurdle models which should be modeling the non-zero outcomes in the same manner. This is because Hurdle models depend not only on the regression but the classification model. We can see that if the classification model were perfect (instead of the current f1 = 0.69), other metrics also improved but not nearly as drastic as MedAE and Spearman‚Äôs rank correlation.\n\ntest_df['pred_hurdle_corrected_perfect_cls'] = test_df.has_purchase * test_df.pred_log_exp_corrected\nmetric_hurdle_corrected_perfect_cls = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_hurdle_corrected_perfect_cls'])\nmetric_hurdle_corrected_perfect_cls['model'] = 'hurdle_corrected_perfect_cls'\n\nmetric_df2 = pd.DataFrame([metric_baseline,\n                       metric_winsorized,\n                       metric_log1p,\n                       metric_hurdle,\n                       metric_hurdle_corrected,\n                       metric_hurdle_corrected_perfect_cls,])\nmetric_df2.transpose()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\nroot_mean_squared_error\n3162.478744\n3623.576378\n3725.342296\n3171.760745\n3055.320787\n3030.854831\n\n\nmean_squared_error\n10001271.807776\n13130305.763947\n13878175.221577\n10060066.223275\n9334985.110424\n9186081.006625\n\n\nmean_absolute_error\n715.644266\n627.788007\n618.976847\n584.916293\n613.394664\n479.558294\n\n\nr2\n0.381617\n0.188147\n0.141906\n0.377981\n0.422813\n0.43202\n\n\npearsonr\n0.619072\n0.575799\n0.581717\n0.67697\n0.67697\n0.687639\n\n\nspearmanr\n0.470085\n0.504302\n0.533816\n0.510708\n0.510708\n0.929419\n\n\nmedian_absolute_error\n232.982083\n219.622481\n89.554954\n199.178014\n232.555574\n34.991964\n\n\nearths_mover_distance\n287.777288\n432.128843\n581.049444\n286.381443\n241.618399\n234.587018\n\n\nmodel\nbaseline\nwinsorized\nlog1p\nhurdle\nhurdle_corrected\nhurdle_corrected_perfect_cls\n\n\n\n\n\n\n\n\n\nWhy Baseline Regression Performs Best at Aggregate Level\nIf we look at aggregated mean or sum of actual sales vs predicted sales, baseline regression performs best by far. This is due to the fact that without any constraints a regressor only minimizes the MSE loss and usually ends up predicting values around the mean to balance between under- and over-predictions. However, this level of prediction is often not very useful as a single point and more often done by in a time series setup.\n\ntest_df[['TargetSales','pred_baseline','pred_winsorized','pred_log1p_expm1','pred_hurdle','pred_hurdle_corrected']].mean()\n\nTargetSales              760.558808\npred_baseline            791.043945\npred_winsorized          508.281555\npred_log1p_expm1         186.200281\npred_hurdle              527.286811\npred_hurdle_corrected    647.560493\ndtype: float64\n\n\n\ntest_df[['TargetSales','pred_baseline','pred_winsorized','pred_log1p_expm1','pred_hurdle','pred_hurdle_corrected']].sum()\n\nTargetSales              523264.460000\npred_baseline            544238.250000\npred_winsorized          349697.718750\npred_log1p_expm1         128105.793618\npred_hurdle              362773.326124\npred_hurdle_corrected    445521.619008\ndtype: float64"
  }
]