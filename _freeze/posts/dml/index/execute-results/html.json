{
  "hash": "602be7ffbcbd1ef7f5b0efd5aab75f15",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Understanding Double/debiased machine learning (DML)\"\nauthor: \"cstorm125\"\ndate: \"2025-10-03\"\ncategories: [causal-inference, double-machine-learning, frisch-waugh-lovell, neyman-orthogonality]\nimage: \"featured_image.jpg\"\nsidebar:\n  contents: auto\nformat:\n  html:\n    code-fold: true\njupyter: python3\n---\n\n[Double/debiased machine learning (DML)](https://arxiv.org/pdf/1608.00060) is one of the most prominent causal inference algorithms in modern practice from pricing, promotion targeting to multi-million-dollar, one-way-door infrastructure decisions. At its crux is [Neyman orthogonality](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-38/issue-6/On-Optimal-Asymptotic-Tests-of-Composite-Statistical-Hypotheses/10.1214/aoms/1177698617.full). It posits that given observed data $W$ and true causal parameter $\\theta_0$, a score function $\\psi$ is Neyman-orthogonal if and only if its first derivative with respect to parameter of the nuisance function $\\eta$, evaluated at true parameter $\\eta_0$, has an expectation of zero.\n\n$$\n\\frac{\\partial}{\\partial \\eta} E[ \\psi(W; \\theta_0, \\eta)]|_{\\eta = \\eta_0} = 0\n$$\n\n![featured_image](featured_image.jpg)\n\n> Side note #1: $\\eta_0$ is the true nuisance function, which is unknown. $\\eta$ represents any candidate nuisance function which can be plugged in for theoretical analysis. $\\hat{\\eta}$ is the estimated nuisance parameter from observed data.\n\n> Side note #2: $\\frac{\\partial}{\\partial \\eta} f(\\eta)|_{\\eta = \\eta_0}$ notation is [Gateaux derivative](https://en.wikipedia.org/wiki/Gateaux_derivative), which I think of as partial derivative with respect to a small perturbation in the direction of function $\\eta$.\n\nThat is, small perturbations of estimated nuisance parameter $\\hat{\\eta}$, aka *errors in estimation*, do not prevent the score function from converging at $n^{-\\frac{1}{2}}$ rate. \n\n## Partially Linear Regression (PLR) Variant\n\nSpecifically for DML, it will hold as long as both nuisance functions (outcome estimator $g$ and treatment estimator $m$) converges at $n^{-\\frac{1}{4}}$ rate. Let us look at a simple variant [partially linear regression (PLR)](https://docs.doubleml.org/stable/guide/models.html#partially-linear-models-plm) where we are estimating average treatment effect (ATE) $\\theta_0$:\n\n$$\n\\begin{aligned}\nY &= g_0(X) + \\theta_0 D + U \\quad; E[U|X,D] = 0 \\\\\nD &= m_0(X) + V \\quad; E[V|X] = 0\n\\end{aligned}\n$$\n\nwhere \n\n- $Y$: $n \\times 1$ outcome vector where $n$ is number of examples\n\n- $X$: $n \\times k$ feature vector where $k$ is number of features\n\n- $\\theta_0$: scalar coefficient of causal effect\n\n- $g_0$ and $m_0$: $n \\times 1$ outcome vectors of nuisance functions\n\n- $U$ and $V$: $n \\times 1$ error vectors\n\n## Population Moment Condition\n\nWe derive the population [moment condition](https://en.wikipedia.org/wiki/Generalized_method_of_moments) to solve for $\\theta_0$ as:\n\n$$\n\\begin{aligned}\nE[U \\cdot V] &= 0 \\\\\n&= E[E[U \\cdot V|X,D]] \\quad \\text{by Law of Iterated Expectation} \\\\\n&= E[V \\cdot E[U|X,D]] \\quad \\text{; }V = D - m_0(X)\\\\\n&= E[V \\cdot 0] \\quad \\text{; }E[U|X,D] = 0\\\\\n&= 0 \\\\\nE[(Y - g_0(X) - \\theta_0 D) \\cdot (D - m_0(X))] &= 0 \\\\\n\\end{aligned}\n$$\n\n## Score Function based on Residualized Regression\n\nThis looks remarkably similar to an OLS moment condition to solve for a consistent $\\theta_0$ where we regress the residuals of $Y$ on those of $D$. Given:\n\n$$\n\\begin{aligned}\n\\tilde{Y} &= Y - g_0(X) \\\\\n\\tilde{D} &= D - m_0(X) \\\\\n\\end{aligned}\n$$\n\nthen \n\n$$\n\\begin{aligned}\nE[(Y - g_0(X) - \\theta_0 D) \\cdot (D - m_0(X))] &= E[(\\tilde{Y} - \\theta D) \\cdot \\tilde{D}] \\\\\n&= E[\\big(\\tilde{Y} - \\theta (\\tilde{D} + m_0(X))\\big) \\cdot \\tilde{D]} \\\\\n&= E[\\big(\\tilde{Y} - \\theta \\tilde{D}\\big) \\cdot \\tilde{D}]  - E[\\theta m_0(X) \\tilde{D}] \\\\\n\\end{aligned}\n$$\n\nThe latter term $E[\\theta m_0(X) \\tilde{D}]$ simplifies to zero:\n\n$$\n\\begin{aligned}\nE[\\theta m_0(X) \\tilde{D}] &=\\theta E[m_0(X) \\tilde{D}] \\\\\n&= \\theta E[m_0(X) \\cdot (D-m_0(X))] \\\\\n\\text{By Law of Iterated Expectations,} \\\\\n&= \\theta E[E[m_0(X) \\cdot (D-m_0(X))]|X] \\\\\n&= \\theta E\\big[m_0(X) \\cdot E[m_0(X)-m_0(X)]\\big] \\text{ ; as } E[D|X] = m_0(X) \\\\\n&=0\n\\end{aligned}\n$$\n\nTherefore the population moment condition can in fact be rewritten as:\n\n$$\n\\begin{aligned}\nE[(Y - g_0(X) - \\theta_0 D) \\cdot (D - m_0(X))] \n&= E[\\big(\\tilde{Y} - \\theta \\tilde{D}\\big) \\cdot \\tilde{D}]   \\\\\n&= (Y-g(X) - \\theta (D-m(X))) \\cdot (D - m(X))\n\\end{aligned}\n$$\n\n\nThe moment condition represents the following residualized regression:\n\n$$\n\\begin{aligned}\nY &= g_0(X) + \\theta_0 D + U \\\\\n\\tilde{Y} &=  \\theta_0 (\\tilde{D} + m_0(X)) + U \\\\\n&= \\theta_0 \\tilde{D} + \\theta_0 m_0(X) + U \\\\\n&= \\theta_0 \\tilde{D} + \\varepsilon\n\\end{aligned}\n$$\n\nThis can be interpreted as a generalization of the [Frisch–Waugh–Lovell (FWL) theorem](https://en.wikipedia.org/wiki/Frisch%E2%80%93Waugh%E2%80%93Lovell_theorem). However, unlike in FWL, $\\varepsilon$ does NOT satisfy other OLS assumptions such as zero-mean of errors $E[\\varepsilon] = 0$, exogeneity $E[\\varepsilon|\\tilde{D}] = 0$ and homoskedasticity $Var(\\varepsilon|\\tilde{D}) = \\sigma^2$. Nonetheless, it estimates a consistent $\\theta_0$ as:\n\n$$\n\\begin{aligned}\nE[\\varepsilon \\cdot \\tilde{D}] &= 0 \\\\\n&= E[(\\tilde{Y} - \\theta_0 \\tilde{D}) \\cdot \\tilde{D}] \\\\\n&= E[\\tilde{Y}\\tilde{D} - \\theta_0 \\tilde{D^2}]\\\\\n&= E[\\tilde{Y}\\tilde{D}] - \\theta_0 E[\\tilde{D^2}]  \\text{; } \\theta_0 \\text{ is a constant vector }\\\\\n\\theta_0&= \\frac{E[\\tilde{Y}\\tilde{D}]}{E[\\tilde{D^2}]}  \\text{; population parameter derivation}\\\\\n\\hat{\\theta} &= \\frac{\\sum_{i=1}^{N} \\tilde{D}_i \\tilde{Y}_i}{\\sum_{i=1}^{N} \\tilde{D}_i^2}  \\text{; sample estimation}\\\\\n\\end{aligned}\n$$\n\n## Score Function That Satisfies Neyman Orthogonality\n\nThis moment condition satisfies Neyman orthogonality because:\n\n$$\n\\begin{aligned}\n\\frac{\\partial}{\\partial g} E\\Big[\\psi(W; \\theta_0, g, m_0)\\Big]|_{g = g_0} &=\\frac{\\partial}{\\partial g} E\\Big[\\Big(Y-g(X) - \\theta_0 (D-m_0(X)\\Big)\\Big(D - m(X)\\Big) \\cdot h_g(X)\\Big]|_{g = g_0} \\\\\n&= 0 \\\\\n\\frac{\\partial}{\\partial m} \\Big[\\psi(W; \\theta_0, g_0, m)\\Big]|_{m = m_0} &=\\frac{\\partial}{\\partial m} E\\Big[\\Big(Y-g_0(X) - \\theta_0 (D-m(X)\\Big) \\Big(D - m(X)\\Big) \\cdot h_m(X)\\Big]|_{m = m_0} \\\\\n&= 0 \\\\\n\\end{aligned}\n$$\n\n\nwhere $h_g(X)$ and $h_m(X)$ are functional perturbations in nuisance function estimation. Specifically, for $g$, it is shown as:\n\n$$\n\\begin{aligned}\nE\\Big[\\frac{\\partial}{\\partial g} \\psi(W;\\theta_0,g,m_0)\\Big] |_{g = g_0} &= 0 \\\\\n&= E\\Big[\\frac{\\partial}{\\partial g}((Y-g(X) - \\theta_0 (D-m_0(X)) (D - m(X)) \\cdot h_g\\Big] \\\\\n&= E\\Big[-1 \\cdot (D - m_0(X)) \\cdot h_g\\Big] \\\\\n\\text{By Law of Iterated Expecations,} \\\\\n&= E\\Big[E[(m_0(X) - D) \\cdot h_g]|X\\Big]  \\\\\n\\text{Since } E[h_g(X)|X] = h_g(X), \\\\\n&= E\\Big[h_g(X) \\cdot E[m_0(X) - D|X]\\Big] \\\\\n\\text{Since } E[D|X] = m_0(X), \\\\\n&= E[h_g(X) \\cdot 0] \\\\\n&= 0\n\\end{aligned}\n$$\n\nand for $m$, it is shown as:\n\n$$\n\\begin{aligned}\nE\\Big[\\frac{\\partial}{\\partial m}\\psi(W;\\theta_0,g_0,m)\\cdot h_m\\Big]\\Big|_{m=m_0}\n= E\\Big[\\frac{\\partial}{\\partial m}\\Big(Y-g_0(X)-\\theta_0(D-m(X)\\Big)\\Big(D-m(X)\\Big)\\cdot h_m\\Big]|_{m=m_0} \\\\\n\\end{aligned}\n$$\n\nTo simplify the derivation, let:\n\n- $A$ be $Y-g_0(X)-\\theta_0(D-m(X))$\n\n- $B$ be $D-m(X)$\n\n- $\\psi(W;\\theta_0,g_0,m) = A\\cdot B$\n\n- $\\frac{\\partial A}{ \\partial m} = \\theta_0$\n\n- $\\frac{\\partial B}{\\partial m} = -1$\n\nthen \n\n$$\n\\begin{aligned}\nE\\Big[\\frac{\\partial}{\\partial m}\\psi(W;\\theta_0,g_0,m) \\cdot h_m\\Big] |_{m = m_0} &= E\\Big[h_m \\cdot (-A + \\theta_0 B)\\Big] \\\\\n&= E\\Big[h_m \\cdot (-Y + g_0(X) + 2\\theta_0(D-m(X))\\Big] \\\\\n\\text{By Law of Iterated Expecations:} \\\\\n&= E\\Big[E[h_m \\cdot (-Y + g_0(X) + 2\\theta_0(D-m(X))|X]\\Big] \\\\\n\\text{Since } E[h_m(X)|X] = h_m(X): \\\\\n&= E\\Big[h_m \\cdot \\Big(E[-Y + g_0(X) + 2\\theta_0(D-m(X)|X]\\Big)\\Big]  \\\\\n\\text{Since } E[D|X] = m_0(X) \\text{ and } E[Y|X] = g_0(X): \\\\\n&= E[h_m \\cdot 0]  \\\\\n&= 0\n\\end{aligned}\n$$\n\n## Neyman Orthogonality Ensures $n^{-\\frac{1}{2}}$ Convergence\n\nTo demonstrate that Neyman orthogonality ensures a $n^{-\\frac{1}{2}}$ convergence for our DML/PLR estimator, we can derive $\\hat{\\theta} - \\theta_0$ from the [Taylor expansion](https://en.wikipedia.org/wiki/Taylor_series) of the estimated score function around true parameters $\\theta_0, g_0, m_0$, where remainder term $R$ represents higher-order effects:\n\n$$\n\\begin{aligned}\n\\psi(W;\\hat{\\theta}, \\hat{g}, \\hat{m}) &= \\psi(W;\\theta_0, g_0, m_0) + \\frac{\\partial}{\\partial \\theta}\\psi(W;\\theta_0,g_0,m_0) (\\hat{\\theta} - \\theta_0) \\\\\n&\\quad + \\frac{\\partial}{\\partial m}\\psi(W;\\theta_0,g_0,m_0) (\\hat{m}-m_0) + \\frac{\\partial }{\\partial g}\\psi(W;\\theta_0,g_0,m_0) (\\hat{g}-g_0) + R \\\\\n&= \\psi(W;\\theta_0, g_0, m_0) + \\frac{\\partial }{\\partial \\theta}\\psi(W;\\theta_0,g_0,m_0) (\\hat{\\theta} - \\theta_0) \\\\\n\\text{By Neyman orthogonality,}\\\\\n&\\quad + 0 \\cdot  (\\hat{m}-m_0) + 0 \\cdot (\\hat{g}-g_0) + R \\\\\n&= \\psi(W;\\theta_0, g_0, m_0) + \\frac{\\partial}{\\partial \\theta} \\psi(W;\\theta_0,g_0,m_0) (\\hat{\\theta} - \\theta_0) + R\\\\\n\\end{aligned}\n$$\n\nDue to sample moment condition $\\frac{1}{n} \\sum_{i=1}^{N} \\psi(W_i;\\hat{\\theta}, \\hat{g}, \\hat{m}) =0$, \n\n$$\n\\begin{aligned}\n0&=\\psi(W;\\theta_0, g_0, m_0) + \\frac{\\partial}{\\partial \\theta}\\psi(W;\\theta_0,g_0,m_0) (\\hat{\\theta} - \\theta_0) + R\\\\\n\\frac{\\partial}{\\partial \\theta} \\psi(\\theta_0,g_0,m_0) (\\hat{\\theta} - \\theta_0) &= - \\psi(\\theta_0, g_0, m_0) - R \\\\\n\\hat{\\theta} - \\theta_0 &= \\big[-\\frac{\\partial}{\\partial \\theta}\\psi(\\theta_0,g_0,m_0) \\big]^{-1} \\big[  \\psi(\\theta_0, g_0, m_0) + R\\big] \\\\\n\\end{aligned}\n$$\n\nFirst, let us consider why $n^{\\frac{1}{2}} \\cdot (\\hat{\\theta} - \\theta_0) \\xrightarrow{distribution} \\mathcal{N}(0, \\Sigma)$. Let $C = \\big[-\\frac{\\partial}{\\partial \\theta}\\psi(\\theta_0,g_0,m_0) \\big]^{-1}$ be a constant matrix.\n\n$$\n\\begin{aligned}\nn^{\\frac{1}{2}} \\cdot (\\hat{\\theta} - \\theta_0) &= C \\cdot \\big[ \\psi(\\theta_0, g_0, m_0) + R\\big] \\\\\n&= n^{\\frac{1}{2}} \\cdot C \\cdot \\big(\\psi(\\theta_0, g_0, m_0) + R \\big) \\\\\n&= n^{\\frac{1}{2}} \\cdot C \\cdot \\psi(\\theta_0, g_0, m_0) + n^{\\frac{1}{2}} \\cdot C \\cdot R \n\\end{aligned}\n$$\n\nFor the leading term $n^{\\frac{1}{2}} \\cdot C \\cdot \\psi(\\theta_0, g_0, m_0)$, it converges to $\\mathcal{N}(0, C \\cdot E[\\psi \\psi^\\top] \\cdot C^\\top)$ because:\n\n$$\n\\begin{aligned}\nE[n^{\\frac{1}{2}} \\cdot C \\cdot \\psi(W;\\theta_0, g_0, m_0)] &= n^{\\frac{1}{2}} \\cdot C \\cdot E[\\psi(W;\\theta_0, g_0, m_0)] \\\\\n&= n^{\\frac{1}{2}} \\cdot C \\cdot 0 \\quad \\text{By population moment condition} \\\\\n&= 0 \\\\\nVar(n^{\\frac{1}{2}} \\cdot C \\cdot \\psi(W;\\theta_0, g_0, m_0)) &=  n \\cdot C \\cdot Var(\\psi(W;\\theta_0, g_0, m_0)) \\cdot C^\\top\\\\\n\\text{By estimation using sample average,} \\\\\n&= n \\cdot C \\cdot Var(\\frac{1}{n}\\sum_{i=1}^n \\psi_i(W_i;\\theta_0, g_0, m_0)) \\cdot C^\\top \\\\\n&= n \\cdot C \\cdot \\frac{1}{n^2} Var(\\sum_{i=1}^n \\psi_i(W_i;\\theta_0, g_0, m_0)) \\cdot C^\\top \\\\\n&= n \\cdot C \\cdot \\frac{1}{n^2} \\sum_{i=1}^n Var(\\psi_i(W_i;\\theta_0, g_0, m_0)) \\cdot C^\\top \\quad \\text{by i.i.d.} \\\\\n&= n \\cdot C \\cdot \\frac{1}{n^2} \\cdot n \\cdot E[\\psi \\psi^\\top] \\cdot C^\\top  \\\\\n&=  C \\cdot E[\\psi \\psi^\\top] \\cdot C^\\top  \\\\\n\\end{aligned}\n$$\n\nFor the trailing term:\n\n$$\n\\begin{aligned}\nR &=\\frac{1}{2} (\\hat \\theta - \\theta_0)^\\top \\, \\partial^2_{\\theta\\theta} \\psi \\, (\\hat \\theta - \\theta_0) \\\\\n&\\quad + \\frac{1}{2} (\\hat g - g_0)^\\top \\, \\partial^2_{gg} \\psi \\, (\\hat g - g_0) \n+ \\frac{1}{2} (\\hat m - m_0)^\\top \\, \\partial^2_{mm} \\psi \\, (\\hat m - m_0) \\\\\n&\\quad + (\\hat \\theta - \\theta_0)^\\top \\, \\partial^2_{\\theta g} \\psi \\, (\\hat g - g_0) \n+ (\\hat \\theta - \\theta_0)^\\top \\, \\partial^2_{\\theta m} \\psi \\, (\\hat m - m_0) \n+ (\\hat g - g_0)^\\top \\, \\partial^2_{g m} \\psi \\, (\\hat m - m_0) + R^{3+}\n\\end{aligned}\n$$\n\nSince we have shown that $\\hat \\theta - \\theta_0$ converges at $n^{-\\frac{1}{2}}$, we can ignore terms that involve it. Therefore, as long as $(\\hat m - m_0)^\\top(\\hat g - g_0)$, $(\\hat g - g_0)^\\top(\\hat g - g_0)$ and $(\\hat m - m_0)^\\top(\\hat m - m_0)$ converges faster than $n^{-\\frac{1}{2}}$ーthat is the sum of convergence rates among $\\hat{g}$ and $\\hat{m}$ーlarger than $n^{-\\frac{1}{2}}$, the term will have $n^{-\\frac{1}{2}}$ convergence rate. This is also the minimum condition such that terms in $R^{3+}$ converge at at least $n^{-\\frac{1}{2}}$.\n\n## Now That We Have Nailed The Theory\n\nDML relies on Neyman orthogonality to make the final estimate insensitive to errors in nuisance function estimation, as long as the nuisance function estimators converge at at least $n^{-\\frac{1}{4}}$. It is one of the most beautiful piece of statistical frameworks I have ever studied. However, the lingering question is how do we know our models are converging at $n^{-\\frac{1}{4}}$? The original paper formulates techniques to encourage faster convergence and avoid overfitting such as [cross-fitting](https://docs.doubleml.org/stable/guide/resampling.html), but to the best of my knowledge, did not provide any concrete metric to monitor. In the next installment, I will try to lay out some practical metrics and techniques I use to solve this problem.\n\n## Appendix\n\n### $n^{-\\frac{1}{2}}$ Convergence Rate\n\nAn estimator $\\hat{\\theta}$ with true parameter $\\theta_0$  has convergence rate of $n^{-\\frac{1}{2}}$ when:\n\n$$\nn^{\\frac{1}{2}} \\, (\\hat{\\theta} - \\theta_0) \\xrightarrow{distribution} \\mathcal{N}(0, \\sigma^2)\n$$\n\n#### Sample Mean\n\nAn example would be sample mean $\\bar{X}$, which is an estimator of true mean $\\mu$ of a distribution. By [Central Limit Theorem (CLT)](https://en.wikipedia.org/wiki/Central_limit_theorem), a distribution of sample means approaches a normal distribution of mean $\\mu$ and variance $\\frac{\\sigma^2}{n}$.\n\nNote that the estimator converges to $\\mathcal{N}(0, \\sigma^2)$ when we substract $\\mu$ and multiply it by $n^{\\frac{1}{2}}$ (effectively the residuals multiplied by $n^{\\frac{1}{2}}$) because \n\n$$\n\\begin{aligned}\nE[X] &= \\mu \\\\\nE[X] - \\mu &= 0 \\\\\n(E[X] - \\mu) \\cdot n^{\\frac{1}{2}} &= 0 \\cdot n^{\\frac{1}{2}} \\\\\n&= 0\n\\end{aligned}\n$$\n\nand \n\n$$\n\\begin{aligned}\nVar(X) &= E[(X-E[X])^2] \\\\\nVar(X - \\mu) &= E[(X - \\mu - E[X - \\mu])^2] \\\\\n&= E[(X - \\mu - E[X] + \\mu)^2] \\\\\n&= E[(X- E[X])^2] \\\\\n&= Var(X) \\\\\n\nVar(X) \\cdot n^{\\frac{1}{2}} &= E[(X \\cdot n^{\\frac{1}{2}} - E[X \\cdot n^{\\frac{1}{2}}])^2] \\\\\n&= E[(X \\cdot n^{\\frac{1}{2}} - E[X] \\cdot n^{\\frac{1}{2}}])^2] \\\\\n&= E[(n \\cdot (X-E[X]))^2] \\\\\n&= n \\cdot E[(X-E[X])^2] \\\\\n&= n \\cdot \\frac{\\sigma^2}{n} \\\\\n&= \\sigma^2\n\\end{aligned}\n$$\n\n#### Ordinary Least Squares (OLS)\n\nSimilarly, an [OLS](https://en.wikipedia.org/wiki/Linear_regression) estimator also converges at $n^{-\\frac{1}{2}}$ due to given: \n\n$$\nY = X\\beta_0 + \\varepsilon\n$$\n\nwhere \n$Y$: $n \\times 1$ outcome vector where $n$ is number of examples\n$X$: $n \\times k$ feature vector where $k$ is number of features\n$\\beta_0$: $k \\times 1$ coefficient vector\n$\\varepsilon$: $n \\times 1$ error vector\n\nSolve for:\n\n$$\n\\hat\\beta = \\arg\\min_\\beta (Y - X\\beta)^\\top (Y - X\\beta)\n$$\n\nTaking derivative of $\\beta$:\n\n$$\n\\begin{aligned}\nL &= (Y - X\\beta)^\\top (Y - X\\beta) \\\\\n&= Y^\\top Y - Y^\\top X \\beta - (X\\beta)^\\top Y + (X\\beta)^\\top X \\beta \\\\\n&= Y^\\top Y - 2 X^\\top \\beta^\\top Y + \\beta^\\top X^\\top X \\beta  \\quad \\text{; 2nd and 3rd terms are the same scala} \\\\\n\\frac{\\partial L}{\\partial \\beta} &= 0 \\quad \\text{; not related to } \\beta \\\\\n  & - 2X^\\top Y \\quad ; \\frac{\\partial x^\\top B}{\\partial x} = B \\\\\n  & + 2 X^\\top X \\beta \\quad ; \\frac{\\partial x^\\top B x}{\\partial x} = 2Bx \\\\\n\\end{aligned}\n$$\n\nThen setting it to zero, we derive the exact solution as:\n\n$$\n\\begin{aligned}\n-2X^\\top Y + 2 X^\\top X \\beta &= 0 \\\\\n\\hat\\beta = (X^\\top X)^{-1} X^\\top Y\n\\end{aligned}\n$$\n\n\nSubstituting $Y = X\\beta_0 + \\varepsilon$ gives\n\n$$\n\\begin{aligned}\n\\hat\\beta &= (X^\\top X)^{-1} X^\\top Y \\\\\n&= (X^\\top X)^{-1} X^\\top (X\\beta_0 + \\varepsilon) \\\\\n&= \\beta_0 + (X^\\top X)^{-1} X^\\top \\varepsilon \\\\\n\\hat\\beta -  \\beta_0 &= (X^\\top X)^{-1} X^\\top \\varepsilon\n\\end{aligned}\n$$\n\n\nMultiplying both sides by $n^\\frac{1}{2}$:\n\n$$\nn^\\frac{1}{2}(\\hat\\beta - \\beta_0) \n= \\left(\\frac{1}{n} X^\\top X\\right)^{-1} \n  \\left( \\frac{1}{n^\\frac{1}{2}} X^\\top \\varepsilon \\right)\n$$\n\nThe first term, $\\frac{1}{n}X^\\top X$, is the *sample* uncentered second moment matrix (dimension $k \\times k$) that converges to *population* uncentered second moment matrix $Q = E[X_i X_i^\\top]$ for $i = 1, 2, ..., n$ and $X_i$ has dimension $k \\times 1$  by [Law of Large Numbers (LLN)](https://en.wikipedia.org/wiki/Law_of_large_numbers).\n\n$$\n\\frac{1}{n}X^\\top X \\xrightarrow{probability} Q\n$$\n\nThe second term, $\\frac{1}{n^\\frac{1}{2}} X^\\top \\varepsilon$, is the scaled sum of random variables (cross products between features and errors, both of which are random variables by assumptions; dimension $k \\times 1$). This converges to a normal distribution by [Central Limit Theorem (CLT)](https://en.wikipedia.org/wiki/Central_limit_theorem).\n\nThe cross product terms $X_i\\varepsilon_i$ are zero-mean due to [exogeneity] assumption $E[\\varepsilon_i|X_i] = 0$ and [Law of Iterated Expectation](https://en.wikipedia.org/wiki/Law_of_total_expectation):\n\n$$\n\\begin{aligned}\nE[\\frac{1}{n^\\frac{1}{2}} X^\\top \\varepsilon] &= \\frac{1}{n^\\frac{1}{2}}E[ X_i \\varepsilon_i] \\\\\nE[ X_i \\varepsilon_i] & =E[E[X_i \\varepsilon_i | X_i]] \\quad \\text{by Law of Iterated Expectation} \\\\\n&= E[X_i \\cdot E[\\varepsilon_i | X_i]] \\quad \\text{due to } E[X_i|X_i] = X_i \\\\\n& = E[X_i \\cdot 0] \\quad \\text{by exogeneity assumption}  \\\\\n&= 0\n\\end{aligned}\n$$\n\nDue to [homoskedasticity]((https://en.wikipedia.org/wiki/Homoscedasticity_and_heteroscedasticity)) assumption, the covariance matrix $\\Omega$ (dimension $k \\times k$) is\n\n$$\n\\begin{aligned}\n\\Omega &= Var(X_i \\varepsilon_i) \\\\\n&= E[(X_i \\varepsilon_i - E[X_i \\varepsilon_i])(X_i \\varepsilon_i - E[X_i \\varepsilon_i])^T] \\\\\n&= E[(X_i \\varepsilon_i - 0)(X_i \\varepsilon_i - 0)^T] \\\\\n&= E[\\varepsilon_i^2 X_i X_i^T] \\quad \\text{as errors are scala} \\\\ \n&= E[E[\\varepsilon_i^2 X_i X_i^T|X_i]] \\quad \\text{by Law of Iterated Expectation} \\\\\n& =E[X_i X_i^T \\cdot E[\\varepsilon_i^2|X_i]] \\text{ due to } E[X_i X_i^T|X_i] = X_i X_i^T \\\\\n& =E[X_i X_i^T \\cdot \\sigma^2] \\quad \\text{by homosckedasticity assumption} \\\\\n&= \\sigma^2 Q \\quad \\text{as } \\sigma^2 \\text{ is constant}\n\\end{aligned}\n$$\n\nWith these derivations, we can say that\n\n$$\n\\begin{aligned}\n\\left(\\frac{1}{n} X^\\top X\\right)^{-1} \\xrightarrow{probability} Q^{-1} \\\\\n\\left(\\frac{1}{n^\\frac{1}{2}} X^\\top \\varepsilon \\right) \\xrightarrow{distribution} \\mathcal{N}(0, \\sigma^2 Q) \n\\end{aligned}\n$$\n\nWith [Slutsky's Theorem](https://en.wikipedia.org/wiki/Slutsky%27s_theorem) and the fact that $Q = E[X_i X_i^\\top]$ is a symmetrical matrix of covariances $(Q^{-1})^\\top = Q^{-1}$, we can say that the entire term converges to a normal distribution.\n\n$$\n\\begin{aligned}\n\\left(\\frac{1}{n} X^\\top X\\right)^{-1} \\left(\\frac{1}{n^\\frac{1}{2}} X^\\top \\varepsilon \\right)  \\xrightarrow{distribution} \\mathcal{N}(0, Q^{-1} (\\sigma^2 Q) (Q^{-1})^\\top)  \\\\\n\\xrightarrow{distribution} \\mathcal{N}(0, Q^{-1} \\sigma^2 Q Q^{-1})) \\quad \\quad \\\\\n\\xrightarrow{distribution} \\mathcal{N}(0, Q^{-1} \\sigma^2) \\quad \\quad \\quad \\quad \\quad \\\\\n\\end{aligned}\n$$\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}