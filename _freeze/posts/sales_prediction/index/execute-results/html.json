{
  "hash": "aceaaae36bc4b1cb82a52869d8b42340",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Predict How Much A Customer Will Spend\"\nauthor: \"cstorm125\"\ndate: \"2024-11-25\"\ncategories: [retail, zero-inflated, long/fat-tailed, hurdle]\nimage: \"featured_image.jpg\"\nformat:\n  html:\n    code-fold: true\njupyter: python3\n---\n\n\nI have spent nearly a decade as a data scientist in the retail sector, but I have been approaching customer spend predictions the wrong way until I attended [Gregory M. Duncan](https://scholar.google.com/citations?user=EZ9sTM4AAAAJ&hl=en)'s lecture. Accurately predicting how much an individual customer will spend in the next X days enables key retail use cases such as personalized promotion (determine X in Buy-X-Get-Y), customer targeting for upselling (which customers have higher purchasing power), and early churn detection (customers do not spend as much as they should). What makes this problem particularly difficult is because the distribution of customer spending is both **[zero-inflated](https://en.wikipedia.org/wiki/Zero-inflated_model)** and **[long/fat-tailed](https://en.wikipedia.org/wiki/Heavy-tailed_distribution)**. Intuitively, most customers who visit your store are not going to make a purchase and among those who do, there will be some super customers who purchase an outrageous amount more than the average customer. Some parametric models allow for zero-inflated outcomes such as [Poisson](https://en.wikipedia.org/wiki/Poisson_distribution), [negative binomial](https://en.wikipedia.org/wiki/Negative_binomial_distribution), [Conway-Maxwell-Poisson](https://en.wikipedia.org/wiki/Conway%E2%80%93Maxwell%E2%80%93Poisson_distribution); however, they do not handle the long/fat-tailed explicitly. Even for non-parametric models such as decision tree ensembles, more resources (trees and splits) will be dedicated to separating zeros and handling outliers; this could lead to deterioration in performance. Using the real-world dataset [UCI Online Retail](https://archive.ics.uci.edu/dataset/352/online+retail), we will compare the performance of common approaches namely naive baseline regression, regression on winsorized outcome, regression on log-plus-one-transformed outcome to what Duncan suggested: hurdle model with Duan's method. We will demonstrate why this approach outperforms the others in most evaluation metrics and why it might not in some.\n\n![featured_image](featured_image.jpg)\n\n::: {#1e017386 .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport random\nfrom ucimlrepo import fetch_ucirepo \nimport boto3\nimport json\nfrom tqdm.auto import tqdm\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom autogluon.tabular import TabularDataset, TabularPredictor\n\nfrom sklearn.metrics import (\n    mean_squared_error, mean_absolute_error, r2_score, median_absolute_error,\n    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n)\nfrom scipy.stats import pearsonr, spearmanr, wasserstein_distance\n\ndef calculate_regression_metrics(y_true, y_pred):\n    return {\n        'root_mean_squared_error': np.sqrt(mean_squared_error(y_true, y_pred)),\n        'mean_squared_error': mean_squared_error(y_true, y_pred),\n        'mean_absolute_error': mean_absolute_error(y_true, y_pred),\n        'r2': r2_score(y_true, y_pred),\n        'pearsonr': pearsonr(y_true, y_pred)[0],\n        'spearmanr': spearmanr(y_true, y_pred)[0],\n        'median_absolute_error': median_absolute_error(y_true, y_pred),\n        'earths_mover_distance': wasserstein_distance(y_true, y_pred)\n    }\n\ndef caluclate_classification_metrics(y_true, y_pred):\n    return {\n        'accuracy': accuracy_score(y_true, y_pred),\n        'precision': precision_score(y_true, y_pred, average='weighted'),\n        'recall': recall_score(y_true, y_pred, average='weighted'),\n        'f1_score': f1_score(y_true, y_pred, average='weighted'),\n        'confusion_matrix': confusion_matrix(y_true, y_pred)\n    }\n\ndef string_to_yearmon(date):\n    date = date.split()\n    date = date[0].split('/') + date[1].split(':')\n    date = date[2] + '-' + date[0].zfill(2) #+ '-' + date[1].zfill(2) + ' ' + date[3].zfill(2) + ':' + date[4].zfill(2)\n    return date\n\ndef call_llama(system_prompt, input):\n    template = f\"\"\"<s>[INST] <<SYS>>{system_prompt}<</SYS>>{input}[/INST]\"\"\"\n    client = boto3.client(service_name='bedrock-runtime',region_name='us-west-2')\n    body = json.dumps({\n        \"prompt\": template,\n        \"temperature\": 0.,\n        \"top_p\": 0.9,\n        \"max_gen_len\": 2048,\n    })\n    response = client.invoke_model(\n        body=body,\n        modelId='us.meta.llama3-2-90b-instruct-v1:0',\n        accept='application/json',\n        contentType='application/json'\n    )\n    response_body = json.loads(response['body'].read())\n    return response_body\n\ndef call_claude(system_prompt, input):\n    client = boto3.client(service_name='bedrock-runtime',region_name='us-west-2')\n    body=json.dumps(\n        {\n            \"anthropic_version\": \"bedrock-2023-05-31\",\n            \"max_tokens\": 2048,\n            \"messages\": [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": system_prompt + '\\n' + input,\n                    }\n                    ]\n                }\n                ]\n        }  \n    )  \n\n    \n    response = client.invoke_model(body=body, \n                                   modelId='anthropic.claude-3-5-sonnet-20241022-v2:0',\n                                   contentType='application/json',\n                                   accept='application/json')\n    response_body = json.loads(response.get('body').read())\n   \n    return response_body\n```\n:::\n\n\n## This Is Not a Drill: Real-world Datasets, Meticulous Feature Engineering, State-of-the-art AutoML\n\nTo make this exercise as realistic as possible, we will use a real-world dataset (as opposed to a simulated one), perform as much feature engineering as we would in a real-world setting, and employ the best AutoML solution the market has to offer in [AutoGluon](https://auto.gluon.ai/dev/index.html).\n\n::: {#491ea0eb .cell execution_count=2}\n``` {.python .cell-code}\nonline_retail = fetch_ucirepo(id=352) \ntransaction_df = online_retail['data']['original']\noriginal_nb = transaction_df.shape[0]\n\n#create yearmon for train-valid split\ntransaction_df['yearmon'] = transaction_df.InvoiceDate.map(string_to_yearmon)\n\n#get rid of transactions without cid\ntransaction_df = transaction_df[~transaction_df.CustomerID.isna()].reset_index(drop=True)\nhas_cid_nb = transaction_df.shape[0]\n\n#fill in unknown descriptions\ntransaction_df.Description = transaction_df.Description.fillna('UNKNOWN')\n\n#convert customer id to string\ntransaction_df['CustomerID'] = transaction_df['CustomerID'].map(lambda x: str(int(x)))\n\n#simplify by filtering unit price and quantity to be non-zero (get rid of discounts, cancellations, etc)\ntransaction_df = transaction_df[(transaction_df.UnitPrice>0)&\\\n                                (transaction_df.Quantity>0)].reset_index(drop=True)\nhas_sales_nb = transaction_df.shape[0]\n\n#add sales\ntransaction_df['Sales'] = transaction_df.UnitPrice * transaction_df.Quantity\n```\n:::\n\n\nWe use the [UCI Online Retail](https://archive.ics.uci.edu/dataset/352/online+retail) dataset, which contain transactions from a UK-based, non-store online retail from 2010\\-12 and 2011\\-12. We perform the following data processing:\n\n1. Remove transactions without `CustomerID`; from 541\\,909 to 406\\,829 transactions\n2. Filter out transactions where either `UnitPrice` or `Quantity` is less than zero; from 406\\,829 to 397\\,884 transactions\n3. Fill in missing product `Description` with value `UNKNOWN`.\n\n::: {#53f5ebf9 .cell execution_count=3}\n``` {.python .cell-code}\nprint(transaction_df.shape)\ntransaction_df.sample(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(397884, 10)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>InvoiceNo</th>\n      <th>StockCode</th>\n      <th>Description</th>\n      <th>Quantity</th>\n      <th>InvoiceDate</th>\n      <th>UnitPrice</th>\n      <th>CustomerID</th>\n      <th>Country</th>\n      <th>yearmon</th>\n      <th>Sales</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>244313</th>\n      <td>566950</td>\n      <td>23026</td>\n      <td>DRAWER KNOB VINTAGE GLASS STAR</td>\n      <td>6</td>\n      <td>9/15/2011 16:34</td>\n      <td>2.08</td>\n      <td>17594</td>\n      <td>United Kingdom</td>\n      <td>2011-09</td>\n      <td>12.48</td>\n    </tr>\n    <tr>\n      <th>390470</th>\n      <td>580808</td>\n      <td>22543</td>\n      <td>MINI JIGSAW BAKE A CAKE</td>\n      <td>48</td>\n      <td>12/6/2011 11:24</td>\n      <td>0.19</td>\n      <td>16133</td>\n      <td>United Kingdom</td>\n      <td>2011-12</td>\n      <td>9.12</td>\n    </tr>\n    <tr>\n      <th>236686</th>\n      <td>566247</td>\n      <td>47566</td>\n      <td>PARTY BUNTING</td>\n      <td>20</td>\n      <td>9/11/2011 12:03</td>\n      <td>4.95</td>\n      <td>16107</td>\n      <td>United Kingdom</td>\n      <td>2011-09</td>\n      <td>99.00</td>\n    </tr>\n    <tr>\n      <th>361574</th>\n      <td>578022</td>\n      <td>21843</td>\n      <td>RED RETROSPOT CAKE STAND</td>\n      <td>1</td>\n      <td>11/22/2011 13:25</td>\n      <td>10.95</td>\n      <td>17371</td>\n      <td>United Kingdom</td>\n      <td>2011-11</td>\n      <td>10.95</td>\n    </tr>\n    <tr>\n      <th>293093</th>\n      <td>571601</td>\n      <td>22728</td>\n      <td>ALARM CLOCK BAKELIKE PINK</td>\n      <td>2</td>\n      <td>10/18/2011 10:53</td>\n      <td>3.75</td>\n      <td>17073</td>\n      <td>United Kingdom</td>\n      <td>2011-10</td>\n      <td>7.50</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe formulate the problem as predicting the sales (`TargetSales`) during Q4 2011 for each customers who bought at least one item during Q1-Q3 2011. Note that we are interested in predicting the **spend per customer** as accurately as possible; this is common for marketing use cases such as determining what spend threshold to give each customer in a promotion, targeting customers for upselling, or detecting early signs of churns. It is notably different from predicting **total spend of all customers** during a time period, which usually requires a different approach.\n\n::: {#8012c9fe .cell execution_count=4}\n``` {.python .cell-code}\nfeature_period = {'start': '2011-01', 'end': '2011-09'}\noutcome_period = {'start': '2011-10', 'end': '2011-12'}\n\nfeature_transaction = transaction_df[(transaction_df.yearmon>=feature_period['start'])&\\\n                                      (transaction_df.yearmon<=feature_period['end'])]\noutcome_transaction = transaction_df[(transaction_df.yearmon>=outcome_period['start'])&\\\n                                      (transaction_df.yearmon<=outcome_period['end'])]\n\n#aggregate sales during outcome period\noutcome_sales = outcome_transaction.groupby('CustomerID').Sales.sum().reset_index()\n\n#aggregate sales during feature period\nfeature_sales = feature_transaction.groupby('CustomerID').Sales.sum().reset_index()\n\n#merge to get TargetSales including those who spent during feature period but not during outcome (zeroes)\noutcome_df = feature_sales[['CustomerID']].merge(outcome_sales, on='CustomerID', how='left')\noutcome_df['Sales'] = outcome_df['Sales'].fillna(0)\noutcome_df.columns = ['CustomerID', 'TargetSales']\n```\n:::\n\n\nWe transform the transaction dataset into a customer-level dataset where we calculate features using transactions between 2011-01 to 2011-09 and outcome using transactions between 2011-10 to 2011-12, summing `Quantity` times `UnitPrice`. We left-join the customers in feature set to outcome set. This will result in the zero-inflated nature of the outcome as not all customers will come back in Q4. The distribution of non-zero sales is naturally long/fat-tailed with a few customers having extraordinarily high amount of sales in Q4. This resulted in a customer-level dataset with 3\\,438 customers.\n\n::: {#31bd5fa1 .cell execution_count=5}\n``` {.python .cell-code}\n#confirm zero-inflated, long/fat-tailed\noutcome_df.TargetSales.describe(percentiles=[i/10 for i in range(10)])\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\ncount      3438.000000\nmean        666.245829\nstd        4016.843037\nmin           0.000000\n0%            0.000000\n10%           0.000000\n20%           0.000000\n30%           0.000000\n40%           0.000000\n50%         102.005000\n60%         263.006000\n70%         425.790000\n80%         705.878000\n90%        1273.611000\nmax      168469.600000\nName: TargetSales, dtype: float64\n```\n:::\n:::\n\n\n::: {#f3452b1e .cell execution_count=6}\n``` {.python .cell-code}\n#confirm zero-inflated, long/fat-tailed\noutcome_df[outcome_df.TargetSales<=10_000].TargetSales.hist(bins=100)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=605 height=411}\n:::\n:::\n\n\nWe represent a customer using traditional RFM features namely recency of purchase, purchase days, total sales, number of distinct products purchased, number of distinct category purchased, customer tenure within 2011, average purchase frequency, average purchase value, and percentage of purchase across all 9 categories. This is based on data from Q1-Q3 2011.\n\nSince the [UCI Online Retail](https://archive.ics.uci.edu/dataset/352/online+retail) dataset does not have a category but only contains descriptions over 3,000 items, we use `LLaMA 3.2 90B` to infer categories based on randomly selected 1,000 descriptions. This is to make the category preference representation for each customer, which is more tractable than including features about all 3\\,548 items. After that, we use `Claude 3.5 v2` to label a category for each description as it performs structured output a little more reliably. The categories are:\n\n1. Home Decor\n2. Kitchen and Dining\n3. Fashion Accessories\n4. Stationary and Gifts\n5. Toys and Games\n6. Seasonal and Holiday\n7. Personal Care and Wellness\n8. Outdoor and Garden\n9. Others\n\n::: {#c22ac0f2 .cell execution_count=7}\n``` {.python .cell-code}\ndescriptions = feature_transaction.Description.unique().tolist()\nprint(descriptions[:5])\n\n#randomize descriptions with seed 112 to get which categories we should use\nnp.random.seed(112)\nrandom_descriptions = np.random.choice(descriptions, 1000, replace=False)\n\nres = call_llama(\n    'You are a product categorization assistant at a retail website.',\n    'Given the following product descriptions, come up with a few product categories they should be classified into.'+'\\n'.join(random_descriptions)\n)\n\ncategories = [\n    'Home Decor',\n    'Kitchen and Dining',\n    'Fashion Accessories',\n    'Stationary and Gifts',\n    'Toys and Games',\n    'Seasonal and Holiday',\n    'Personal Care and Wellness',\n    'Outdoor and Garden',   \n]\n\nprint(res['generation'])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['JUMBO BAG PINK POLKADOT', 'BLUE POLKADOT WRAP', 'RED RETROSPOT WRAP ', 'RECYCLING BAG RETROSPOT ', 'RED RETROSPOT SHOPPER BAG']\n <<SYS>>Based on the product descriptions, I would categorize them into the following categories:\n\n1. Home Decor:\n\t* Wall art\n\t* Decorative items (e.g. vases, figurines, etc.)\n\t* Lighting (e.g. candles, lanterns, etc.)\n\t* Textiles (e.g. throw pillows, blankets, etc.)\n2. Kitchen and Dining:\n\t* Cookware and utensils\n\t* Tableware (e.g. plates, cups, etc.)\n\t* Kitchen decor (e.g. signs, magnets, etc.)\n\t* Food and drink items (e.g. tea, coffee, etc.)\n3. Fashion and Accessories:\n\t* Jewelry (e.g. necklaces, earrings, etc.)\n\t* Handbags and wallets\n\t* Clothing and accessories (e.g. scarves, hats, etc.)\n\t* Beauty and personal care items (e.g. cosmetics, skincare, etc.)\n4. Stationery and Gifts:\n\t* Cards and gift wrap\n\t* Stationery (e.g. notebooks, pens, etc.)\n\t* Gift items (e.g. mugs, keychains, etc.)\n5. Toys and Games:\n\t* Toys (e.g. stuffed animals, puzzles, etc.)\n\t* Games and puzzles\n6. Seasonal and Holiday:\n\t* Christmas decorations and gifts\n\t* Easter decorations and gifts\n\t* Other seasonal and holiday items\n7. Outdoor and Garden:\n\t* Garden decor (e.g. planters, statues, etc.)\n\t* Outdoor furniture and accessories\n\t* Gardening tools and supplies\n\nNote that some products may fit into multiple categories, but I have assigned them to the one that seems most relevant.\n```\n:::\n:::\n\n\n::: {#dc96d700 .cell execution_count=8}\n``` {.python .cell-code}\n#loop through descriptions in batches of batch_size\nres_texts = []\nbatch_size = 100\nfor i in tqdm(range(0, len(descriptions), batch_size)):\n    batch = descriptions[i:i+batch_size]\n    d = \"\\n\".join(batch)\n    inp = f'''Categorize the following product descriptions into {\", \".join(categories)} or Others, if they do not fall into any. \nOnly answer in the following format:\n\n\"product description of product #1\"|\"product category classified into\"\n\"product description of product #2\"|\"product category classified into\"\n...\n\"product description of product #n\"|\"product category classified into\"\n\nHere are the product descriptions:\n{d}\n'''\n    while True:\n        res = call_claude('You are a product categorizer at a retail website', inp)\n        # if res['generation_token_count'] > 1: #for llama\n        if res['usage']['output_tokens'] > 1:\n            break\n        else:\n            print('Retrying...')\n            time.sleep(2)\n    res_text = res['content'][0]['text'].strip().split('\\n')\n        #for llama\n        # .replace('[SYS]','').replace('<<SYS>>','')\\\n        # .replace('[/SYS]','').replace('<</SYS>>','')\\\n    if res_text!='':\n        res_texts.extend(res_text)\n\nwith open('../../data/sales_prediction/product_description_category.csv','w') as f:\n    f.write('\"product_description\"|\"category\"\\n')\n    for i in res_texts:\n        f.write(f'{i}\\n')\n```\n:::\n\n\nHere is the share of product descriptions in each annotated category:\n\n::: {#8eae31e5 .cell execution_count=9}\n``` {.python .cell-code}\nproduct_description_category = pd.read_csv('../../data/sales_prediction/product_description_category.csv',\n                                           sep='|')\n\n#clean product_description\nproduct_description_category['Description'] = descriptions\nproduct_description_category.category.value_counts(normalize=True)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\ncategory\nHome Decor                    0.328636\nKitchen and Dining            0.195885\nFashion Accessories           0.138670\nStationary and Gifts          0.116122\nSeasonal and Holiday          0.087373\nPersonal Care and Wellness    0.047351\nToys and Games                0.045096\nOutdoor and Garden            0.032976\nOthers                        0.007892\nName: proportion, dtype: float64\n```\n:::\n:::\n\n\nWe merge the RFM features with preference features, that is share of sales in each category for every customer, then the outcome `TargetSales` to create the universe set for the problem.\n\n::: {#767f1237 .cell execution_count=10}\n``` {.python .cell-code}\nfeature_transaction_cat = feature_transaction.merge(product_description_category,\n                                                    how='inner',\n                                                    on = 'Description',)\nfeature_transaction.shape, feature_transaction_cat.shape\n\n#convert invoice date to datetime\nfeature_transaction_cat['InvoiceDate'] = pd.to_datetime(feature_transaction_cat['InvoiceDate'])\n\n# last date in feature set\ncurrent_date = feature_transaction_cat['InvoiceDate'].max()\n\n#rfm\ncustomer_features = feature_transaction_cat.groupby('CustomerID').agg({\n    'InvoiceDate': [\n        ('recency', lambda x: (current_date - x.max()).days),\n        ('first_purchase_date', 'min'),\n        ('purchase_day', 'nunique'),\n    ],\n    'InvoiceNo': [('nb_invoice', 'nunique')],\n    'Sales': [\n        ('total_sales', 'sum')\n    ],\n    'StockCode': [('nb_product', 'nunique')],\n    'category': [('nb_category', 'nunique')]\n}).reset_index()\n\n# Flatten column names\ncustomer_features.columns = [\n    'CustomerID',\n    'recency',\n    'first_purchase_date',\n    'purchase_day',\n    'nb_invoice',\n    'total_sales',\n    'nb_product',\n    'nb_category'\n]\n\ncustomer_features['customer_lifetime'] = (current_date - customer_features['first_purchase_date']).dt.days\ncustomer_features['avg_purchase_frequency'] = customer_features['customer_lifetime'] / customer_features['purchase_day']\ncustomer_features['avg_purchase_value'] = customer_features['total_sales'] / customer_features['purchase_day']\n\n#category preference\ncategory_sales = feature_transaction_cat.pivot_table(\n    values='Sales', \n    index='CustomerID', \n    columns='category', \n    aggfunc='sum', \n    fill_value=0\n)\ncategory_sales.columns = [i.lower().replace(' ','_') for i in category_sales.columns]\ncustomer_features = customer_features.merge(category_sales, on='CustomerID', how='left')\n\ntotal_sales = customer_features['total_sales']\nfor col in category_sales.columns:\n    percentage_col = f'per_{col}'\n    customer_features[percentage_col] = customer_features[col] / total_sales\n\nselected_features = [\n 'recency',\n 'purchase_day',\n 'total_sales',\n 'nb_product',\n 'nb_category',\n 'customer_lifetime',\n 'avg_purchase_frequency',\n 'avg_purchase_value',\n 'per_fashion_accessories',\n 'per_home_decor',\n 'per_kitchen_and_dining',\n 'per_others',\n 'per_outdoor_and_garden',\n 'per_personal_care_and_wellness',\n 'per_seasonal_and_holiday',\n 'per_stationary_and_gifts',\n 'per_toys_and_games']\n\noutcome_variable = 'TargetSales'\n\ncustomer_features = customer_features[[ 'CustomerID']+selected_features]\ndf = outcome_df.merge(customer_features, on='CustomerID').drop('CustomerID', axis=1)\nprint(df.shape)\ndf.sample(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(3438, 18)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TargetSales</th>\n      <th>recency</th>\n      <th>purchase_day</th>\n      <th>total_sales</th>\n      <th>nb_product</th>\n      <th>nb_category</th>\n      <th>customer_lifetime</th>\n      <th>avg_purchase_frequency</th>\n      <th>avg_purchase_value</th>\n      <th>per_fashion_accessories</th>\n      <th>per_home_decor</th>\n      <th>per_kitchen_and_dining</th>\n      <th>per_others</th>\n      <th>per_outdoor_and_garden</th>\n      <th>per_personal_care_and_wellness</th>\n      <th>per_seasonal_and_holiday</th>\n      <th>per_stationary_and_gifts</th>\n      <th>per_toys_and_games</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2606</th>\n      <td>0.00</td>\n      <td>53</td>\n      <td>2</td>\n      <td>597.48</td>\n      <td>138</td>\n      <td>8</td>\n      <td>184</td>\n      <td>92.000000</td>\n      <td>298.740</td>\n      <td>0.079383</td>\n      <td>0.433973</td>\n      <td>0.343710</td>\n      <td>0.003465</td>\n      <td>0.000000</td>\n      <td>0.041357</td>\n      <td>0.016570</td>\n      <td>0.056688</td>\n      <td>0.024854</td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>0.00</td>\n      <td>78</td>\n      <td>2</td>\n      <td>2209.85</td>\n      <td>37</td>\n      <td>6</td>\n      <td>226</td>\n      <td>113.000000</td>\n      <td>1104.925</td>\n      <td>0.030771</td>\n      <td>0.275245</td>\n      <td>0.628549</td>\n      <td>0.000000</td>\n      <td>0.021178</td>\n      <td>0.022535</td>\n      <td>0.000000</td>\n      <td>0.021721</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2900</th>\n      <td>3893.79</td>\n      <td>10</td>\n      <td>6</td>\n      <td>4099.11</td>\n      <td>78</td>\n      <td>9</td>\n      <td>172</td>\n      <td>28.666667</td>\n      <td>683.185</td>\n      <td>0.003879</td>\n      <td>0.761507</td>\n      <td>0.104540</td>\n      <td>0.003879</td>\n      <td>0.012442</td>\n      <td>0.014015</td>\n      <td>0.051597</td>\n      <td>0.043312</td>\n      <td>0.004830</td>\n    </tr>\n    <tr>\n      <th>2187</th>\n      <td>0.00</td>\n      <td>227</td>\n      <td>1</td>\n      <td>122.40</td>\n      <td>1</td>\n      <td>1</td>\n      <td>227</td>\n      <td>227.000000</td>\n      <td>122.400</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>322</th>\n      <td>0.00</td>\n      <td>68</td>\n      <td>1</td>\n      <td>147.12</td>\n      <td>3</td>\n      <td>2</td>\n      <td>68</td>\n      <td>68.000000</td>\n      <td>147.120</td>\n      <td>0.881729</td>\n      <td>0.118271</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nUnivariate correlation expectedly pinpoints `total_sales` in during Q1-Q3 2011 as the most predictive feature; however, we can see that it is still not very predictive. This shows that the problem is not a trivial one.\n\n::: {#7702c17e .cell execution_count=11}\n``` {.python .cell-code}\nprint(df[['TargetSales','total_sales']].corr())\n\n#target and most predictive variable\ndf[df.TargetSales<=25_000].plot.scatter(x='TargetSales',y='total_sales')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             TargetSales  total_sales\nTargetSales     1.000000     0.558558\ntotal_sales     0.558558     1.000000\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-2.png){width=619 height=429}\n:::\n:::\n\n\nWe randomly split the dataset into train and test sets at 80/20 ratio. We also confirm the distribution of `TargetSales` is similar across percentiles between train and test and only different at the upper end.\n\n::: {#1981b9a3 .cell execution_count=12}\n``` {.python .cell-code}\n#split into train-valid sets\ntrain_df, test_df = train_test_split(df,\n                                      test_size=0.2, \n                                      random_state=112)\npd.concat([train_df.TargetSales.describe(percentiles=[i/10 for i in range(10)]).reset_index(),\ntest_df.TargetSales.describe(percentiles=[i/10 for i in range(10)]).reset_index(),], axis=1)\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>TargetSales</th>\n      <th>index</th>\n      <th>TargetSales</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>count</td>\n      <td>2750.000000</td>\n      <td>count</td>\n      <td>688.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>mean</td>\n      <td>642.650436</td>\n      <td>mean</td>\n      <td>760.558808</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>std</td>\n      <td>4015.305436</td>\n      <td>std</td>\n      <td>4024.524400</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>min</td>\n      <td>0.000000</td>\n      <td>min</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0%</td>\n      <td>0.000000</td>\n      <td>0%</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>10%</td>\n      <td>0.000000</td>\n      <td>10%</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>20%</td>\n      <td>0.000000</td>\n      <td>20%</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>30%</td>\n      <td>0.000000</td>\n      <td>30%</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>40%</td>\n      <td>0.000000</td>\n      <td>40%</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>50%</td>\n      <td>91.350000</td>\n      <td>50%</td>\n      <td>113.575000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>60%</td>\n      <td>260.308000</td>\n      <td>60%</td>\n      <td>277.836000</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>70%</td>\n      <td>426.878000</td>\n      <td>70%</td>\n      <td>418.187000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>80%</td>\n      <td>694.164000</td>\n      <td>80%</td>\n      <td>759.582000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>90%</td>\n      <td>1272.997000</td>\n      <td>90%</td>\n      <td>1255.670000</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>max</td>\n      <td>168469.600000</td>\n      <td>max</td>\n      <td>77099.380000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Naive Baseline Regression\n\nThe most naive solution is to simply predict `TargetSales` based on the features. We use a stacked ensemble of LightGBM, CatBoost, XGBoost, Random Forest and Extra Trees via AutoGluon. We train with `good_quality` preset, stated to be [\"Stronger than any other AutoML Framework\"](https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets), for speedy training and inference but feel free to try more performant options. We exclude the neural-network models as they require further preprocessing of the features. We use an industry-grade, non-parametric model to be as close to a real use case as possible and make a point that the methodology works not only in a toy-dataset setup.\n\n::: {#76ee73b7 .cell execution_count=13}\n``` {.python .cell-code}\npreset = 'good_quality'\n\npredictor = TabularPredictor(label='TargetSales').fit(train_df[selected_features + ['TargetSales']], \n                                                      presets=preset,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\ntest_df['pred_baseline'] = predictor.predict(test_df[selected_features])\n```\n:::\n\n\n::: {#09d48c4d .cell execution_count=14}\n``` {.python .cell-code}\nmetric_baseline = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_baseline'])\nmetric_baseline['model'] = 'baseline'\nmetric_baseline\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\n{'root_mean_squared_error': 3162.478744240967,\n 'mean_squared_error': 10001271.807775924,\n 'mean_absolute_error': 715.6442657130541,\n 'r2': 0.3816166296854987,\n 'pearsonr': 0.6190719671013133,\n 'spearmanr': 0.47008461549340863,\n 'median_absolute_error': 232.98208312988282,\n 'earths_mover_distance': 287.77728784026124,\n 'model': 'baseline'}\n```\n:::\n:::\n\n\n## Regression on Winsorized Outcome\n\n::: {#d0136cd0 .cell execution_count=15}\n``` {.python .cell-code}\noutlier_per = 0.99\noutlier_cap_train = train_df['TargetSales'].quantile(outlier_per)\n```\n:::\n\n\nAn alternative approach to deal with long/fat-tailed outcome is to train on a winsorized outcome. In our case, we cap the outlier at 99.0% or `TargetSales` equals 7\\,180\\.81. While this solves the long/fat-tailed issues, it does not deal with zero inflation and also introduce bias to the outcome. This leads to better performance when tested on the winsorized outcome, but not so much on the original outcome.\n\n::: {#1f88753e .cell execution_count=16}\n``` {.python .cell-code}\n#winsorize\ntrain_df['TargetSales_win'] = train_df['TargetSales'].map(lambda x: outlier_cap_train if x> outlier_cap_train else x)\ntest_df['TargetSales_win'] = test_df['TargetSales'].map(lambda x: outlier_cap_train if x> outlier_cap_train else x)\n\npredictor = TabularPredictor(label='TargetSales_win').fit(train_df[selected_features+['TargetSales_win']],\n                                                      presets=preset,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\n\ntest_df['pred_winsorized'] = predictor.predict(test_df[selected_features])\n```\n:::\n\n\n::: {#b4e70517 .cell execution_count=17}\n``` {.python .cell-code}\nmetric_winsorized = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_winsorized'])\nmetric_winsorized['model'] = 'winsorized'\nmetric_winsorized\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\n{'root_mean_squared_error': 3623.576377551195,\n 'mean_squared_error': 13130305.76394704,\n 'mean_absolute_error': 627.7880071099414,\n 'r2': 0.18814697894155963,\n 'pearsonr': 0.5757989413256978,\n 'spearmanr': 0.504301956183441,\n 'median_absolute_error': 219.62248107910156,\n 'earths_mover_distance': 432.1288432991232,\n 'model': 'winsorized'}\n```\n:::\n:::\n\n\n## Regression on Log-plus-one-transformed Outcome\n\nLog transformation handles long/fat-tailed distribution and is especially useful for certain models since the transformed distribution is closer normal. However, it cannot handle zero-valued outcome and oftentimes scientists end up adding 1 to the outcome (so often that `numpy` even has a function for it). This not only introduces bias to the prediction, but also does not solve the zero-inflation as it becomes one-inflation instead.\n\n::: {#f6fa6f18 .cell execution_count=18}\n``` {.python .cell-code}\n#log\ntrain_df['TargetSales_log1p'] = train_df['TargetSales'].map(np.log1p)\ntest_df['TargetSales_log1p'] = test_df['TargetSales'].map(np.log1p)\n\n#from zero-inflated to one-inflated\ntrain_df['TargetSales_log1p'].hist()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-19-output-1.png){width=583 height=415}\n:::\n:::\n\n\nWe can see that this is the best performing approach so far, which is one of the reasons why so many scientists end up going for this not-entirely-correct approach.\n\n::: {#a3a8a907 .cell execution_count=19}\n``` {.python .cell-code}\npredictor = TabularPredictor(label='TargetSales_log1p').fit(train_df[selected_features+['TargetSales_log1p']],\n                                                      presets=preset,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\n\ntest_df['pred_log1p'] = predictor.predict(test_df[selected_features])\ntest_df['pred_log1p_expm1'] = test_df['pred_log1p'].map(np.expm1)\n```\n:::\n\n\n::: {#3a478639 .cell execution_count=20}\n``` {.python .cell-code}\nmetric_log1p = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_log1p_expm1'])\nmetric_log1p['model'] = 'log1p'\nmetric_log1p\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\n{'root_mean_squared_error': 3725.342295894091,\n 'mean_squared_error': 13878175.221577456,\n 'mean_absolute_error': 618.9768466651894,\n 'r2': 0.14190585634701047,\n 'pearsonr': 0.5817166874396966,\n 'spearmanr': 0.5338156315937898,\n 'median_absolute_error': 89.55495441784018,\n 'earths_mover_distance': 581.0494444960044,\n 'model': 'log1p'}\n```\n:::\n:::\n\n\n## Hurdle Model\n\nHurdle model is a two-stage approach that handles zero inflation by first having a classification model to predict if the outcome is zero or not, then a regression model, trained only on examples with actual non-zero outcomes, to fit a log-transformed outcome. When retransforming the predictions from log to non-log numbers, we perform correction of underestimation using Duan's method. During inference time, we multiply the predictions from the classification and corrected regression model.\n\n::: {#7707a407 .cell execution_count=21}\n``` {.python .cell-code}\ntrain_df['has_purchase'] = train_df.TargetSales.map(lambda x: 1 if x>0 else 0)\ntest_df['has_purchase'] = test_df.TargetSales.map(lambda x: 1 if x>0 else 0)\n\npredictor_cls = TabularPredictor(label='has_purchase').fit(train_df[selected_features+['has_purchase']],\n                                                      presets=preset,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\ntest_df['pred_binary'] = predictor_cls.predict(test_df[selected_features])\n```\n:::\n\n\nFor our splits, 51.42% of train and 53.05% of test include customers with non-zero purchase outcome. As with all two-stage approaches, we need to make sure the intermediate model performs reasonably in classifying zero/non-zero outcomes.\n\n::: {#64867a84 .cell execution_count=22}\n``` {.python .cell-code}\ncaluclate_classification_metrics(test_df['has_purchase'], test_df['pred_binary'])\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```\n{'accuracy': 0.6918604651162791,\n 'precision': 0.6941069004479309,\n 'recall': 0.6918604651162791,\n 'f1_score': 0.6921418829824787,\n 'confusion_matrix': array([[229,  94],\n        [118, 247]])}\n```\n:::\n:::\n\n\n::: {#5e18a5d2 .cell execution_count=23}\n``` {.python .cell-code}\ntrain_df_nonzero = train_df[train_df.has_purchase==1].reset_index(drop=True)\ntest_df_nonzero = test_df[test_df.has_purchase==1].reset_index(drop=True)\n\n#log\ntrain_df_nonzero['TargetSales_log'] = train_df_nonzero['TargetSales'].map(np.log)\ntest_df_nonzero['TargetSales_log'] = test_df_nonzero['TargetSales'].map(np.log)\n```\n:::\n\n\nAfter that, we perform log-transformed regression on the examples with non-zero outcome (1\\,414 examples in train). Without the need to worry about `ln(0)` outcome, the regression is much more straightforward albeit with fewer examples to train on.\n\n::: {#600fe759 .cell execution_count=24}\n``` {.python .cell-code}\ntrain_df_nonzero['TargetSales_log'].hist()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-25-output-1.png){width=575 height=415}\n:::\n:::\n\n\n::: {#f6d17c66 .cell execution_count=25}\n``` {.python .cell-code}\npredictor_reg = TabularPredictor(label='TargetSales_log').fit(train_df_nonzero[selected_features+['TargetSales_log']],\n                                                      presets=preset,\n                                                      excluded_model_types=['NN_TORCH','FASTAI','KNN'],\n                                                      )\ntest_df_nonzero['pred_log'] = predictor_reg.predict(test_df_nonzero[selected_features])\ntest_df_nonzero['pred_log_exp'] = test_df_nonzero['pred_log'].map(np.exp)\n\ntest_df['pred_log'] = predictor_reg.predict(test_df[selected_features])\ntest_df['pred_log_exp'] = test_df['pred_log'].map(np.exp)\n\ntest_df['pred_hurdle'] = test_df.pred_binary * test_df.pred_log_exp\n```\n:::\n\n\nFor inference, we combine the binary prediction (purchase/no purchase) from the classification model with the re-transformed (exponentialized) numerical prediction from the regression model by simply multiplying them together. As you can see, this approach yields the best performance so far and this is where I used to think everything has been accounted for.\n\n::: {#8518e2e8 .cell execution_count=26}\n``` {.python .cell-code}\nmetric_hurdle = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_hurdle'])\nmetric_hurdle['model'] = 'hurdle'\nmetric_hurdle\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\n{'root_mean_squared_error': 3171.760744960863,\n 'mean_squared_error': 10060066.22327469,\n 'mean_absolute_error': 584.9162934881963,\n 'r2': 0.3779813431428882,\n 'pearsonr': 0.6769697889999318,\n 'spearmanr': 0.5107083593715698,\n 'median_absolute_error': 199.1780137692856,\n 'earths_mover_distance': 286.381442541919,\n 'model': 'hurdle'}\n```\n:::\n:::\n\n\n## But Wait, There Is MoreーEnter Naihua Duan\n\nIn the previous section, we have blissfully assumed that we can freely log-transform and re-transform the outcome during training and inference without any bias. This is not the case as there is a small bias generated in the process due to the error term.\n\n$$ln(y) = f(X) + \\epsilon$$\n\nwhere \n\n* $y$ is actual outcome.\n\n* $X$ is the features.\n\n* $f(.)$ is a trained model.\n\n* $\\epsilon$ is the error term.\n\nwhen re-transforming\n\n$$\n\\begin{align}\ny &= exp(ln(y)) \\\\\n&= exp(f(X) + \\epsilon ) \\\\\n&= exp(f(X)) \\cdot exp(\\epsilon) \\\\\nE[y] &= E[exp(f(X))] \\cdot E[exp(\\epsilon)]\n\\end{align}\n$$\n\nThe average treatment affect (ATE; $E[y]$) is underestimated by $E[exp(\\epsilon)]$. [Naihua Duan (段乃華)](https://en.wikipedia.org/wiki/Naihua_Duan), a Taiwanese biostatistician, suggested a consistent estimator of $E[exp(\\epsilon)]$ in [his 1983 work](https://www.jstor.org/stable/2288126) as \n\n$$\n\\begin{align}\n\\hat \\lambda &= E[exp(ln(y) - ln(\\hat y))]\n\\end{align}\n$$\n\nwhere \n\n* $\\hat \\lambda$ is the Duan's smearing estimator of the bias from re-transformation $E[exp(\\epsilon)]$\n\n* $\\hat y$ is the prediction aka $f(X)$\n\n```\nFun Fact: If you assume Duan were a western name, you would have been \npronouncing the method's name incorrectly since it should be [twàn]'s \nmethod, NOT /dwɑn/'s method.\n```\n\nWe can easily derive Duan's smearing estimator by taking mean of error between actual and predicted `TargetSales` in the training set.\n\n::: {#a3332a30 .cell execution_count=27}\n``` {.python .cell-code}\ntrain_df_nonzero['pred_log'] = predictor_reg.predict(train_df_nonzero[selected_features])\ntrain_df_nonzero['pred_log_exp'] = train_df_nonzero['pred_log'].map(np.exp)\n\nsmearing_estimator = np.mean(np.exp(train_df_nonzero['TargetSales_log'] - train_df_nonzero['pred_log']))\nsmearing_estimator\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```\n1.2280991653046711\n```\n:::\n:::\n\n\nWe multiply this to the predictions of the hurdle model to correct the underestimation due to re-transformation bias.\n\n::: {#0acd3930 .cell execution_count=28}\n``` {.python .cell-code}\ntest_df['pred_log_exp_corrected'] = test_df['pred_log_exp'] * smearing_estimator\ntest_df['pred_hurdle_corrected'] = test_df.pred_binary * test_df.pred_log_exp_corrected\n\nmetric_hurdle_corrected = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_hurdle_corrected'])\nmetric_hurdle_corrected['model'] = 'hurdle_corrected'\nmetric_hurdle_corrected\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```\n{'root_mean_squared_error': 3055.3207868281233,\n 'mean_squared_error': 9334985.110424023,\n 'mean_absolute_error': 613.3946643257099,\n 'r2': 0.42281345159207295,\n 'pearsonr': 0.6769697889999318,\n 'spearmanr': 0.5107083593715698,\n 'median_absolute_error': 232.55557358084502,\n 'earths_mover_distance': 241.61839859133218,\n 'model': 'hurdle_corrected'}\n```\n:::\n:::\n\n\n## The Eval Bar\n\nWe can see that the hurdle model with Duan's correction performs best across majority of the metrics. We will now deep dive on metrics where it did not to understand the caveats when taking this approach.\n\n::: {#ead26f6e .cell execution_count=29}\n``` {.python .cell-code}\nmetric_df = pd.DataFrame([metric_baseline,\n                       metric_winsorized,\n                       metric_log1p,\n                       metric_hurdle,\n                       metric_hurdle_corrected,])\n\nrank_df = metric_df.copy()\nfor col in metric_df.columns.tolist()[:-1]:\n    if col in ['r2', 'pearsonr', 'spearmanr']:\n        rank_df[f'{col}_rank'] = rank_df[col].rank(ascending=False)\n    else:\n        rank_df[f'{col}_rank'] = rank_df[col].rank(ascending=True)\nrank_df = rank_df.drop(metric_df.columns.tolist()[:-1], axis=1)\nrank_df['avg_rank'] = rank_df.iloc[:,1:].mean(axis=1)\nrank_df.transpose()\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>model</th>\n      <td>baseline</td>\n      <td>winsorized</td>\n      <td>log1p</td>\n      <td>hurdle</td>\n      <td>hurdle_corrected</td>\n    </tr>\n    <tr>\n      <th>root_mean_squared_error_rank</th>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>mean_squared_error_rank</th>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>mean_absolute_error_rank</th>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>r2_rank</th>\n      <td>2.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>pearsonr_rank</th>\n      <td>3.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>1.5</td>\n      <td>1.5</td>\n    </tr>\n    <tr>\n      <th>spearmanr_rank</th>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>1.0</td>\n      <td>2.5</td>\n      <td>2.5</td>\n    </tr>\n    <tr>\n      <th>median_absolute_error_rank</th>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>earths_mover_distance_rank</th>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>avg_rank</th>\n      <td>3.375</td>\n      <td>4.0</td>\n      <td>3.625</td>\n      <td>2.25</td>\n      <td>1.75</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#3ebb6ddd .cell execution_count=30}\n``` {.python .cell-code}\nmetric_df.transpose()\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>root_mean_squared_error</th>\n      <td>3162.478744</td>\n      <td>3623.576378</td>\n      <td>3725.342296</td>\n      <td>3171.760745</td>\n      <td>3055.320787</td>\n    </tr>\n    <tr>\n      <th>mean_squared_error</th>\n      <td>10001271.807776</td>\n      <td>13130305.763947</td>\n      <td>13878175.221577</td>\n      <td>10060066.223275</td>\n      <td>9334985.110424</td>\n    </tr>\n    <tr>\n      <th>mean_absolute_error</th>\n      <td>715.644266</td>\n      <td>627.788007</td>\n      <td>618.976847</td>\n      <td>584.916293</td>\n      <td>613.394664</td>\n    </tr>\n    <tr>\n      <th>r2</th>\n      <td>0.381617</td>\n      <td>0.188147</td>\n      <td>0.141906</td>\n      <td>0.377981</td>\n      <td>0.422813</td>\n    </tr>\n    <tr>\n      <th>pearsonr</th>\n      <td>0.619072</td>\n      <td>0.575799</td>\n      <td>0.581717</td>\n      <td>0.67697</td>\n      <td>0.67697</td>\n    </tr>\n    <tr>\n      <th>spearmanr</th>\n      <td>0.470085</td>\n      <td>0.504302</td>\n      <td>0.533816</td>\n      <td>0.510708</td>\n      <td>0.510708</td>\n    </tr>\n    <tr>\n      <th>median_absolute_error</th>\n      <td>232.982083</td>\n      <td>219.622481</td>\n      <td>89.554954</td>\n      <td>199.178014</td>\n      <td>232.555574</td>\n    </tr>\n    <tr>\n      <th>earths_mover_distance</th>\n      <td>287.777288</td>\n      <td>432.128843</td>\n      <td>581.049444</td>\n      <td>286.381443</td>\n      <td>241.618399</td>\n    </tr>\n    <tr>\n      <th>model</th>\n      <td>baseline</td>\n      <td>winsorized</td>\n      <td>log1p</td>\n      <td>hurdle</td>\n      <td>hurdle_corrected</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Why Duan's Correction Results in Slightly Worse MAE?\n\nDuan's method adjusts for underestimation from re-transformation of log outcome. This could lead to smaller extreme errors, but more frequent occurrences of less extreme ones. We verify this hypothesis by comparing mean absolute error before and after transformation for errors originally under and over 99th percentile. We confirm that is the case for our problem.\n\n::: {#bd3ff6b1 .cell execution_count=31}\n``` {.python .cell-code}\nerr_hurdle = (test_df['TargetSales'] - test_df['pred_hurdle']).abs()\nerr_hurdle_corrected = (test_df['TargetSales'] - test_df['pred_hurdle_corrected']).abs()\n\nprint('Distribution of errors for Hurdle model without correction')\nerr_hurdle.describe(percentiles=[.25, .5, .75, .9, .95, .99]) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDistribution of errors for Hurdle model without correction\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=30}\n```\ncount      688.000000\nmean       584.916293\nstd       3119.628924\nmin          0.000000\n25%          0.000000\n50%        199.178014\n75%        475.603446\n90%        862.530026\n95%       1237.540954\n99%       6763.777844\nmax      55731.205996\ndtype: float64\n```\n:::\n:::\n\n\n::: {#f39df705 .cell execution_count=32}\n``` {.python .cell-code}\nprint('Hurdle Model without correction')\nprint(f'Mean absolute error under 99th percentile: {err_hurdle[err_hurdle<6763.777844].mean()}')\nprint(f'Mean absolute error over 99th percentile: {err_hurdle[err_hurdle>6763.777844].mean()}')\n\nprint('Hurdle Model with correction')\nprint(f'Mean absolute error under 99th percentile: {err_hurdle_corrected[err_hurdle<6763.777844].mean()}')\nprint(f'Mean absolute error over 99th percentile: {err_hurdle_corrected[err_hurdle>6763.777844].mean()}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHurdle Model without correction\nMean absolute error under 99th percentile: 355.4918014848842\nMean absolute error over 99th percentile: 22904.641872667555\nHurdle Model with correction\nMean absolute error under 99th percentile: 392.7718802742851\nMean absolute error over 99th percentile: 22076.839798471465\n```\n:::\n:::\n\n\n### Importance of Classification Model\n\nThe overperformance of log-transform regression over both hurdle model approarches in Spearman's rank correlation and median absolute error demonstrates the importance of a classification model. At first glance, it is perplexing since we have just spent a large portion of this article to justify that hurdle models handle zero inflation better and re-transformation without Duan's method is biased. However, it becomes clear once you compare performance of the hurdle model with a classification model (f1 = 0.69) and a hypothetical, perfect classification model. Other metrics also improved but not nearly as drastic as MedAE and Spearman's rank correlation.\n\n::: {#4f04211a .cell execution_count=33}\n``` {.python .cell-code}\ntest_df['pred_hurdle_corrected_perfect_cls'] = test_df.has_purchase * test_df.pred_log_exp_corrected\nmetric_hurdle_corrected_perfect_cls = calculate_regression_metrics(test_df['TargetSales'], test_df['pred_hurdle_corrected_perfect_cls'])\nmetric_hurdle_corrected_perfect_cls['model'] = 'hurdle_corrected_perfect_cls'\n\nmetric_df2 = pd.DataFrame([metric_baseline,\n                       metric_winsorized,\n                       metric_log1p,\n                       metric_hurdle,\n                       metric_hurdle_corrected,\n                       metric_hurdle_corrected_perfect_cls,])\nmetric_df2.transpose()\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>root_mean_squared_error</th>\n      <td>3162.478744</td>\n      <td>3623.576378</td>\n      <td>3725.342296</td>\n      <td>3171.760745</td>\n      <td>3055.320787</td>\n      <td>3030.854831</td>\n    </tr>\n    <tr>\n      <th>mean_squared_error</th>\n      <td>10001271.807776</td>\n      <td>13130305.763947</td>\n      <td>13878175.221577</td>\n      <td>10060066.223275</td>\n      <td>9334985.110424</td>\n      <td>9186081.006625</td>\n    </tr>\n    <tr>\n      <th>mean_absolute_error</th>\n      <td>715.644266</td>\n      <td>627.788007</td>\n      <td>618.976847</td>\n      <td>584.916293</td>\n      <td>613.394664</td>\n      <td>479.558294</td>\n    </tr>\n    <tr>\n      <th>r2</th>\n      <td>0.381617</td>\n      <td>0.188147</td>\n      <td>0.141906</td>\n      <td>0.377981</td>\n      <td>0.422813</td>\n      <td>0.43202</td>\n    </tr>\n    <tr>\n      <th>pearsonr</th>\n      <td>0.619072</td>\n      <td>0.575799</td>\n      <td>0.581717</td>\n      <td>0.67697</td>\n      <td>0.67697</td>\n      <td>0.687639</td>\n    </tr>\n    <tr>\n      <th>spearmanr</th>\n      <td>0.470085</td>\n      <td>0.504302</td>\n      <td>0.533816</td>\n      <td>0.510708</td>\n      <td>0.510708</td>\n      <td>0.929419</td>\n    </tr>\n    <tr>\n      <th>median_absolute_error</th>\n      <td>232.982083</td>\n      <td>219.622481</td>\n      <td>89.554954</td>\n      <td>199.178014</td>\n      <td>232.555574</td>\n      <td>34.991964</td>\n    </tr>\n    <tr>\n      <th>earths_mover_distance</th>\n      <td>287.777288</td>\n      <td>432.128843</td>\n      <td>581.049444</td>\n      <td>286.381443</td>\n      <td>241.618399</td>\n      <td>234.587018</td>\n    </tr>\n    <tr>\n      <th>model</th>\n      <td>baseline</td>\n      <td>winsorized</td>\n      <td>log1p</td>\n      <td>hurdle</td>\n      <td>hurdle_corrected</td>\n      <td>hurdle_corrected_perfect_cls</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Remember What Problem We Are Solving\n\nOne last thing to remember is that we are trying to predict **sales of each individual customer**, not **total sales of all customers**. If we look at aggregated mean or sum of actual sales vs predicted sales, baseline regression performs best by far. This is due to the fact that without any constraints a regressor only minimizes the MSE loss and usually ends up predicting values around the mean to balance between under- and over-predictions. However, this level of prediction is often not very useful as a single point. Imagine you want to give promotions with higher or lower spend thresholds to customers according to their purchasing power; you will not be able to do so with a model that is accurate on aggregate but not so much on individual customers.\n\n::: {#087813e6 .cell execution_count=34}\n``` {.python .cell-code}\ntest_df[['TargetSales','pred_baseline','pred_winsorized','pred_log1p_expm1','pred_hurdle','pred_hurdle_corrected']].mean()\n```\n\n::: {.cell-output .cell-output-display execution_count=33}\n```\nTargetSales              760.558808\npred_baseline            791.043945\npred_winsorized          508.281555\npred_log1p_expm1         186.200281\npred_hurdle              527.286811\npred_hurdle_corrected    647.560493\ndtype: float64\n```\n:::\n:::\n\n\n## Closing Remarks\n\nAnd this is how you predict how much a customer will spend in the least wrong way. My hope is that you will not need to spend ten years in data science to find out how to do it like I did.\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}