{
  "hash": "458e3636be7899913538ce58f68a7c4f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Predict How Much A Customer Will Spend\"\nauthor: \"cstorm125\"\ndate: \"2024-11-25\"\ncategories: [news, code, analysis]\nimage: \"featured_image.jpg\"\nformat:\n  html:\n    code-fold: true\njupyter: python3\n---\n\n\nI have spent nearly a decade as a data scientist in the retail sector, but I have been approaching customer spend predictions the wrong way until I attended [Gregory M. Duncan](https://scholar.google.com/citations?user=EZ9sTM4AAAAJ&hl=en)'s lecture. Accurately predicting how much an individual customer will spend in the next X days enables key retail use cases such as personalized promotion (determine X in Buy-X-Get-Y), customer targeting for upselling (which customers have higher purchasing power), and early churn detection (customers do not spend as much as they should). What makes this problem particularly difficult is because the distribution of customer spending is both **[zero-inflated](https://en.wikipedia.org/wiki/Zero-inflated_model)** and **[long/fat-tailed](https://en.wikipedia.org/wiki/Heavy-tailed_distribution)**. Intuitively, most customers who visit your store are not going to make a purchase and among those who do, there will be some super customers who purchase an outrageous amount more than the average customer. Some parametric models allow for zero-inflated outcomes such as [Poisson](https://en.wikipedia.org/wiki/Poisson_distribution), [negative binomial](https://en.wikipedia.org/wiki/Negative_binomial_distribution), [Conway-Maxwell-Poisson](https://en.wikipedia.org/wiki/Conway%E2%80%93Maxwell%E2%80%93Poisson_distribution); however, they do not handle the long/fat-tailed explicitly. Even for non-parametric models such as decision tree ensembles, more resources (trees and splits) will be dedicated to separating zeros and handling outliers; this could lead to deterioration in performance. Using the real-world dataset [UCI Online Retail](https://archive.ics.uci.edu/dataset/352/online+retail), we will compare the performance of common approaches namely naive baseline regression, regression on winsorized outcome, regression on log-plus-one-transformed outcome to what Duncan suggested: hurdle model with Duan's method. we will demonstrate why this approach outperforms the others in most evaluation metrics and why it might not in some.\n\n::: {#7bbbc96f .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport random\nfrom ucimlrepo import fetch_ucirepo \nimport boto3\nimport json\nfrom tqdm.auto import tqdm\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom autogluon.tabular import TabularDataset, TabularPredictor\n\nfrom sklearn.metrics import (\n    mean_squared_error, mean_absolute_error, r2_score, median_absolute_error,\n    accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n)\nfrom scipy.stats import pearsonr, wasserstein_distance\n\ndef calculate_regression_metrics(y_true, y_pred):\n    return {\n        'root_mean_squared_error': np.sqrt(mean_squared_error(y_true, y_pred)),\n        'mean_squared_error': mean_squared_error(y_true, y_pred),\n        'mean_absolute_error': mean_absolute_error(y_true, y_pred),\n        'r2': r2_score(y_true, y_pred),\n        'pearsonr': pearsonr(y_true, y_pred)[0],\n        'median_absolute_error': median_absolute_error(y_true, y_pred),\n        'earths_mover_distance': wasserstein_distance(y_true, y_pred)\n    }\n\ndef caluclate_classification_metrics(y_true, y_pred):\n    return {\n        'accuracy': accuracy_score(y_true, y_pred),\n        'precision': precision_score(y_true, y_pred, average='weighted'),\n        'recall': recall_score(y_true, y_pred, average='weighted'),\n        'f1_score': f1_score(y_true, y_pred, average='weighted'),\n        'confusion_matrix': confusion_matrix(y_true, y_pred)\n    }\n\ndef string_to_yearmon(date):\n    date = date.split()\n    date = date[0].split('/') + date[1].split(':')\n    date = date[2] + '-' + date[0].zfill(2) #+ '-' + date[1].zfill(2) + ' ' + date[3].zfill(2) + ':' + date[4].zfill(2)\n    return date\n\ndef call_llama(system_prompt, input):\n    template = f\"\"\"<s>[INST] <<SYS>>{system_prompt}<</SYS>>{input}[/INST]\"\"\"\n    client = boto3.client(service_name='bedrock-runtime',region_name='us-west-2')\n    body = json.dumps({\n        \"prompt\": template,\n        \"temperature\": 0.,\n        \"top_p\": 0.9,\n        \"max_gen_len\": 2048,\n    })\n    response = client.invoke_model(\n        body=body,\n        modelId='us.meta.llama3-2-90b-instruct-v1:0',\n        accept='application/json',\n        contentType='application/json'\n    )\n    response_body = json.loads(response['body'].read())\n    return response_body\n\ndef call_claude(system_prompt, input):\n    client = boto3.client(service_name='bedrock-runtime',region_name='us-west-2')\n    body=json.dumps(\n        {\n            \"anthropic_version\": \"bedrock-2023-05-31\",\n            \"max_tokens\": 2048,\n            \"messages\": [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": system_prompt + '\\n' + input,\n                    }\n                    ]\n                }\n                ]\n        }  \n    )  \n\n    \n    response = client.invoke_model(body=body, \n                                   modelId='anthropic.claude-3-5-sonnet-20241022-v2:0',\n                                   contentType='application/json',\n                                   accept='application/json')\n    response_body = json.loads(response.get('body').read())\n   \n    return response_body\n```\n:::\n\n\n## This Is Not a Drill: Real-world Datasets, Meticulous Feature Engineering, State-of-the-art AutoML\n\nTo make this exercise as realistic as possible, we will use a real-world dataset (as opposed to a simulated one), perform as much feature engineering as we would in a real-world setting, and employ the best AutoML solution the market has to offer in [AutoGluon](https://auto.gluon.ai/dev/index.html).\n\n::: {#9d80c210 .cell execution_count=2}\n``` {.python .cell-code}\nonline_retail = fetch_ucirepo(id=352) \ntransaction_df = online_retail['data']['original']\noriginal_nb = transaction_df.shape[0]\n\n#create yearmon for train-valid split\ntransaction_df['yearmon'] = transaction_df.InvoiceDate.map(string_to_yearmon)\n\n#get rid of transactions without cid\ntransaction_df = transaction_df[~transaction_df.CustomerID.isna()].reset_index(drop=True)\nhas_cid_nb = transaction_df.shape[0]\n\n#fill in unknown descriptions\ntransaction_df.Description = transaction_df.Description.fillna('UNKNOWN')\n\n#convert customer id to string\ntransaction_df['CustomerID'] = transaction_df['CustomerID'].map(lambda x: str(int(x)))\n\n#simplify by filtering unit price and quantity to be non-zero (get rid of discounts, cancellations, etc)\ntransaction_df = transaction_df[(transaction_df.UnitPrice>0)&\\\n                                (transaction_df.Quantity>0)].reset_index(drop=True)\nhas_sales_nb = transaction_df.shape[0]\n\n#add sales\ntransaction_df['Sales'] = transaction_df.UnitPrice * transaction_df.Quantity\n```\n:::\n\n\nWe use the [UCI Online Retail](https://archive.ics.uci.edu/dataset/352/online+retail) dataset, which contain transactions from a UK-based, non-store online retail from 2010\\-12 and 2011\\-12. We perform the following data processing:\n\n1. Remove transactions without `CustomerID`; from 541909 to 406829 transactions\n2. Filter out transactions where either `UnitPrice` or `Quantity` is less than zero; from 406829 to 397884 transactions\n3. Fill in missing product `Description` with value `UNKNOWN`.\n\n::: {#c5b65b6c .cell execution_count=3}\n``` {.python .cell-code}\nprint(transaction_df.shape)\ntransaction_df.sample(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(397884, 10)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>InvoiceNo</th>\n      <th>StockCode</th>\n      <th>Description</th>\n      <th>Quantity</th>\n      <th>InvoiceDate</th>\n      <th>UnitPrice</th>\n      <th>CustomerID</th>\n      <th>Country</th>\n      <th>yearmon</th>\n      <th>Sales</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>49301</th>\n      <td>543036</td>\n      <td>22423</td>\n      <td>REGENCY CAKESTAND 3 TIER</td>\n      <td>2</td>\n      <td>2/2/2011 15:57</td>\n      <td>12.75</td>\n      <td>17223</td>\n      <td>United Kingdom</td>\n      <td>2011-02</td>\n      <td>25.50</td>\n    </tr>\n    <tr>\n      <th>168967</th>\n      <td>558098</td>\n      <td>22082</td>\n      <td>RIBBON REEL STRIPES DESIGN</td>\n      <td>10</td>\n      <td>6/26/2011 13:31</td>\n      <td>1.65</td>\n      <td>18044</td>\n      <td>United Kingdom</td>\n      <td>2011-06</td>\n      <td>16.50</td>\n    </tr>\n    <tr>\n      <th>26898</th>\n      <td>540099</td>\n      <td>20724</td>\n      <td>RED RETROSPOT CHARLOTTE BAG</td>\n      <td>20</td>\n      <td>1/4/2011 16:41</td>\n      <td>0.85</td>\n      <td>15808</td>\n      <td>United Kingdom</td>\n      <td>2011-01</td>\n      <td>17.00</td>\n    </tr>\n    <tr>\n      <th>231396</th>\n      <td>565614</td>\n      <td>22456</td>\n      <td>NATURAL SLATE CHALKBOARD LARGE</td>\n      <td>1</td>\n      <td>9/5/2011 15:20</td>\n      <td>4.95</td>\n      <td>17309</td>\n      <td>United Kingdom</td>\n      <td>2011-09</td>\n      <td>4.95</td>\n    </tr>\n    <tr>\n      <th>31152</th>\n      <td>540538</td>\n      <td>82484</td>\n      <td>WOOD BLACK BOARD ANT WHITE FINISH</td>\n      <td>1</td>\n      <td>1/9/2011 14:36</td>\n      <td>6.45</td>\n      <td>17841</td>\n      <td>United Kingdom</td>\n      <td>2011-01</td>\n      <td>6.45</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe formulate the problem as predicting the sales (`TargetSales`) during Q4 2011 for each customers who bought at least one item during Q1-Q3 2011. Note that we are interested in predicting the **spend per customer** as accurately as possible; this is common for marketing use cases such as determining what spend threshold to give each customer in a promotion, targeting customers for upselling, or detecting early signs of churns. It is notably different from predicting **total spend of all customers** during a time period, which usually requires a different approach.\n\n::: {#6a146589 .cell execution_count=4}\n``` {.python .cell-code}\nfeature_period = {'start': '2011-01', 'end': '2011-09'}\noutcome_period = {'start': '2011-10', 'end': '2011-12'}\n\nfeature_transaction = transaction_df[(transaction_df.yearmon>=feature_period['start'])&\\\n                                      (transaction_df.yearmon<=feature_period['end'])]\noutcome_transaction = transaction_df[(transaction_df.yearmon>=outcome_period['start'])&\\\n                                      (transaction_df.yearmon<=outcome_period['end'])]\n\n#aggregate sales during outcome period\noutcome_sales = outcome_transaction.groupby('CustomerID').Sales.sum().reset_index()\n\n#aggregate sales during feature period\nfeature_sales = feature_transaction.groupby('CustomerID').Sales.sum().reset_index()\n\n#merge to get TargetSales including those who spent during feature period but not during outcome (zeroes)\noutcome_df = feature_sales[['CustomerID']].merge(outcome_sales, on='CustomerID', how='left')\noutcome_df['Sales'] = outcome_df['Sales'].fillna(0)\noutcome_df.columns = ['CustomerID', 'TargetSales']\n```\n:::\n\n\nWe transform the transaction dataset into a customer-level dataset where we calculate features using transactions between 2011-01 to 2011-09 and outcome using transactions between 2011-10 to 2011-12, summing `Quantity` times `UnitPrice`. We left-join the customers in feature set to outcome set. This will result in the zero-inflated nature of the outcome as not all customers will come back in Q4. The distribution of non-zero sales is naturally long/fat-tailed with a few customers having extraordinarily high amount of sales in Q4. This resulted in a customer-level dataset with 3438 customers.\n\n::: {#d9d5d785 .cell execution_count=5}\n``` {.python .cell-code}\n#confirm zero-inflated, long/fat-tailed\noutcome_df.TargetSales.describe(percentiles=[i/10 for i in range(10)])\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\ncount      3438.000000\nmean        666.245829\nstd        4016.843037\nmin           0.000000\n0%            0.000000\n10%           0.000000\n20%           0.000000\n30%           0.000000\n40%           0.000000\n50%         102.005000\n60%         263.006000\n70%         425.790000\n80%         705.878000\n90%        1273.611000\nmax      168469.600000\nName: TargetSales, dtype: float64\n```\n:::\n:::\n\n\n::: {#02a3ee9b .cell execution_count=6}\n``` {.python .cell-code}\n#confirm zero-inflated, long/fat-tailed\noutcome_df[outcome_df.TargetSales<=10_000].TargetSales.hist(bins=100)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=605 height=411}\n:::\n:::\n\n\nWe represent a customer using traditional RFM features namely recency of purchase, purchase days, total sales, number of distinct products purchased, number of distinct category purchased, customer tenure within 2011, average purchase frequency, average purchase value, and percentage of purchase across all 9 categories. This is based on data from Q1-Q3 2011.\n\nSince the [UCI Online Retail](https://archive.ics.uci.edu/dataset/352/online+retail) dataset does not have a category but only contains descriptions over 3,000 items, we use `LLaMA 3.2 90B` to infer categories based on randomly selected 1,000 descriptions. This is to make the category preference representation for each customer, which is more tractable than including features about all 3548 items. After that, we use `Claude 3.5 v2` to label a category for each description as it performs structured output a little more reliably. The categories are:\n\n1. Home Decor\n2. Kitchen and Dining\n3. Fashion Accessories\n4. Stationary and Gifts\n5. Toys and Games\n6. Seasonal and Holiday\n7. Personal Care and Wellness\n8. Outdoor and Garden\n9. Others\n\n::: {#61b69cb5 .cell execution_count=7}\n``` {.python .cell-code}\ndescriptions = feature_transaction.Description.unique().tolist()\nprint(descriptions[:5])\n\n#randomize descriptions with seed 112 to get which categories we should use\nnp.random.seed(112)\nrandom_descriptions = np.random.choice(descriptions, 1000, replace=False)\n\nres = call_llama(\n    'You are a product categorization assistant at a retail website.',\n    'Given the following product descriptions, come up with a few product categories they should be classified into.'+'\\n'.join(random_descriptions)\n)\n\ncategories = [\n    'Home Decor',\n    'Kitchen and Dining',\n    'Fashion Accessories',\n    'Stationary and Gifts',\n    'Toys and Games',\n    'Seasonal and Holiday',\n    'Personal Care and Wellness',\n    'Outdoor and Garden',   \n]\n\nprint(res['generation'])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['JUMBO BAG PINK POLKADOT', 'BLUE POLKADOT WRAP', 'RED RETROSPOT WRAP ', 'RECYCLING BAG RETROSPOT ', 'RED RETROSPOT SHOPPER BAG']\n <<SYS>>Based on the product descriptions, I would categorize them into the following categories:\n\n1. Home Decor:\n\t* Wall art\n\t* Decorative items (e.g. vases, figurines, etc.)\n\t* Lighting (e.g. candles, lanterns, etc.)\n\t* Textiles (e.g. throw pillows, blankets, etc.)\n2. Kitchen and Dining:\n\t* Cookware and utensils\n\t* Tableware (e.g. plates, cups, etc.)\n\t* Kitchen decor (e.g. signs, magnets, etc.)\n\t* Food and drink items (e.g. tea, coffee, etc.)\n3. Fashion and Accessories:\n\t* Jewelry (e.g. necklaces, earrings, etc.)\n\t* Handbags and wallets\n\t* Clothing and accessories (e.g. scarves, hats, etc.)\n4. Stationery and Gifts:\n\t* Cards and gift wrap\n\t* Stationery (e.g. notebooks, pens, etc.)\n\t* Gift items (e.g. mugs, keychains, etc.)\n5. Toys and Games:\n\t* Toys (e.g. stuffed animals, puzzles, etc.)\n\t* Games and puzzles\n6. Seasonal and Holiday:\n\t* Christmas decorations and gifts\n\t* Easter decorations and gifts\n\t* Other seasonal items (e.g. Halloween, etc.)\n7. Personal Care and Wellness:\n\t* Beauty and personal care items (e.g. skincare, haircare, etc.)\n\t* Wellness and self-care items (e.g. essential oils, etc.)\n8. Outdoor and Garden:\n\t* Garden decor and accessories\n\t* Outdoor furniture and decor\n\t* Gardening tools and supplies\n\nNote that some products may fit into multiple categories, but I have assigned them to the one that seems most relevant.\n```\n:::\n:::\n\n\n::: {#e724d670 .cell execution_count=8}\n``` {.python .cell-code}\n#loop through descriptions in batches of batch_size\nres_texts = []\nbatch_size = 100\nfor i in tqdm(range(0, len(descriptions), batch_size)):\n    batch = descriptions[i:i+batch_size]\n    d = \"\\n\".join(batch)\n    inp = f'''Categorize the following product descriptions into {\", \".join(categories)} or Others, if they do not fall into any. \nOnly answer in the following format:\n\n\"product description of product #1\"|\"product category classified into\"\n\"product description of product #2\"|\"product category classified into\"\n...\n\"product description of product #n\"|\"product category classified into\"\n\nHere are the product descriptions:\n{d}\n'''\n    while True:\n        res = call_claude('You are a product categorizer at a retail website', inp)\n        # if res['generation_token_count'] > 1: #for llama\n        if res['usage']['output_tokens'] > 1:\n            break\n        else:\n            print('Retrying...')\n            time.sleep(2)\n    res_text = res['content'][0]['text'].strip().split('\\n')\n        #for llama\n        # .replace('[SYS]','').replace('<<SYS>>','')\\\n        # .replace('[/SYS]','').replace('<</SYS>>','')\\\n    if res_text!='':\n        res_texts.extend(res_text)\n\nwith open('../../data/sales_prediction/product_description_category.csv','w') as f:\n    f.write('\"product_description\"|\"category\"\\n')\n    for i in res_texts:\n        f.write(f'{i}\\n')\n```\n:::\n\n\nHere is the share of product descriptions in each annotated category:\n\n::: {#fd1f9c78 .cell execution_count=9}\n``` {.python .cell-code}\nproduct_description_category = pd.read_csv('../../data/sales_prediction/product_description_category.csv',\n                                           sep='|')\n\n#clean product_description\nproduct_description_category['Description'] = descriptions\nproduct_description_category.category.value_counts(normalize=True)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\ncategory\nHome Decor                    0.328636\nKitchen and Dining            0.195885\nFashion Accessories           0.138670\nStationary and Gifts          0.116122\nSeasonal and Holiday          0.087373\nPersonal Care and Wellness    0.047351\nToys and Games                0.045096\nOutdoor and Garden            0.032976\nOthers                        0.007892\nName: proportion, dtype: float64\n```\n:::\n:::\n\n\nWe merge the RFM features with preference features, that is share of sales in each category for every customer, then the outcome `TargetSales` to create the universe set for the problem.\n\n::: {#481e52d1 .cell execution_count=10}\n``` {.python .cell-code}\nfeature_transaction_cat = feature_transaction.merge(product_description_category,\n                                                    how='inner',\n                                                    on = 'Description',)\nfeature_transaction.shape, feature_transaction_cat.shape\n\n#convert invoice date to datetime\nfeature_transaction_cat['InvoiceDate'] = pd.to_datetime(feature_transaction_cat['InvoiceDate'])\n\n# last date in feature set\ncurrent_date = feature_transaction_cat['InvoiceDate'].max()\n\n#rfm\ncustomer_features = feature_transaction_cat.groupby('CustomerID').agg({\n    'InvoiceDate': [\n        ('recency', lambda x: (current_date - x.max()).days),\n        ('first_purchase_date', 'min'),\n        ('purchase_day', 'nunique'),\n    ],\n    'InvoiceNo': [('nb_invoice', 'nunique')],\n    'Sales': [\n        ('total_sales', 'sum')\n    ],\n    'StockCode': [('nb_product', 'nunique')],\n    'category': [('nb_category', 'nunique')]\n}).reset_index()\n\n# Flatten column names\ncustomer_features.columns = [\n    'CustomerID',\n    'recency',\n    'first_purchase_date',\n    'purchase_day',\n    'nb_invoice',\n    'total_sales',\n    'nb_product',\n    'nb_category'\n]\n\ncustomer_features['customer_lifetime'] = (current_date - customer_features['first_purchase_date']).dt.days\ncustomer_features['avg_purchase_frequency'] = customer_features['customer_lifetime'] / customer_features['purchase_day']\ncustomer_features['avg_purchase_value'] = customer_features['total_sales'] / customer_features['purchase_day']\n\n#category preference\ncategory_sales = feature_transaction_cat.pivot_table(\n    values='Sales', \n    index='CustomerID', \n    columns='category', \n    aggfunc='sum', \n    fill_value=0\n)\ncategory_sales.columns = [i.lower().replace(' ','_') for i in category_sales.columns]\ncustomer_features = customer_features.merge(category_sales, on='CustomerID', how='left')\n\ntotal_sales = customer_features['total_sales']\nfor col in category_sales.columns:\n    percentage_col = f'per_{col}'\n    customer_features[percentage_col] = customer_features[col] / total_sales\n\nselected_features = [\n 'recency',\n 'purchase_day',\n 'total_sales',\n 'nb_product',\n 'nb_category',\n 'customer_lifetime',\n 'avg_purchase_frequency',\n 'avg_purchase_value',\n 'per_fashion_accessories',\n 'per_home_decor',\n 'per_kitchen_and_dining',\n 'per_others',\n 'per_outdoor_and_garden',\n 'per_personal_care_and_wellness',\n 'per_seasonal_and_holiday',\n 'per_stationary_and_gifts',\n 'per_toys_and_games']\n\noutcome_variable = 'TargetSales'\n\ncustomer_features = customer_features[[ 'CustomerID']+selected_features]\ndf = outcome_df.merge(customer_features, on='CustomerID').drop('CustomerID', axis=1)\nprint(df.shape)\ndf.sample(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(3438, 18)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TargetSales</th>\n      <th>recency</th>\n      <th>purchase_day</th>\n      <th>total_sales</th>\n      <th>nb_product</th>\n      <th>nb_category</th>\n      <th>customer_lifetime</th>\n      <th>avg_purchase_frequency</th>\n      <th>avg_purchase_value</th>\n      <th>per_fashion_accessories</th>\n      <th>per_home_decor</th>\n      <th>per_kitchen_and_dining</th>\n      <th>per_others</th>\n      <th>per_outdoor_and_garden</th>\n      <th>per_personal_care_and_wellness</th>\n      <th>per_seasonal_and_holiday</th>\n      <th>per_stationary_and_gifts</th>\n      <th>per_toys_and_games</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2606</th>\n      <td>0.00</td>\n      <td>53</td>\n      <td>2</td>\n      <td>597.48</td>\n      <td>138</td>\n      <td>8</td>\n      <td>184</td>\n      <td>92.000000</td>\n      <td>298.740</td>\n      <td>0.079383</td>\n      <td>0.433973</td>\n      <td>0.343710</td>\n      <td>0.003465</td>\n      <td>0.000000</td>\n      <td>0.041357</td>\n      <td>0.016570</td>\n      <td>0.056688</td>\n      <td>0.024854</td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>0.00</td>\n      <td>78</td>\n      <td>2</td>\n      <td>2209.85</td>\n      <td>37</td>\n      <td>6</td>\n      <td>226</td>\n      <td>113.000000</td>\n      <td>1104.925</td>\n      <td>0.030771</td>\n      <td>0.275245</td>\n      <td>0.628549</td>\n      <td>0.000000</td>\n      <td>0.021178</td>\n      <td>0.022535</td>\n      <td>0.000000</td>\n      <td>0.021721</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2900</th>\n      <td>3893.79</td>\n      <td>10</td>\n      <td>6</td>\n      <td>4099.11</td>\n      <td>78</td>\n      <td>9</td>\n      <td>172</td>\n      <td>28.666667</td>\n      <td>683.185</td>\n      <td>0.003879</td>\n      <td>0.761507</td>\n      <td>0.104540</td>\n      <td>0.003879</td>\n      <td>0.012442</td>\n      <td>0.014015</td>\n      <td>0.051597</td>\n      <td>0.043312</td>\n      <td>0.004830</td>\n    </tr>\n    <tr>\n      <th>2187</th>\n      <td>0.00</td>\n      <td>227</td>\n      <td>1</td>\n      <td>122.40</td>\n      <td>1</td>\n      <td>1</td>\n      <td>227</td>\n      <td>227.000000</td>\n      <td>122.400</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>322</th>\n      <td>0.00</td>\n      <td>68</td>\n      <td>1</td>\n      <td>147.12</td>\n      <td>3</td>\n      <td>2</td>\n      <td>68</td>\n      <td>68.000000</td>\n      <td>147.120</td>\n      <td>0.881729</td>\n      <td>0.118271</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nUnivariate correlation expectedly pinpoints `total_sales` in during Q1-Q3 2011 as the most predictive feature; however, we can see that it is still not very predictive. This shows that the problem is not a trivial one.\n\n::: {#d76968d3 .cell execution_count=11}\n``` {.python .cell-code}\nprint(df[['TargetSales','total_sales']].corr())\n\n#target and most predictive variable\ndf[df.TargetSales<=25_000].plot.scatter(x='TargetSales',y='total_sales')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             TargetSales  total_sales\nTargetSales     1.000000     0.558558\ntotal_sales     0.558558     1.000000\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-2.png){width=619 height=429}\n:::\n:::\n\n\nWe randomly split the dataset into train and test sets at 80/20 ratio. We also confirm the distribution of `TargetSales` is similar across percentiles between train and test and only different at the upper end.\n\n::: {#68f4dcd3 .cell execution_count=12}\n``` {.python .cell-code}\n#split into train-valid sets\ntrain_df, test_df = train_test_split(df,\n                                      test_size=0.2, \n                                      random_state=112)\npd.concat([train_df.TargetSales.describe(percentiles=[i/10 for i in range(10)]).reset_index(),\ntest_df.TargetSales.describe(percentiles=[i/10 for i in range(10)]).reset_index(),], axis=1)\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>TargetSales</th>\n      <th>index</th>\n      <th>TargetSales</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>count</td>\n      <td>2750.000000</td>\n      <td>count</td>\n      <td>688.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>mean</td>\n      <td>642.650436</td>\n      <td>mean</td>\n      <td>760.558808</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>std</td>\n      <td>4015.305436</td>\n      <td>std</td>\n      <td>4024.524400</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>min</td>\n      <td>0.000000</td>\n      <td>min</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0%</td>\n      <td>0.000000</td>\n      <td>0%</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>10%</td>\n      <td>0.000000</td>\n      <td>10%</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>20%</td>\n      <td>0.000000</td>\n      <td>20%</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>30%</td>\n      <td>0.000000</td>\n      <td>30%</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>40%</td>\n      <td>0.000000</td>\n      <td>40%</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>50%</td>\n      <td>91.350000</td>\n      <td>50%</td>\n      <td>113.575000</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>60%</td>\n      <td>260.308000</td>\n      <td>60%</td>\n      <td>277.836000</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>70%</td>\n      <td>426.878000</td>\n      <td>70%</td>\n      <td>418.187000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>80%</td>\n      <td>694.164000</td>\n      <td>80%</td>\n      <td>759.582000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>90%</td>\n      <td>1272.997000</td>\n      <td>90%</td>\n      <td>1255.670000</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>max</td>\n      <td>168469.600000</td>\n      <td>max</td>\n      <td>77099.380000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}